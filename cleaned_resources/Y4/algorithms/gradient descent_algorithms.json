{
  "title": "gradient descent",
  "language": "javascript",
  "topics": [
    "database",
    "networking",
    "algorithms"
  ],
  "purpose": "\u2022 Fixes \u2022 Local minima Gradient descent (reminder)",
  "code": "Minimum of a function is found by following the slope of the function\nf\nf(x)\nguess\nf(m)\nx\nm\n1\nGradient descent (illustration)\nf\nf(x)\nguess\nnext step\nf(m)\nx\nm\nGradient descent (illustration)\nf\nf(x)\nnew gradient\nguess\nnext step\nf(m)\nx\nm\n2\nGradient descent (illustration)\nf\nf(x)\nguess\nnext step\nf(m)\nx\nm\nGradient descent (illustration)\nf\nf(x)\nguess\nstop\nf(m)\nx\nm\n3\nGradient descent: algorithm\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nguess\nGradient descent: algorithm\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nDirection: downhill\n4\nGradient descent: algorithm\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nstep\nGradient descent: algorithm\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nNow you are here\n5\nGradient descent: algorithm\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nStop when \u201cclose\u201d\nfrom minimum\nGradient descent: algorithm\nStart with a point (guess) guess = x\nRepeat\nDetermine a descent direction direction = -f\u2019(x)\nChoose a step step = h > 0\nUpdate x:=x\u2013hf\u2019(x)\nUntil stopping criterion is satisfied f\u2019(x)~0\n6\nExample of 2D gradient: pic of the MATLAB demo\nIllustration of the gradient in 2D\nExample of 2D gradient: pic of the MATLAB demo\nIllustration of the gradient in 2D\n7\nExample of 2D gradient: pic of the MATLAB demo\nIllustration of the gradient in 2D\nExample of 2D gradient: pic of the MATLAB demo\nDefinition of the gradient in 2D\nThis is just a genaralization of the derivative in two dimensions.\nThis can be generalized to any dimension.\n8\nExample of 2D gradient: pic of the MATLAB demo\nIllustration of the gradient in 2D\nExample of 2D gradient: pic of the MATLAB demo\nGradient descent works in 2D\n9\nGeneralization to multiple dimensions\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nguess\nGeneralization to multiple dimensions\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nDirection: downhill\n10\nGeneralization to multiple dimensions\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nstep\nGeneralization to multiple dimensions\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nNow you are here\n11\nGeneralization to multiple dimensions\nStart with a point (guess)\nRepeat\nDetermine a descent direction\nChoose a step\nUpdate\nUntil stopping criterion is satisfied\nStop when \u201cclose\u201d\nfrom minimum\nGeneralization to multiple dimensions\nStart with a point (guess) guess = x\nRepeat\nDetermine a descent direction direction = -f(x)\nChoose a step step = h > 0\nUpdate x:=x\u2013h Vf\u2019(x)\nUntil stopping criterion is satisfied Vf\u2019(x)~0\n12\nMultiple dimensions\nEverything that you have seen with derivatives can be generalized\nwith the gradient.\nFor the descent method, f\u2019(x) can be replaced by\nIn two dimensions, and by\nin N dimensions.\nExample of 2D gradient: MATLAB demo\nThe cost to buy a portfolio is:\nIf you want to minimize the price to buy your portfolio, you\nneed to compute the gradient of its price:\n13\nStock\n1\nStock\n2\nStock\ni\nStock\nN\nProblem 1: choice of the step\nWhen updating the current computation:\n- small steps: inefficient\n- large steps: potentially bad results\nf\nf(x)\nguess\nToo many steps:\nstop takes too long to converge\nf(m)\nx\nm\nProblem 1: choice of the step\nWhen updating the current computation:\n- small steps: inefficient\n- large steps: potentially bad results\nf\nf(x)\nNext point (went too far)\nCurrent point\nf(m)\nx\nm\n14\nProblem 2: \u00ab ping pong effect \u00bb\n[S. Boyd, L. Vandenberghe, Convex Convex Optimization lect. Notes, Stanford Univ. 2004 ]\nProblem 2: \u00ab ping pong effect \u00bb\n[S. Boyd, L. Vandenberghe, Convex Convex Optimization lect. Notes, Stanford Univ. 2004 ]\n15\nProblem 2: (other norm dependent issues)\n[S. Boyd, L. Vandenberghe, Convex Convex Optimization lect. Notes, Stanford Univ. 2004 ]\nProblem 3: stopping criterion\nIntuitive criterion:\nIn multiple dimensions:\nOr equivalently\nRarely used in practice.\nMore about this in EE227A (convex optimization, Prof. L. El Ghaoui).\n16\nFixes\nSeveral methods exist to address this problem\n- Line search methods, in particular\n- Backtracking line search\n- Exact line search\n- Normalized steepest descent\n- Newton steps\nFundamental problem of the method: local minima\nLocal minima: pic of the MATLAB demo\nThe iterations of the algorithm\nconverge to a local minimum\n17\nLocal minima: pic of the MATLAB demo\nView of the algorithm is \u00ab myopic \u00bb\n18\n",
  "context": "\u2022 Fixes\n\u2022 Local minima\nGradient descent (reminder)",
  "source_file": "resources\\Year 4\\gradient descent.pdf",
  "line_numbers": [
    8,
    242
  ]
}