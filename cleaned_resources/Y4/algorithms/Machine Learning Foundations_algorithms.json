{
  "title": "Machine Learning Foundations",
  "language": "cpp",
  "topics": [
    "machine_learning",
    "fundamentals",
    "networking",
    "algorithms",
    "database"
  ],
  "purpose": "age 23years gender female annualsalary NTD1,000,000",
  "code": "unknowntargetfunction\nyearinresidence 1year\nf:X \u2192Y\nyearinjob 0.5year\n(idealcreditapprovalformula) currentdebt 200,000\nlearning\ntrainingexamples finalhypothesis\nalgorithm\nD:(x\n1\n,y\n1\n),\u00b7\u00b7\u00b7,(x\nN\n,y\nN\n)\nA\ng\u2248f\n(historicalrecordsinbank) (\u2018learned\u2019formulatobeused)\nhypothesisset\nH\n(setofcandidateformula)\nwhat hypothesis set can we use?\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 2/22\nLearningtoAnswerYes/No PerceptronHypothesisSet\nA Simple Hypothesis Set: the \u2018Perceptron\u2019\nage 23years\nannualsalary NTD1,000,000\nyearinjob 0.5year\ncurrentdebt 200,000\n\u2022 For x = (x ,x ,\u00b7\u00b7\u00b7 ,x ) \u2018features of customer\u2019, compute a\n1 2 d\nweighted \u2018score\u2019 and\n(cid:88)d\napprove credit if wx > threshold\ni i\ni=1\n(cid:88)d\ndeny credit if wx < threshold\ni i\ni=1\n(cid:8) (cid:9)\n\u2022 Y: +1(good),\u22121(bad) , 0 ignored\u2014linear formula h \u2208 H are\n(cid:32)(cid:32) (cid:33) (cid:33)\nd\n(cid:88)\nh(x) = sign wx \u2212threshold\ni i\ni=1\ncalled \u2018perceptron\u2019 hypothesis historically\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 3/22\nLearningtoAnswerYes/No PerceptronHypothesisSet\nVector Form of Perceptron Hypothesis\n(cid:32)(cid:32) (cid:33) (cid:33)\nd\n(cid:88)\nh(x) = sign wx \u2212threshold\ni i\ni=1\n\uf8eb \uf8f6\n(cid:32) (cid:33)\nd\n(cid:88)\n= sign\uf8ec wx +(\u2212threshold)\u00b7(+1)\uf8f7\n\uf8ed i i \uf8f8\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)\ni=1\nw x\n0 0\n(cid:32) (cid:33)\nd\n(cid:88)\n= sign wx\ni i\ni=0\n(cid:16) (cid:17)\n= sign wTx\n\u2022 each \u2018tall\u2019 w represents a hypothesis h & is multiplied with\n\u2018tall\u2019 x \u2014will use tall versions to simplify notation\nwhat do perceptrons h \u2018look like\u2019?\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 4/22\nLearningtoAnswerYes/No PerceptronHypothesisSet\nPerceptrons in\nR2\nh(x) = sign(w +w x +w x )\n0 1 1 2 2\n\u2022 customer features x: points on the plane (or points in Rd)\n\u2022 labels y: \u25e6 (+1), \u00d7 (-1)\n\u2022 hypothesis h: lines (or hyperplanes in Rd)\n\u2014positive on one side of a line, negative on the other side\n\u2022 different line classifies customers differently\nperceptrons \u21d4 linear (binary) classifiers\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 5/22\nReference Answer: 2\nThe occurrence of keywords with positive\nweights increase the \u2018spam score\u2019, and hence\nthose keywords should often appear in spams.\nLearningtoAnswerYes/No PerceptronHypothesisSet\nFun Time\nConsider using a perceptron to detect spam messages.\nAssume that each email is represented by the frequency of keyword\noccurrence, and output +1 indicates a spam. Which keywords below\nshall have large positive weights in a good perceptron for the task?\n1 coffee, tea, hamburger, steak\n2 free, drug, fantastic, deal\n3 machine, learning, statistics, textbook\n4 national, Taiwan, university, coursera\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 6/22\nLearningtoAnswerYes/No PerceptronHypothesisSet\nFun Time\nConsider using a perceptron to detect spam messages.\nAssume that each email is represented by the frequency of keyword\noccurrence, and output +1 indicates a spam. Which keywords below\nshall have large positive weights in a good perceptron for the task?\n1 coffee, tea, hamburger, steak\n2 free, drug, fantastic, deal\n3 machine, learning, statistics, textbook\n4 national, Taiwan, university, coursera\nReference Answer: 2\nThe occurrence of keywords with positive\nweights increase the \u2018spam score\u2019, and hence\nthose keywords should often appear in spams.\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 6/22\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSelect g from H\nH = all possible perceptrons, g =?\n\u2022 want: g \u2248 f (hard when f unknown)\n\u2022 almost necessary: g \u2248 f on D, ideally\ng(x ) = f(x ) = y\nn n n\n\u2022 difficult: H is of infinite size\n\u2022 idea: start from some g , and \u2018correct\u2019 its\n0\nmistakes on D\nwill represent g by its weight vector w\n0 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 7/22\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nPerceptron Learning Algorithm\nstart from some w (say, 0), and \u2018correct\u2019 itsym= + is 1 takes on D w+y x\n0\nFor t = 0,1,... y= +1 w+y xx\nw\n(cid:0) (cid:1)\n1 find a mistake of w t called x n(t) ,y n(t) x\nw\n(cid:16) (cid:17)\nsign wTx (cid:54)= y\nt n(t) n(t)\ny= \u22121 w\n2 (try to) correct the mistake by\ny=w \u2212 +y 1 x w x\nw \u2190 w +y x\nt+1 t n(t) n(t) x\nw+y x\n... until no more mistakes\nreturn last w (called w ) as g\nPLA\nThat\u2019s it!\n\u2014A fault confessed is half redressed. :-)\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 8/22\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nPractical Implementation of PLA\nstart from some w (say, 0), and \u2018correct\u2019 its mistakes on D\n0\nCyclic PLA\nFor t = 0,1,...\n(cid:0) (cid:1)\n1 find the next mistake of w t called x n(t) ,y n(t)\n(cid:16) (cid:17)\nsign wTx (cid:54)= y\nt n(t) n(t)\n2 correct the mistake by\nw \u2190 w +y x\nt+1 t n(t) n(t)\n... until a full cycle of not encountering mistakes\nnext can follow na\u00efve cycle (1,\u00b7\u00b7\u00b7 ,N)\nor precomputed random cycle\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 9/22\nuuuuuuuufuinpppppppppadddddddddlaaaaaaaaalyttttttttteeeeeeeee::::::::: 234567891\nx\n3\nww((tt+)1)\nww((tt+)1)\nwwww((((tttt+))+11))\nw\nwwwP(((tttL++)A11))\nww((tt+)1)\nwwww((tt((+)tt+)11))\nx\n1\nxxxx\n9999\nxxx\n111444\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\ninitially\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuuufinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l23456789ly\nx\n3\nww((tt+)1)\nww((tt+)1)\nwww(((ttt+))1)\nw\nwwwP(((tttL++)A11))\nww((tt+)1)\nwwww((tt((+)tt+)11))\nxxxx\n9999\nxxx\n111444\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 1\nw(t+1)\nx\n1\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuufuinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l34567891ly\nx\n3\nww((tt+)1)\nww((tt+)1)\nwww(((ttt+)+11))\nw\nwwwP(((tttL++)A11))\nww((tt+)1)\nwww(t(()tt+)1)\nx\n1\nxxx\n999\nxxx\n111444\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 2\nw(t)\nw(t+1)\nx\n9\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuufuinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l24567891ly\nx\n3\nw(t)\nww((tt+)1)\nwwww((((tttt+))+11))\nw\nwwwP(((tttL++)A11))\nww((tt+)1)\nwww(t((+tt+)11))\nx\n1\nxxxx\n9999\nxx\n1144\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 3\nw(t+1)\nw(t)\nx\n14\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuufuinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l23567891ly\nw(t+1)\nww((tt+)1)\nwwww((((tttt+))+11))\nw\nwwP((ttL+)A1)\nww((tt+)1)\nwwww((tt((+)tt+)11))\nx\n1\nxxxx\n9999\nxxx\n111444\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 4\nx\n3\nw(t)\nw(t+1)\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuufuinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l23467891ly\nx\n3\nww((tt+)1)\nww((tt+)1)\nwwww((((tttt+))+11))\nw\nwwP((ttL++A11))\nww((tt+)1)\nwww((tt(+)t)1)\nx\n1\nxxx\n999\nxxx\n111444\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 5\nw(t)\nw(t+1)\nx\n9\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuufuinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l23457891ly\nx\n3\nww((tt+)1)\nww((tt+)1)\nwww(((ttt))+1)\nw\nwwwP(((tttL++)A11))\nww((tt+)1)\nwww((tt(+)t+11))\nx\n1\nxxxx\n9999\nxx\n1144\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 6\nw(t+1)\nw(t)\nx\n14\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuufuinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l23456891ly\nx\n3\nww((tt+)1)\nww((tt+)1)\nwww(((ttt+)+11))\nw\nwwwP(((tttL++)A11))\nw(t)\nwwww((tt((+)tt+)11))\nx\n1\nxxx\n999\nxxx\n111444\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 7\nw(t)\nw(t+1)\nx\n9\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuufuinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l23456791ly\nx\n3\nww((tt+)1)\nw(t)\nwwww((((tttt+))+11))\nw\nwwwP(((tttL++)A11))\nw(t+1)\nwwww((tt((+)tt+)11))\nx\n1\nxxxx\n9999\nxx\n1144\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 8\nw(t+1)\nw(t)\nx\n14\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuufuinnppppppppaddddddddilaaaaaaaatlyitttttttteeeeeeeea:::::::: l23456781ly\nx\n3\nww((tt+)1)\nw(t+1)\nwwww((((tttt+))+11))\nwwwP((ttL+)A1)\nww((tt+)1)\nwwww((tt((+)tt+)11))\nx\n1\nxxx\n999\nxxx\n111444\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nupdate: 9\nw(t)\nw(t+1)\nx\n9\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\niuuuuuuuuunpppppppppdddddddddiaaaaaaaaatittttttttteeeeeeeeea::::::::: l234567891ly\nx\n3\nww((tt+)1)\nww((tt+)1)\nwwww((((tttt+))+11))\nwww(((ttt++)11))\nww((tt+)1)\nwwww((tt((+)tt+)11))\nx\n1\nxxxx\n9999\nxxx\n111444\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSeeing is Believing\nfinally\nw\nPLA\nworked like a charm with < 20 lines!!\n(note: made x (cid:29) x = 1 for visual purpose)\ni 0\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 10/22\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nSome Remaining Issues of PLA\n\u2018correct\u2019 mistakes on D until no mistakes\nAlgorithmic: halt (with no mistake)?\n\u2022 na\u00efve cyclic: ??\n\u2022 random cyclic: ??\n\u2022 other variant: ??\nLearning: g \u2248 f?\n\u2022 on D, if halt, yes (no mistake)\n\u2022 outside D: ??\n\u2022 if not halting: ??\n[to be shown] if (...), after \u2018enough\u2019 corrections,\nany PLA variant halts\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 11/22\nReference Answer: 3\nSimply multiply the second part of the rule by\ny x . The result shows that the rule\nn n\nsomewhat \u2018tries to correct the mistake.\u2019\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nFun Time\nLet\u2019s try to think about why PLA may work.\nLetn = n(t), according to therule ofPLA below, which formula istrue?\n(cid:16) (cid:17)\nsign wTx (cid:54)= y , w \u2190 w +y x\nt n n t+1 t n n\n1 wT t+1 x n = y n\n2 sign(wT t+1 x n ) = y n\n3 y n wT t+1 x n \u2265 y n wT t x n\n4 y n wT t+1 x n < y n wT t x n\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 12/22\nLearningtoAnswerYes/No PerceptronLearningAlgorithm(PLA)\nFun Time\nLet\u2019s try to think about why PLA may work.\nLetn = n(t), according to therule ofPLA below, which formula istrue?\n(cid:16) (cid:17)\nsign wTx (cid:54)= y , w \u2190 w +y x\nt n n t+1 t n n\n1 wT t+1 x n = y n\n2 sign(wT t+1 x n ) = y n\n3 y n wT t+1 x n \u2265 y n wT t x n\n4 y n wT t+1 x n < y n wT t x n\nReference Answer: 3\nSimply multiply the second part of the rule by\ny x . The result shows that the rule\nn n\nsomewhat \u2018tries to correct the mistake.\u2019\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 12/22\nLearningtoAnswerYes/No GuaranteeofPLA\nLinear Separability\n\u2022 if PLA halts (i.e. no more mistakes),\n(necessary condition) D allows some w to make no mistake\n\u2022 call such D linear separable\n(linearseparable) (notlinearseparable) (notlinearseparable)\nassume linear separable D,\ndoes PLA always halt?\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 13/22\nLearningtoAnswerYes/No GuaranteeofPLA\nPLA Fact: w Gets More Aligned with w\nt f\nlinear separable D \u21d4 exists perfect w such that y = sign(wTx )\nf n f n\n\u2022 w perfect hence every x correctly away from line:\nf n\ny wTx \u2265miny wTx > 0\nn(t) f n(t) n f n\nn\n\u2022 wTw \u2191 by updating with any (cid:0) x ,y (cid:1)\nf t n(t) n(t)\nwTw = wT (cid:0) w +y x (cid:1)\nf t+1 f t n(t) n(t)\n\u2265 wTw + miny wTx\nf t n f n\nn\n> wTw + 0.\nf t\nw appears more aligned with w after update\nt f\n(really?)\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 14/22\nLearningtoAnswerYes/No GuaranteeofPLA\nPLA Fact: w Does Not Grow Too Fast\nt\nw changed only when mistake\nt\n\u21d4 sign (cid:0) wTx (cid:1) (cid:54)= y \u21d4 y wTx \u2264 0\nt n(t) n(t) n(t) t n(t)\n\u2022 mistake \u2018limits\u2019 (cid:107)w (cid:107)2 growth, even when updating with \u2018longest\u2019 x\nt n\n(cid:107)w (cid:107)2 = (cid:107)w +y x (cid:107)2\nt+1 t n(t) n(t)\n= (cid:107)w (cid:107)2+2y wTx +(cid:107)y x (cid:107)2\nt n(t) t n(t) n(t) n(t)\n\u2264 (cid:107)w (cid:107)2+0+(cid:107)y x (cid:107)2\nt n(t) n(t)\n\u2264 (cid:107)w (cid:107)2+max(cid:107)y x (cid:107)2\nt n n\nn\nstart from w = 0, after T mistake corrections,\n0\nwT w \u221a\nf T \u2265 T \u00b7constant\n(cid:107)w (cid:107)(cid:107)w (cid:107)\nf T\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 15/22\nReference Answer: 2\nThe maximum value of wT f wt is 1. Since T\n(cid:107)w\nf\n(cid:107)(cid:107)wt (cid:107)\nmistake corrections increase the inner\n\u221a\nproduct by T\u00b7 constant, the maximum\nnumber of corrected mistakes is 1/constant2.\nLearningtoAnswerYes/No GuaranteeofPLA\nFun Time\nLet\u2019s upper-bound T, the number of mistakes that PLA \u2018corrects\u2019.\nwT\nDefine R2 = max(cid:107)x (cid:107)2 \u03c1 = miny f x\nn n n\nn n (cid:107)w f (cid:107)\nWe want to show that T \u2264 (cid:3). Express the upper bound (cid:3) by the two\nterms above.\n1 R/\u03c1\n2\nR2/\u03c12\n3\nR/\u03c12\n4\n\u03c12/R2\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 16/22\nLearningtoAnswerYes/No GuaranteeofPLA\nFun Time\nLet\u2019s upper-bound T, the number of mistakes that PLA \u2018corrects\u2019.\nwT\nDefine R2 = max(cid:107)x (cid:107)2 \u03c1 = miny f x\nn n n\nn n (cid:107)w f (cid:107)\nWe want to show that T \u2264 (cid:3). Express the upper bound (cid:3) by the two\nterms above.\n1 R/\u03c1\n2\nR2/\u03c12\n3\nR/\u03c12\n4\n\u03c12/R2\nReference Answer: 2\nThe maximum value of wT f wt is 1. Since T\n(cid:107)w\nf\n(cid:107)(cid:107)wt (cid:107)\nmistake corrections increase the inner\n\u221a\nproduct by T\u00b7 constant, the maximum\nnumber of corrected mistakes is 1/constant2.\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 16/22\nLearningtoAnswerYes/No Non-SeparableData\nMore about PLA\nGuarantee\nas long as linear separable and correct by mistake\n\u2022 inner product of w and w grows fast; length of w grows slowly\nf t t\n\u2022 PLA \u2018lines\u2019 are more and more aligned with w \u21d2 halts\nf\nPros\nsimple to implement, fast, works in any dimension d\nCons\n\u2022 \u2018assumes\u2019 linear separable D to halt\n\u2014property unknown in advance (no need for PLA if we know w )\nf\n\u2022 not fully sure how long halting takes (\u03c1 depends on w )\nf\n\u2014though practically fast\nwhat if D not linear separable?\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 17/22\nLearningtoAnswerYes/No Non-SeparableData\nLearning with Noisy Data\nunknowntargetfunction\nf:X \u2192Y\n+noise\n(idealcreditapprovalformula)\nlearning\ntrainingexamples finalhypothesis\nalgorithm\nD:(x\n1\n,y\n1\n),\u00b7\u00b7\u00b7,(x\nN\n,y\nN\n)\nA\ng\u2248f\n(historicalrecordsinbank) (\u2018learned\u2019formulatobeused)\nhypothesisset\nH\n(setofcandidateformula)\nhow to at least get g \u2248 f on noisy D?\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 18/22\nLearningtoAnswerYes/No Non-SeparableData\nLine with Noise Tolerance\n\u2022 assume \u2018little\u2019 noise: y = f(x ) usually\nn n\n\u2022 if so, g \u2248 f on D \u21d4 y = g(x ) usually\nn n\n\u2022 how about\nN\n(cid:88)\nw \u2190 argmin (cid:114)y (cid:54)= sign(wTx )(cid:122)\ng n n\nw\nn=1\n\u2014NP-hard to solve, unfortunately\ncan we modify PLA to get\nan \u2018approximately good\u2019 g?\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 19/22\nLearningtoAnswerYes/No Non-SeparableData\nPocket Algorithm\nmodifyPLAalgorithm(blacklines)bykeepingbestweightsinpocket\ninitialize pocket weights w\u02c6\nFor t = 0,1,\u00b7\u00b7\u00b7\n1 find a (random) mistake of w t called (x n(t) ,y n(t) )\n2 (try to) correct the mistake by\nw \u2190 w +y x\nt+1 t n(t) n(t)\n3 if w t+1 makes fewer mistakes than w\u02c6, replace w\u02c6 by w t+1\n...until enough iterations\nreturn w\u02c6 (called w ) as g\nPOCKET\na simple modification of PLA to find\n(somewhat) \u2018best\u2019 weights\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 20/22\nReference Answer: 1\nBecause pocket need to check whether w is\nt+1\nbetter than w\u02c6 in each iteration, it is slower than\nPLA. On linear separable D, w is the\nPOCKET\nsame as w , both making no mistakes.\nPLA\nLearningtoAnswerYes/No Non-SeparableData\nFun Time\nShould we use pocket or PLA?\nSince we do not know whether D is linear separable in advance, we\nmay decide to just go with pocket instead of PLA. If D is actually linear\nseparable, what\u2019s the difference between the two?\n1 pocket on D is slower than PLA\n2 pocket on D is faster than PLA\n3 pocket on D returns a better g in approximating f than PLA\n4 pocket on D returns a worse g in approximating f than PLA\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 21/22\nLearningtoAnswerYes/No Non-SeparableData\nFun Time\nShould we use pocket or PLA?\nSince we do not know whether D is linear separable in advance, we\nmay decide to just go with pocket instead of PLA. If D is actually linear\nseparable, what\u2019s the difference between the two?\n1 pocket on D is slower than PLA\n2 pocket on D is faster than PLA\n3 pocket on D returns a better g in approximating f than PLA\n4 pocket on D returns a worse g in approximating f than PLA\nReference Answer: 1\nBecause pocket need to check whether w is\nt+1\nbetter than w\u02c6 in each iteration, it is slower than\nPLA. On linear separable D, w is the\nPOCKET\nsame as w , both making no mistakes.\nPLA\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 21/22\nLearningtoAnswerYes/No Non-SeparableData\nSummary\n1 When Can Machines Learn?\nLecture 1: The Learning Problem\nLecture 2: Learning to Answer Yes/No\nPerceptron Hypothesis Set\nhyperplanes/linear classifiers in Rd\nPerceptron Learning Algorithm (PLA)\ncorrect mistakes and improve iteratively\nGuarantee of PLA\nno mistake eventually if linear separable\nNon-Separable Data\nhold somewhat \u2018best\u2019 weights in pocket\n\u2022 next: thezoooflearningproblems\n2 Why Can Machines Learn?\n3 How Can Machines Learn?\n4 How Can Machines Learn Better?\nHsuan-TienLin (NTUCSIE) MachineLearningFoundations 22/22\n",
  "context": "age 23years\ngender female\nannualsalary NTD1,000,000",
  "source_file": "resources\\Year 4\\Machine Learning Foundations.pdf",
  "line_numbers": [
    32,
    787
  ]
}