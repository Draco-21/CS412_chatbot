{
  "title": "Perceptrons",
  "language": "python",
  "topics": [
    "machine_learning",
    "web_dev",
    "fundamentals",
    "algorithms",
    "data_structures",
    "networking",
    "database"
  ],
  "purpose": "Instructor: Yingyu Liang Perceptron Overview",
  "code": "\u2022 Previous lectures: (Principle for loss function) MLE to derive loss\n\u2022 Example: linear regression; some linear classification models\n\u2022 This lecture: (Principle for optimization) local improvement\n\u2022 Example: Perceptron; SGD\nTask\n\u2217 \ud835\udc47\n(\ud835\udc64 ) \ud835\udc65 = 0\n\u2217 \ud835\udc47\n(\ud835\udc64 ) \ud835\udc65 > 0\n\u2217 \ud835\udc47\n(\ud835\udc64 ) \ud835\udc65 < 0\n\u2217\n\ud835\udc64\nClass +1\nClass -1\nAttempt\n\u2022 Given training data \ud835\udc65 , \ud835\udc66 : 1 \u2264 \ud835\udc56 \u2264 \ud835\udc5b i.i.d. from distribution \ud835\udc37\n\ud835\udc56 \ud835\udc56\n\ud835\udc47\n\u2022 Hypothesis \ud835\udc53 \ud835\udc65 = \ud835\udc64 \ud835\udc65\n\ud835\udc64\n\ud835\udc47\n\u2022 \ud835\udc66 = +1 if \ud835\udc64 \ud835\udc65 > 0\n\ud835\udc47\n\u2022 \ud835\udc66 = \u22121 if \ud835\udc64 \ud835\udc65 < 0\n\ud835\udc47\n\u2022 Prediction: \ud835\udc66 = sign(\ud835\udc53 \ud835\udc65 ) = sign(\ud835\udc64 \ud835\udc65)\n\ud835\udc64\n\u2022 Goal: minimize classification error\nPerceptron Algorithm\n\u2022 Assume for simplicity: all \ud835\udc65 has length 1\n\ud835\udc56\nPerceptron: figure from the lecture note of Nina Balcan\nIntuition: correct the current mistake\n\u2022 If mistake on a positive example\n\ud835\udc47 \ud835\udc47 \ud835\udc47 \ud835\udc47 \ud835\udc47\n\ud835\udc64 \ud835\udc65 = \ud835\udc64 + \ud835\udc65 \ud835\udc65 = \ud835\udc64 \ud835\udc65 + \ud835\udc65 \ud835\udc65 = \ud835\udc64 \ud835\udc65 + 1\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\u2022 If mistake on a negative example\n\ud835\udc47 \ud835\udc47 \ud835\udc47 \ud835\udc47 \ud835\udc47\n\ud835\udc64 \ud835\udc65 = \ud835\udc64 \u2212 \ud835\udc65 \ud835\udc65 = \ud835\udc64 \ud835\udc65 \u2212 \ud835\udc65 \ud835\udc65 = \ud835\udc64 \ud835\udc65 \u2212 1\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61\nThe Perceptron Theorem\n\u2217\n\u2022 Suppose there exists \ud835\udc64 that correctly classifies \ud835\udc65 , \ud835\udc66\n\ud835\udc56 \ud835\udc56\n\u2217\n\u2022 W.L.O.G., all \ud835\udc65 and \ud835\udc64 have length 1, so the minimum distance of any\n\ud835\udc56\nexample to the decision boundary is\n\u2217 \ud835\udc47\n\ud835\udefe = min | \ud835\udc64 \ud835\udc65 |\n\ud835\udc56\n\ud835\udc56\n2\n1\n\u2022 Then Perceptron makes at most mistakes\n\ud835\udefe\nThe Perceptron Theorem\n\u2217\n\u2022 Suppose there exists \ud835\udc64 that correctly classifies \ud835\udc65 , \ud835\udc66\n\ud835\udc56 \ud835\udc56\n\u2217\n\u2022 W.L.O.G., all \ud835\udc65 and \ud835\udc64 have length 1, so the minimum distance of any\n\ud835\udc56\nexample to the decision boundary is\n\u2217 \ud835\udc47\n\ud835\udefe = min | \ud835\udc64 \ud835\udc65 |\n\ud835\udc56 Need not be i.i.d. !\n\ud835\udc56\n2\n1\n\u2022 Then Perceptron makes at most mistakes\n\ud835\udefe\n\ud835\udc5b\nDo not depend on , the\nlength of the data sequence!\nAnalysis\n\ud835\udc47 \u2217\n\u2022 First look at the quantity \ud835\udc64 \ud835\udc64\n\ud835\udc61\n\ud835\udc47 \u2217 \ud835\udc47 \u2217\n\u2022 Claim 1: \ud835\udc64 \ud835\udc64 \u2265 \ud835\udc64 \ud835\udc64 + \ud835\udefe\n\ud835\udc61+1 \ud835\udc61\n\u2022 Proof: If mistake on a positive example \ud835\udc65\n\ud835\udc47 \u2217 \ud835\udc47 \u2217 \ud835\udc47 \u2217 \ud835\udc47 \u2217 \ud835\udc47 \u2217\n\ud835\udc64 \ud835\udc64 = \ud835\udc64 + \ud835\udc65 \ud835\udc64 = \ud835\udc64 \ud835\udc64 + \ud835\udc65 \ud835\udc64 \u2265 \ud835\udc64 \ud835\udc64 + \ud835\udefe\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\u2022 If mistake on a negative example\n\ud835\udc47 \u2217 \ud835\udc47 \u2217 \ud835\udc47 \u2217 \ud835\udc47 \u2217 \ud835\udc47 \u2217\n\ud835\udc64 \ud835\udc64 = \ud835\udc64 \u2212 \ud835\udc65 \ud835\udc64 = \ud835\udc64 \ud835\udc64 \u2212 \ud835\udc65 \ud835\udc64 \u2265 \ud835\udc64 \ud835\udc64 + \ud835\udefe\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61\nAnalysis\nNegative since we made a\n\u2022 Next look at the quantity \ud835\udc64\n\ud835\udc61 mistake on x\n2 2\n\u2022 Claim 2: \ud835\udc64 \u2264 \ud835\udc64 + 1\n\ud835\udc61+1 \ud835\udc61\n\u2022 Proof: If mistake on a positive example \ud835\udc65\n2 2 2 2\n\ud835\udc47\n\ud835\udc64 = \ud835\udc64 + \ud835\udc65 = \ud835\udc64 + \ud835\udc65 + 2\ud835\udc64 \ud835\udc65\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61\nAnalysis: putting things together\n\ud835\udc47 \u2217 \ud835\udc47 \u2217\n\u2022 Claim 1: \ud835\udc64 \ud835\udc64 \u2265 \ud835\udc64 \ud835\udc64 + \ud835\udefe\n\ud835\udc61+1 \ud835\udc61\n2 2\n\u2022 Claim 2: \ud835\udc64 \u2264 \ud835\udc64 + 1\n\ud835\udc61+1 \ud835\udc61\nAfter \ud835\udc40 mistakes:\n\ud835\udc47 \u2217\n\u2022 \ud835\udc64 \ud835\udc64 \u2265 \ud835\udefe\ud835\udc40\n\ud835\udc40+1\n\u2022 \ud835\udc64 \u2264 \u221a\ud835\udc40\n\ud835\udc40+1\n\ud835\udc47 \u2217\n\u2022 \ud835\udc64 \ud835\udc64 \u2264 \ud835\udc64\n\ud835\udc40+1 \ud835\udc40+1\n2\n1\nSo \ud835\udefe\ud835\udc40 \u2264 \u221a\ud835\udc40, and thus \ud835\udc40 \u2264\n\ud835\udefe\nIntuition\nThe correlation gets larger. Could be:\n\ud835\udc47 \u2217 \ud835\udc47 \u2217\n\u2022 Claim 1: \ud835\udc64 \ud835\udc64 \u2265 \ud835\udc64 \ud835\udc64 + \ud835\udefe \u2217\n\ud835\udc64 \ud835\udc64\n\ud835\udc61+1 \ud835\udc61 1. gets closer to\n\ud835\udc61+1\n2 2\n2. \ud835\udc64 gets much longer\n\u2022 Claim 2: \ud835\udc64 \u2264 \ud835\udc64 + 1 \ud835\udc61+1\n\ud835\udc61+1 \ud835\udc61\nRules out the bad case \u201c2. \ud835\udc64 gets\n\ud835\udc61+1\nmuch longer\u201d\nSome side notes on Perceptron\nHistory\nFigure from Pattern Recognition and Machine Learning, Bishop\nNote: connectionism vs symbolism\n\u2022 Symbolism: AI can be achieved by representing concepts as symbols\n\u2022 Example: rule-based expert system, formal grammar\n\u2022 Connectionism: explain intellectual abilities using connections\nbetween neurons (i.e., artificial neural networks)\n\u2022 Example: perceptron, larger scale neural networks\nSymbolism example: Credit Risk Analysis\nExample from Machine learning lecture notes by Tom Mitchell\nConnectionism example\nNeuron/perceptron\nFigure from\nPattern Recognition\nand machine learning,\nBishop\nNote: connectionism v.s. symbolism\n\u2022 Formal theories of logical reasoning, grammar, and other higher\nmental faculties compel us to think of the mind as a machine for rule-\nbased manipulation of highly structured arrays of symbols. What we\nknow of the brain compels us to think of human information\nprocessing in terms of manipulation of a large unstructured set of\nnumbers, the activity levels of interconnected neurons.\n---- The Central Paradox of Cognition (Smolensky et al., 1992)\nNote: online vs batch\n\u2022 Batch: Given training data \ud835\udc65 , \ud835\udc66 : 1 \u2264 \ud835\udc56 \u2264 \ud835\udc5b , typically i.i.d.\n\ud835\udc56 \ud835\udc56\n\u2022 Online: data points arrive one by one\n\u2022 1. The algorithm receives an unlabeled example \ud835\udc65\n\ud835\udc56\n\u2022 2. The algorithm predicts a classification of this example.\n\u2022 3. The algorithm is then told the correct answer \ud835\udc66 , and update its model\n\ud835\udc56\nStochastic gradient descent (SGD)\nGradient descent\n\u0de0\n\u2022 Minimize loss \ud835\udc3f \ud835\udf03 , where the hypothesis is parametrized by \ud835\udf03\n\u2022 Gradient descent\n\u2022 Initialize \ud835\udf03\n0\n\u0de0\n\u2022 \ud835\udf03 = \ud835\udf03 \u2212 \ud835\udf02 \ud835\udefb\ud835\udc3f \ud835\udf03\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61\nStochastic gradient descent (SGD)\n\u2022 Suppose data points arrive one by one\n1\n\u0de0 \ud835\udc5b\n\u2022 \ud835\udc3f \ud835\udf03 = \u03c3 \ud835\udc59(\ud835\udf03, \ud835\udc65 , \ud835\udc66 ), but we only know \ud835\udc59(\ud835\udf03, \ud835\udc65 , \ud835\udc66 ) at time \ud835\udc61\n\ud835\udc61=1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\ud835\udc5b\n\u2022 Idea: simply do what you can based on local information\n\u2022 Initialize \ud835\udf03\n0\n\u2022 \ud835\udf03 = \ud835\udf03 \u2212 \ud835\udf02 \ud835\udefb\ud835\udc59(\ud835\udf03 , \ud835\udc65 , \ud835\udc66 )\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\nExample 1: linear regression\n1\n\ud835\udc47 \u0de0 \ud835\udc5b \ud835\udc47 2\n\u2022 Find \ud835\udc53 \ud835\udc65 = \ud835\udc64 \ud835\udc65 that minimizes \ud835\udc3f \ud835\udc53 = \u03c3 \ud835\udc64 \ud835\udc65 \u2212 \ud835\udc66\n\ud835\udc64 \ud835\udc64 \ud835\udc61=1 \ud835\udc61 \ud835\udc61\n\ud835\udc5b\n1\n\ud835\udc47 2\n\u2022 \ud835\udc59 \ud835\udc64, \ud835\udc65 , \ud835\udc66 = \ud835\udc64 \ud835\udc65 \u2212 \ud835\udc66\n\ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\ud835\udc5b\n2\ud835\udf02\n\ud835\udc61 \ud835\udc47\n\u2022 \ud835\udc64 = \ud835\udc64 \u2212 \ud835\udf02 \ud835\udefb\ud835\udc59 \ud835\udc64 , \ud835\udc65 , \ud835\udc66 = \ud835\udc64 \u2212 \ud835\udc64 \ud835\udc65 \u2212 \ud835\udc66 \ud835\udc65\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\ud835\udc5b\nExample 2: logistic regression\n\u2022 Find \ud835\udc64 that minimizes\n1 1\n\u0de0 \ud835\udc47 \ud835\udc47\n\ud835\udc3f \ud835\udc64 = \u2212 \u0dcd log\ud835\udf0e(\ud835\udc64 \ud835\udc65 ) \u2212 \u0dcd log[1 \u2212 \ud835\udf0e \ud835\udc64 \ud835\udc65 ]\n\ud835\udc61 \ud835\udc61\n\ud835\udc5b \ud835\udc5b\n\ud835\udc66 =1 \ud835\udc66 =\u22121\n\ud835\udc61 \ud835\udc61\n1\n\u0de0 \ud835\udc47\n\ud835\udc3f \ud835\udc64 = \u2212 \u0dcd log\ud835\udf0e(\ud835\udc66 \ud835\udc64 \ud835\udc65 )\n\ud835\udc61 \ud835\udc61\n\ud835\udc5b\n\ud835\udc61\n\u22121\n\ud835\udc47\n\ud835\udc59 \ud835\udc64, \ud835\udc65 , \ud835\udc66 = log\ud835\udf0e(\ud835\udc66 \ud835\udc64 \ud835\udc65 )\n\ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\ud835\udc5b\nExample 2: logistic regression\n\u2022 Find \ud835\udc64 that minimizes\n\u22121\n\ud835\udc47\n\ud835\udc59 \ud835\udc64, \ud835\udc65 , \ud835\udc66 = log\ud835\udf0e(\ud835\udc66 \ud835\udc64 \ud835\udc65 )\n\ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\ud835\udc5b\n\ud835\udf02 \ud835\udf0e \ud835\udc4e 1\u2212\ud835\udf0e \ud835\udc4e\n\ud835\udc61\n\ud835\udc64 = \ud835\udc64 \u2212 \ud835\udf02 \ud835\udefb\ud835\udc59 \ud835\udc64 , \ud835\udc65 , \ud835\udc66 = \ud835\udc64 +\n\ud835\udc66 \ud835\udc65\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\ud835\udc5b \ud835\udf0e(\ud835\udc4e)\n\ud835\udc47\nWhere \ud835\udc4e = \ud835\udc66 \ud835\udc64 \ud835\udc65\n\ud835\udc61 \ud835\udc61 \ud835\udc61\nExample 3: Perceptron\n\ud835\udc47\n\u2022 Hypothesis: \ud835\udc66 = sign(\ud835\udc64 \ud835\udc65)\n\u2022 Define hinge loss\n\ud835\udc47\n\ud835\udc59 \ud835\udc64, \ud835\udc65 , \ud835\udc66 = \u2212\ud835\udc66 \ud835\udc64 \ud835\udc65 \ud835\udd40[mistake on \ud835\udc65 ]\n\ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\u0de0 \ud835\udc47\n\ud835\udc3f \ud835\udc64 = \u2212 \u0dcd \ud835\udc66 \ud835\udc64 \ud835\udc65 \ud835\udd40[mistake on \ud835\udc65 ]\n\ud835\udc61 \ud835\udc61 \ud835\udc61\n\ud835\udc61\n\ud835\udc64 = \ud835\udc64 \u2212 \ud835\udf02 \ud835\udefb\ud835\udc59 \ud835\udc64 , \ud835\udc65 , \ud835\udc66 = \ud835\udc64 + \ud835\udf02 \ud835\udc66 \ud835\udc65 \ud835\udd40[mistake on \ud835\udc65 ]\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\nExample 3: Perceptron\n\ud835\udc47\n\u2022 Hypothesis: \ud835\udc66 = sign(\ud835\udc64 \ud835\udc65)\n\ud835\udc64 = \ud835\udc64 \u2212 \ud835\udf02 \ud835\udefb\ud835\udc59 \ud835\udc64 , \ud835\udc65 , \ud835\udc66 = \ud835\udc64 + \ud835\udf02 \ud835\udc66 \ud835\udc65 \ud835\udd40[mistake on \ud835\udc65 ]\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\u2022 Set \ud835\udf02 = 1. If mistake on a positive example\n\ud835\udc61\n\ud835\udc64 = \ud835\udc64 + \ud835\udc66 \ud835\udc65 = \ud835\udc64 + \ud835\udc65\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\n\u2022 If mistake on a negative example\n\ud835\udc64 = \ud835\udc64 + \ud835\udc66 \ud835\udc65 = \ud835\udc64 \u2212 \ud835\udc65\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\nPros & Cons\nPros:\n\u2022 Widely applicable\n\u2022 Easy to implement in most cases\n\u2022 Guarantees for many losses\n\u2022 Good performance: error/running time/memory etc.\nCons:\n\u2022 No guarantees for non-convex opt (e.g., those in deep learning)\n\u2022 Hyper-parameters: initialization, learning rate\nMini-batch\n\u2022 Instead of one data point, work with a small batch of \ud835\udc4f points\n(\ud835\udc65 \ud835\udc66 ),\u2026, (\ud835\udc65 \ud835\udc66 )\n\ud835\udc61\ud835\udc4f+1, \ud835\udc61\ud835\udc4f+1 \ud835\udc61\ud835\udc4f+\ud835\udc4f, \ud835\udc61\ud835\udc4f+\ud835\udc4f\n\u2022 Update rule\n1\n\ud835\udf03 = \ud835\udf03 \u2212 \ud835\udf02 \ud835\udefb \u0dcd \ud835\udc59 \ud835\udf03 , \ud835\udc65 , \ud835\udc66\n\ud835\udc61+1 \ud835\udc61 \ud835\udc61 \ud835\udc61 \ud835\udc61\ud835\udc4f+\ud835\udc56 \ud835\udc61\ud835\udc4f+\ud835\udc56\n\ud835\udc4f\n1\u2264\ud835\udc56\u2264\ud835\udc4f\n\u2022 Other variants: variance reduction etc.\nHomework\nHomework 1\n\u2022 Assignment online\n\u2022 Course website:\nhttp://www.cs.princeton.edu/courses/archive/spring16/cos495/\n\u2022 Piazza: https://piazza.com/princeton/spring2016/cos495\n\u2022 Due date: Feb 17th (one week)\n\u2022 Submission\n\u2022 Math part: hand-written/print; submit to TA (Office: EE, C319B)\n\u2022 Coding part: in Matlab/Python; submit the .m/.py file on Piazza\nHomework 1\n\u2022 Grading policy: every late day reduces the attainable credit for the\nexercise by 10%.\n\u2022 Collaboration:\n\u2022 Discussion on the problem sets is allowed\n\u2022 Students are expected to finish the homework by himself/herself\n\u2022 The people you discussed with on assignments should be clearly detailed:\nbefore the solution to each question, list all people that you discussed with on\nthat particular question.\n",
  "context": "Instructor: Yingyu Liang\nPerceptron\nOverview",
  "source_file": "resources\\Year 4\\Perceptrons.pdf",
  "line_numbers": [
    7,
    317
  ]
}