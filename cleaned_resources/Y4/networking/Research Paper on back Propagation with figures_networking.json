{
  "title": "Research Paper on back Propagation with figures",
  "language": "css",
  "topics": [
    "machine_learning",
    "web_dev",
    "fundamentals",
    "algorithms",
    "data_structures",
    "networking",
    "database"
  ],
  "purpose": "II.",
  "code": "valued functions. It has been widely used in classification and The network structure we use is multi layer feedforward\nregressionproblems,suchashandwrittencharacters,recogniz- networkwithbackgropapationalgorithm.Theoutputfromone\ning spoken words, and face recognition [1]. layer feeds forward into the next layer of neurons, and nodes\nGiven three data sets, the task is to recognize 10 characters between two layers are fully connected with various weights.\nin unseen test set, after training the neural network with Error is calculated by the difference of outputs from output\nlearning set and validating it by validation set. This problem layer and target outputs. Then errors are feed backward to\nis formulated as a classification problem. Artificial neural adjust weights, which are then applied to next iteration to\nnetwork is known as a good candidate to solve classification generate new network outputs.\nproblems, so we design a basic neural network and attempt to 1) Input units: Data sets in this project is formatted 12 by\nprove its effectiveness by this character recognition problem. 8 pixels, with 0 indicating an \u201coff\u201d pixel and 1 indicating an\nWe need three main steps to solve this problem. Firstly, \u201con\u201d pixel. There is an additional 10 digits vector identifying\nafter initializing the neural network, we train it with training the actual identity of this character.\nset. In each training period, all the training characters, 100 2) Hiddenunits: Neuralnetworkwithonlyonehiddenunit\nexamples of each character in this project, go through the can represent linear functions, and with more than one hidden\nnetwork and get actual network output, By comparing the unit can represent nonlinear functions. In this project, the\nproblem we need to solve is nonlinear so a couple of hidden\nunits are used here. However, choosing the number of hidden\nunits properly is deterministic in a neural network design.\nToo many hidden units do not guarantee higher network\nclassification performance, and too little hidden units may\nfail in achieve high identifying rate. Discussion about how\nto choose hidden units number is in the next section.\n3) Outputunits: Accordingtotherequirementofcharacter\nclassification target, we have 10 output units sequence repre-\nsenting each character. For example, the first character \"A\" is\nrepresentedby[1000000000],withthefirstdigitvalued\n1andtheresttobe0.Thisisalsoconsistentwiththewaythe\ngiven data sets do to represent characters.\nB. Backpropagation algorithm\nOutlining the structure of our target neural network, we\ngivepseudocodeofbackpropagationalgorithm(Algorithm1)\nused in this project. One specific details in updating weights\nis that after training stopping, last iteration\u2019s weights should\nbe stored as neural network weights, because last iteration\u2019s\nweights having the lowest error.\nIII. RESULTS\nA. Training with large iterations\nFirstly, we do an experiment with large iteration numbers,\nsaying 5000 training epochs, to have an general sense about\nerror rate. Fig 1 shows results of both validation set error and\ntraining set error with 5000 iterations. The successful rate is\n93% and training time is 13433.34 seconds. The two curves\nillustrate that training set error is always decreasing while\nvalidation set error increase after certain training epoch. Ob-\nviously, from epoch within 200, minimal error point appears.\nTobemorespecific,wedotheexperimentwith100epochs\nand get more details about error information from validation\nand training sets. The successful rate is 93.44%, and training\ntime is 102.0485 seconds. Rate is higher than training for\n5000 iterations, and training time is sharply decreasing. This\nobservation inspires us that after the turning point is reached,\nmoretrainingiterationcannothelpimprovingperformance.In\nfact, although error of training set is always decreasing with\nmoretrainingiterations,thesuccessfulratewilldecrease.This\nis called overfitting. To avoid overfitting, we use validating\n0.1\n0.08\n0.06\n0.04\n0.02\n0\n500 1000 1500 2000 2500 3000 3500 4000 4500 5000\nEpoch\nrorrE\nAlgorithm 1 Online Backpropagation Pseudo Code for the\ncharacters classification problem\nInitialize all v and w to rand (-0.01, 0.01) ih hj\nRUN =1\nWhile (RUN)\nDO{\nstore all w and v\n/* Training */\nepoch=epoch+1\nfor all (xt,rt)\u2208X in random order training\nfor all hidden nodes z (cid:16)h (cid:17)\nz =sigmoid (cid:80)d w xt\nh j=0 hj j\nfor all output nodes y\ni\ny =softmax(\n(cid:80)H\nv z )\ni h=0 ih h\nfor all weights v\nih\n\u2206v =\u03b7(rt\u2212yt)z\nih i i h\nv =v +\u2206v\nih ih ih\nfor all weights w\n(cid:16)ih (cid:17) \u2206w =\u03b7 (cid:80)K (rt\u2212yt)v z (1\u2212z )xt\nhj i=1 i i ih h h j\nw =w +\u2206w\nih ih hj\n/* Validating */\nfor all (xt,rt)\u2208X\nvalidating\nfor all hidden nodes z\n(cid:16)h (cid:17)\nz =sigmoid (cid:80)d w xt\nh j=0 hj j\nfor all output nodes y\ni\ny =softmax(\n(cid:80)H\nv z )\ni h=(cid:16)0 ih h (cid:17)\nerr(epoch)= 1(cid:80) (cid:80)K (rt\u2212yt) 2 2 x\u2208Xvalidation i=1 i i\nif err(epoch)>err(epoch\u22121)\nRUN =0\n}\n/* Testing */\nfor all (xt,rt)\u2208X\ntesting\nfor all hidden nodes z\n(cid:16)h (cid:17)\nz =sigmoid (cid:80)d w xt\nh j=0 hj j\nfor all output nodes y\ni\ny =softmax( (cid:80)H v z )\ni h=0 ih h\nif y ==r\n\u201dSuccessClassification\u201d\nelse\n\u201dFailedClassification\u201d\nset to constrain the training process. We also collect sum of\nValidation set error squared errors for each output unit in the 100 iterations, as\nTraining set error Figure 2 shows. These ten lines in Figure 3 decreases in the\nwholetrainingperiod,whichreflectthesquareerroroftraining\nset always increases as epoch number increases.\nB. Training with validation\nAccording to the above observation, we add the restraint\ncondition, that if error of validation is lower than 0.05, any\nincreasing after will terminate training process. With this\nconstrain, hopefully higher successful rate will be obtained,\nalong with training time saving. We run the experiments for\nFigure1. Errorversusweightupdates(epoch) 10 times, showing data in Table I. The average classification\n0.1\n0.08\n0.06\n0.04\n0.02\n0 10 20 30 40 50 60 70 80 90 100\nEpoch\nrorrE\nValidation set error\nTraining set error\nFigure2. Errorversusweightupdates(epoch)\n0.05\n0.04\n0.03\n0.02\n0.01\n0\n10 20 30 40 50 60 70 80 90 100\nEpoch\nrorrE\nNo.ofhiddenunits 4 10 15 20\nRate(%) 89.98 93.99 94.18 93.77\nTime(s) 44.85 32.97 44.58 55.26\nTableIII\nEFFECTOFHIDDENUNITSNUMBERINTERMSOFSUCCESSFULRATEAND\nTRAININGTIME\n0.4\n0.3\n0.2\n0.1\n0\n0 50 100 150 200 250 300 350 400\nEpoch\nFigure3. Sumofsquarederrorsforeachoutputunit\nrate is 93.988%, with average training time 32.97 seconds.\nAverage training epoch is less than 50 iterations. Validation\naffords us better network successful rate as well as optimal\ntraining time.\nFor each character, we also calculate its classification rate,\nas Table II shows. From the table, we can see that some\ncharacters have identifying rate as high as 98% ~ 99%, but\nthe worst case character F can just achieve 85.86%.\nExperimentNo. 1 2 3 4 5\nRate(%) 93.64 93.52 93.56 94.44 94.64\nTime(s) 31.16 35.98 31.66 31.11 30.97\nExperimentNo. 6 7 8 9 10\nRate(%) 94.00 94.40 93.56 93.72 94.40\nTime(s) 32.82 33.12 35.45 32.92 34.51\nTableI\nSUCCESSFULCLASSIFICATIONRATEANDTRAININGTIME\nCharacter A C D E F\nRate(%) 88.04 98.64 97.88 95.72 85.86\nCharacter G H L P R\nRate(%) 95.80 92.04 99.72 91.24 95.12\nTableII\nSUCCESSFULRATEFOREACHCHARACTERUSINGTRAINEDNEURAL\nNETWORK\nrorrE\nHidden units No. = 4\nHidden units No. = 10\nHidden units No. = 15\nHidden units No. = 20\nFigure4. Effectofhiddenunitsnumberintermsofsquareerror\nIV. DISCUSSION\nA. Effect of hidden units number\nNumber of hidden units affect neural network performance\ngreatly in terms of degree of freedom a neural network has.\nHidden units represent input of neural network by sequence\ncoding. For example, 10 inputs can be represented by at least\n4 digits because 23 < 10 < 24. But coding with 4 digits\nalso means complex coding rule, which may lead to longer\nconverging time. At very beginning of the experiments, only\n4 hidden nodes are used in the hidden layer, and is proved\nto take relatively long training time. But too many hidden\nunits will not help improve successful identifying rate. We do\nexperiments with 4, 10, 15, and 20 hidden units.\nFrom Table 3, we can see that large hidden units number\nleads to long training time, but did not improve identifying\nperformance significantly. Small hidden units number also\nextendthetrainingperiod,anddegradesuccessfulrateaswell.\n4hiddenunitstakesmorethan70iterationstoconverge,while\nthe other situations having less than 50 iterations.\nAs Figure 4 shows, low hidden units number results in\nhigher error (around 0.08), so that when there are only 4\nhidden nodes, the successful identifying rate is lower than 10\nhiddenunits.Thisiscausedbytheloseofdegreeoffreedomof\nsmall hidden node numbers. Any inaccurate representing can\ncause error increasing. Large number of nodes enable better\nerror tolerance. But increasing hidden nodes did not improve\nperformance significantly, 15 and 20 hidden units have very\nsimilarrateas10hiddenunitsbuttakesmoretimetotrain.Itis\neasy to understand that, more hidden units need more weights\nin between each two layers. So updating each weights will\ntake more time. As a result, 10 -15 hidden units may have\nboth good performance and good converging time.\n0.5\n0.4\n0.3\n0.2\n0.1\n0 0 200 400 600 800 1000\nEpoch\nrorrE\n6 by 4 inputs\n0.5\n12 by 8 inputs\n0.4\n0.3\n0.2\n0.1\n0 0 200 400 600 800 1000\nEpoch\nFigure5. Effectofdifferentinputgeneralizations\nLearningrate 0.01 0.05 0.1 0.5 0.8\nRate(%) 93.97 93.13 93.99 91.34 90.58\nTime(s) 179 32.30 32.97 92.34 141.6\nTableIV\nEFFECTOFLEARNINGRATEINTERMSOFSUCCESSFULRATEAND\nTRAININGTIME\nB. Effect of different input generalizations\nTo have a general sense about the effect of different input\ngeneralizations, we do experiments with 1000 iterations for 6\nby 4 inputs and 12 by 8 inputs separately. Results in Figure\n5 shows that lowering input generalizations will do harm to\nsuccessful rate significantly. In 6 by 4 inputs case, minimal\nerror of validation set is around 0.25, so we set 0.25 as\nthe converge threshold. The successful rate is only 61.48%,\nand training time is 20.94 seconds, which takes around 30\niterations to validate convergence. From this experiment, we\ncan draw the conclusion that lowering input generalization\nwill lose character graphic details, therefore decrease overall\nnetwork classification performance, but can improve training\ntime.\nC. Effect of learning rate\nLearning rate represents learning step size, so that large\nlearning rate implys large delta weights. Typically we choose\n0.1 as the default value. And we try learning value range\nfrom 0.05, 0.1, 0.2, 0.5 and 1.0 to evaluate its influence in\nclassification rate and training time. After run this experiment\nfor several times, we find that the minimal error of validation\nset is larger than 0.05, so 0.05 cannot guarantee convergence.\nInthiscase,weuse0.07asthenewthreshold.Forleaningrate\nof0.8,thresholdis0.08tofulfillconvergencerequirement,this\nthresholdisalsoobtainedbypre-runtheexperimentforavery\nlarge iteration (more than 10000 times).\nAs Table IV shows, smaller learning rate 0.05 has very\nsimilar performance with 0.1, but large learning rate 0.5\nand 1.0 will increase training time significantly and decrease\nsuccessful classification rate. Intuitively, smaller learning rate\nshouldachievethesameoralittlebettersuccessfulrateas0.1,\nbut takes longer time (more than 100 iterations) to converge\nsince each update step is small. Larger learning rate may\nrorrE\neta = 0.01 eta = 0.05\neta = 0.1\neta = 0.5\neta = 0.8\nFigure6. Effectoflearningrateintermsofsquareerror\nresult in coarse-grained weight updating and taking much\nlonger time to converge. The curve in Figure 6 shows heavy\nfluctuation, which is caused by large learning rate. When eta\nequals 0.01, 0.05 and 0.1, the minimal error values are very\nclose, so that in Table 4, the successful classification rates of\nthose three are also close. But curves representing 0.5 and 0.8\neta values are much higher, which mean the minimal errors\narelarger. Globaloptimal solutionis escapedfrom largescale\nweightsupdating. Sothatidentifying ratesarelower, whichis\nalso told by Table 4. Therefore, 0.1 is a proper rate for this\ndata sets and neural network training.\nD. k-fold cross validation\nIn addition to training and validating with given sets, we\nalso do k-fold data sets experiments. In k-fold approach, each\ncharacter has 100 samples from learning set and 250 samples\nfrom validating set, so it is 350 samples for each character.\nThose 350 samples are equally and randomly divided to 10\nfolds. At every iteration, 9 folds are picked as training sets\nand the left one is validating set. Ten iterations compose one\nepoch, in which each fold will play the role of validating set\nfor one time and training set for nine times. Then we use\nthe average validating error to decide converging point. After\nstopping training, all data are used as training samples for the\nlast time weights updating. According to the experiments, k-\nfold approach has successful rate as high as 96.62%. Because\none epoch in k-fold actually trained for 10 times, it is not\nreasonable to compare its epoch-error curve with the regular\nmethod.Afterrunningthisexperimentfor5times,weusethe\nmean value as the final results for k-fold method, as Figure\n7 shows. The reason for better performance of k-fold is our\ntraining set are so small, that regular method only use 100 for\ntrainingand250forvalidating,butk-foldcanuse315samples\nfor training and 35 for validating. More samples in training\nmeans more information are obtained by neural network. At\nthe mean time, training / validating sets rotation avoid over fit\nproblem since at each iteration different training / validating\ndata are used. Although k-fold method takes longer training\ntime, it is worthy the successful classification rate.\nREFERENCES\n[1] TomMitchell,McGrawHill,MachineLearning. Mcgraw-Hill\nInternationalEdit,1997\n[2] Lynne. E. Parker, Handout: Notes on Neural Networks.\nhttp://web.eecs.utk.edu/~parker/Courses/CS425-528-\nfall10/Handouts/Neural-net-notes.pdf.\nFigure7. Classificationrateandtrainingtimeofk-foldandregularapproach\nV. CONCLUSION\nIn this project, an feedforward backpropagation algorithm\nartificial neural network is implemented to identify 10 upper-\ncase characters. Three separate data sets are used as training,\nvalidating,andtestdata.Asdiscussedabove,successfulclassi-\nficationrateisabout94%,andbyusingk-foldapproachitcan\nreach as high as 96.7%. Overfitting will make classification\nrate decrease, as we have discussed in Section 3.1.\nAdditional, several phenomenon are observed in the exper-\niments. Firstly, choosing proper hidden units number is very\nimportant in network training. At the very beginning, only 4\nhidden units are used, and the converging time is quite long,\nwith low successful rate. The reason is too little hidden nodes\nlose degrees of freedom in representing the relation between\ninputs and outputs. However, increasing hidden node number\ndoes not always improve identifying accuracy and on the\notherhand,maytakelongertimetoconverge.Aftertryingfor\nseveral times, we found 10 hidden nodes can represent the 10\ncharacters in reasonable time and successful rate. So properly\nselecting number of hidden nodes is important for neural net-\nwork design. Secondly, for speed up the converging process,\nenlarge \u03b7 value properly will make converging period shorter,\nwhich facilitate observing general converging threshold. But\ntoo large \u03b7 will extend training time and skip global optimal\nvalue. Thirdly, because fluctuations in different network en-\nvironment are quite different, and sometimes are very large\nat the beginning of training periods, it is hard to determine\nwhen to stop training. So that we run each experiment for\n1000 iterations and plot all the error data to have a general\nsense about the error value, then set an approximate minimal\nvalue as converging threshold. This method can guarantee\nan acceptable classification performance, but is a little time\nconsuming.\nAt last, k-fold method is regarded as an improvement\nof regular neural network. Because our data sets are small,\nlackingofenoughdatamakestheinput-outputrelationcannot\nbefullyrepresented.Withk-foldapproach,wecanutilizemore\ndatafortraining,sothatmorerelationinformationaregrasped\nby neural network, then higher identifying rate is achieved.\n",
  "context": "II. EXPERIMENTS\nNeural network is one of the most effective learning meth-\nods to approximate real-valued, discrete-valued, and vector- A. Neural network structure",
  "source_file": "resources\\Year 4\\Research Paper on back Propagation with figures.pdf",
  "line_numbers": [
    52,
    432
  ]
}