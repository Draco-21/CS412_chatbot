{
  "title": "C++ Data Structure",
  "language": "cpp",
  "topics": [
    "machine_learning",
    "web_dev",
    "fundamentals",
    "algorithms",
    "data_structures",
    "networking",
    "database"
  ],
  "purpose": "(opendatastructures.org) and also, more importantly, on a reliable source code man- agementsite(github.com/patmorin/ods).",
  "code": "ingthatanyoneisfreetoshare: tocopy,distributeandtransmitthework;andtoremix: to\nadaptthework,includingtherighttomakecommercialuseofthework. Theonlycondi-\ntion on these rights is attribution: you must acknowledge that the derived work contains\ncodeand/ortextfromopendatastructures.org.\nAnyone can contribute corrections/fixes using the git source-code management\nsystem. Anyone can fork from the current version of the book and develop their own\nversion (for example, in another programming language). They can then ask that their\nchanges be merged back into my version. My hope is that, by doing things this way, this\nbook will continue to be a useful textbook long after my interest in the project (or my\npulse,whichevercomesfirst)haswaned.\nv\nvi\nContents\n1 Introduction 1\n1.0.1 TheNeedforEfficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.1 Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.1.1 TheQueue,Stack,andDequeInterfaces . . . . . . . . . . . . . . . . . 4\n1.1.2 TheListInterface: LinearSequences . . . . . . . . . . . . . . . . . . . 6\n1.1.3 TheUSetInterface: UnorderedSets . . . . . . . . . . . . . . . . . . . . 7\n1.1.4 TheSSetInterface: SortedSets . . . . . . . . . . . . . . . . . . . . . . 7\n1.2 MathematicalBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2.1 ExponentialsandLogarithms . . . . . . . . . . . . . . . . . . . . . . . 9\n1.2.2 Factorials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.2.3 AsymptoticNotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.2.4 RandomizationandProbability . . . . . . . . . . . . . . . . . . . . . . 15\n1.3 TheModelofComputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n1.4 Correctness,TimeComplexity,andSpaceComplexity . . . . . . . . . . . . . 17\n1.5 CodeSamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n1.6 ListofDataStructures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n1.7 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2 Array-BasedLists 27\n2.1 ArrayStack: FastStackOperationsUsinganArray . . . . . . . . . . . . . . . 29\n2.1.1 TheBasics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.1.2 GrowingandShrinking. . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n2.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.2 FastArrayStack: AnOptimizedArrayStack . . . . . . . . . . . . . . . . . . . 33\nvii\nContents Contents\n2.3 ArrayQueue: AnArray-BasedQueue . . . . . . . . . . . . . . . . . . . . . . . 34\n2.3.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2.4 ArrayDeque: FastDequeOperationsUsinganArray . . . . . . . . . . . . . . 37\n2.4.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n2.5 DualArrayDeque: BuildingaDequefromTwoStacks . . . . . . . . . . . . . . 40\n2.5.1 Balancing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n2.5.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n2.6 RootishArrayStack: ASpace-EfficientArrayStack . . . . . . . . . . . . . . . 45\n2.6.1 AnalysisofGrowingandShrinking . . . . . . . . . . . . . . . . . . . . 49\n2.6.2 SpaceUsage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n2.6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n2.6.4 ComputingSquareRoots . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n2.7 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n3 LinkedLists 57\n3.1 SLList: ASingly-LinkedList . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n3.1.1 QueueOperations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n3.1.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n3.2 DLList: ADoubly-LinkedList . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n3.2.1 AddingandRemoving . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n3.2.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n3.3 SEList: ASpace-EfficientLinkedList . . . . . . . . . . . . . . . . . . . . . . . 64\n3.3.1 SpaceRequirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n3.3.2 FindingElements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n3.3.3 AddinganElement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n3.3.4 RemovinganElement . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n3.3.5 AmortizedAnalysisofSpreadingandGathering . . . . . . . . . . . . 71\n3.3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.4 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4 Skiplists 77\n4.1 TheBasicStructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\nviii\nContents Contents\n4.2 SkiplistSSet: AnEfficientSSetImplementation . . . . . . . . . . . . . . . . 79\n4.2.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n4.3 SkiplistList: AnEfficientRandom-AccessListImplementation . . . . . . 82\n4.3.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n4.4 AnalysisofSkiplists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n4.5 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n5 HashTables 93\n5.1 ChainedHashTable: HashingwithChaining . . . . . . . . . . . . . . . . . . . 93\n5.1.1 MultiplicativeHashing . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.1.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.2 LinearHashTable: LinearProbing . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.2.1 AnalysisofLinearProbing . . . . . . . . . . . . . . . . . . . . . . . . . 102\n5.2.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n5.2.3 TabulationHashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n5.3 HashCodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n5.3.1 HashCodesforPrimitiveDataTypes . . . . . . . . . . . . . . . . . . . 107\n5.3.2 HashCodesforCompoundObjects . . . . . . . . . . . . . . . . . . . . 107\n5.3.3 HashCodesforArraysandStrings . . . . . . . . . . . . . . . . . . . . 109\n5.4 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6 BinaryTrees 115\n6.1 BinaryTree: ABasicBinaryTree . . . . . . . . . . . . . . . . . . . . . . . . . 116\n6.1.1 RecursiveAlgorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.1.2 TraversingBinaryTrees . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.2 BinarySearchTree: AnUnbalancedBinarySearchTree . . . . . . . . . . . . 120\n6.2.1 Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n6.2.2 Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n6.2.3 Removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n6.2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n6.3 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\nix\nContents Contents\n7 RandomBinarySearchTrees 131\n7.1 RandomBinarySearchTrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n7.1.1 ProofofLemma7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n7.1.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n7.2 Treap: ARandomizedBinarySearchTree . . . . . . . . . . . . . . . . . . . . 135\n7.2.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n7.3 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n8 ScapegoatTrees 149\n8.1 ScapegoatTree: ABinarySearchTreewithPartialRebuilding. . . . . . . . . 150\n8.1.1 AnalysisofCorrectnessandRunning-Time . . . . . . . . . . . . . . . 153\n8.1.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n8.2 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n9 Red-BlackTrees 159\n9.1 2-4Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n9.1.1 AddingaLeaf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n9.1.2 RemovingaLeaf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n9.2 RedBlackTree: ASimulated2-4Tree . . . . . . . . . . . . . . . . . . . . . . . 162\n9.2.1 Red-BlackTreesand2-4Trees . . . . . . . . . . . . . . . . . . . . . . . 164\n9.2.2 Left-LeaningRed-BlackTrees . . . . . . . . . . . . . . . . . . . . . . . 167\n9.2.3 Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\n9.2.4 Removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n9.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\n9.4 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n10 Heaps 183\n10.1 BinaryHeap: AnImplicitBinaryTree . . . . . . . . . . . . . . . . . . . . . . . 183\n10.1.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n10.2 MeldableHeap: ARandomizedMeldableHeap . . . . . . . . . . . . . . . . . 187\n10.2.1 Analysisofmerge(h1,h2) . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n10.2.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\nx\nContents Contents\n10.3 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n11 SortingAlgorithms 195\n11.1 Comparison-BasedSorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\n11.1.1 Merge-Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\n11.1.2 Quicksort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n11.1.3 Heap-sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n11.1.4 ALower-BoundforComparison-BasedSorting . . . . . . . . . . . . . 204\n11.2 CountingSortandRadixSort . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n11.2.1 CountingSort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n11.2.2 Radix-Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n11.3 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n12 Graphs 213\n12.1 AdjacencyMatrix: RepresentingaGraphbyaMatrix. . . . . . . . . . . . . . 215\n12.2 AdjacencyLists: AGraphasaCollectionofLists. . . . . . . . . . . . . . . . 217\n12.3 GraphTraversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n12.3.1 Breadth-FirstSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n12.3.2 Depth-FirstSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n12.4 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n13 DataStructuresforIntegers 229\n13.1 BinaryTrie: Adigitalsearchtree . . . . . . . . . . . . . . . . . . . . . . . . . 230\n13.2 XFastTrie: SearchinginDoubly-LogarithmicTime . . . . . . . . . . . . . . . 235\n13.3 YFastTrie: ADoubly-LogarithmicTimeSSet . . . . . . . . . . . . . . . . . . 238\n13.4 DiscussionandExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\nxi\nContents Contents\nxii\nChapter 1\nIntroduction\nEverycomputersciencecurriculumintheworldincludesacourseondatastructuresand\nalgorithms. Data structures are that important; they improve our quality of life and even\nsave lives on a regular basis. Many multi-million and several multi-billion dollar compa-\nnieshavebeenbuiltarounddatastructures.\nHow can this be? If we think about it for even a few minutes, we realize that we\ninteractwithdatastructuresconstantly.\n\u2022 Open a file: File system data structures are used to locate the parts of that file on\ndisk so they can be retrieved. This isn\u2019t easy; disks contain hundreds of millions of\nblocks. Thecontentsofyourfilecouldbestoredonanyoneofthem.\n\u2022 Lookupacontactonyourphone: Adatastructureisusedtolookupaphonenumber\nbased on partial information even before you finish dialing/typing. This isn\u2019t easy;\nyourphonemaycontaininformationaboutalotofpeople\u2014everyoneyouhaveever\nhadphoneoremailcontactwith\u2014andyourphonedoesn\u2019thaveaveryfastprocessor\noralotofmemory.\n\u2022 Login to your favourite social network: The network servers use your login infor-\nmation look up your account information. This isn\u2019t easy; the most popular social\nnetworkshavehundredsofmillionsofactiveusers.\n\u2022 Do a web search: The search engine uses data structures to find the web pages con-\ntainingyoursearchterms. Thisisn\u2019teasy;thereareover8.5billionwebpagesonthe\nInternetandeachpagecontainsalotofpotentialsearchterms.\n\u2022 Phone emergency services (9-1-1): The emergency services network looks up your\nphone number in a data structure that maps phone numbers to addresses so that\n1\n1.Introduction\npolicecars,ambulances,orfiretruckscanbesenttherewithoutdelay. Thisisimpor-\ntant; the person making the call may not be able to provide the exact address they\narecallingfromandadelaycanmeanthedifferencebetweenlifeordeath.\n1.0.1 TheNeedforEfficiency\nInthenextsection,welookattheoperationssupportedbythemostcommonlyuseddata\nstructures. Anyone with even a little bit of programming experience will see that these\noperations are not hard to implement correctly. We can store the data in an array or a\nlinked list and each operation can be implemented by iterating over all the elements of\nthearrayorlistandpossiblyaddingorremovinganelement.\nThis kind of implementation is easy, but not very efficient. Does it really mat-\nter? Computers are getting faster and faster. Maybe the obvious implementation is good\nenough. Let\u2019sdosomeback-of-the-envelopecalculationstofindout.\nNumber of operations: Imagineanapplicationwithamoderately-sizeddataset, sayof\nonemillion(106),items. Itisreasonable,inmostapplications,toassumethattheapplica-\ntion will want to look up each item at least once. This means we can expect to do at least\none million (106) lookups in this data. If each of these 106 lookups inspects each of the\n106 items,thisgivesatotalof106 106=1012 (onethousandbillion)inspections.\n\u00d7\nProcessorspeeds: Atthetimeofwriting,evenaveryfastdesktopcomputercannotdo\nmore than one billion (109) operations per second.1 This means that this application will\ntakeatleast1012/109 =1000seconds, orroughly16minutesand40seconds. 16minutes\nis an eon in computer time, but a person might be willing to put up with it (if they were\nheadedoutforacoffeebreak).\nBiggerdatasets: NowconsideracompanylikeGoogle,thatindexesover8.5billionweb\npages. By our calculations, doing any kind of query over this data would take at least 8.5\nseconds. Wealreadyknowthatthisisn\u2019tthecase;websearchescompleteinmuchlessthan\n8.5 seconds, and they do much more complicated queries than just asking if a particular\npageisintheirlistofindexedpages. Atthetimeofwriting,Googlereceivesapproximately\n4,500 queries per second, meaning that they would require at least 4,500 8.5 = 38,250\n\u00d7\nveryfastserversjusttokeepup.\n1Computerspeedsareatmostafewgigahertz(billionsofcyclespersecond)andeachoperationtypically\ntakesafewcycles.\n2\n1.Introduction\nThesolution: Theseexamplestellusthattheobviousimplementationsofdatastructures\ndo not scale well when the number of items, n, in the data structure and the number of\noperations, m, performed on the data structure are both large. In these cases, the time\n(measuredin,say,machineinstructions)isroughlyn m.\n\u00d7\nThe solution, of course, is to carefully organize data within the data structure so\nthatnoteveryoperationrequiresinspectingeverydataitem. Althoughitsoundsimpossi-\nble at first, we will see data structures where a search requires looking at only 2 items on\naverage, independent of the number of items stored in the data structure. In our billion\ninstruction per second computer it takes only 0.000000002 seconds to search in a data\nstructure containing a billion items (or a trillion, or a quadrillion, or even a quintillion\nitems).\nWe will also see implementations of data structures that keep the items in sorted\norder, where the number of items inspected during an operation grows very slowly as a\nfunction of the number of items in the data structure. For example, we can maintain a\nsorted set of one billion items while inspecting at most 60 items during any operation.\nInourbillioninstructionpersecondcomputer,theseoperationstake0.00000006seconds\neach.\nThe remainder of this chapter briefly reviews some of the main concepts used\nthroughout the rest of the book. Section 1.1 describes the interfaces implemented by all\nthedatastructuresdescribedinthisbookandshouldbeconsideredrequiredreading. The\nremainingsectionsdiscuss:\n\u2022 somemathematicalbackgroundincludingexponentials,logarithms,factorials,asymp-\ntotic(big-Oh)notation,probability,andrandomization;\n\u2022 themodelofcomputation;\n\u2022 correctness,runningtime,andspace;\n\u2022 anoverviewoftherestofthechapters;and\n\u2022 thesamplecodeandtypesettingconventions.\nAreaderwithorwithoutabackgroundintheseareascaneasilyskipthemnowandcome\nbacktothemlaterifnecessary.\n3\n1.Introduction 1.1.Interfaces\n1.1 Interfaces\nIn discussing data structures, it is important to understand the difference between a data\nstructure\u2019sinterfaceanditsimplementation. Aninterfacedescribeswhatadatastructure\ndoes,whileanimplementationdescribeshowthedatastructuredoesit.\nAn interface, sometimes also called an abstract data type, defines the set of opera-\ntions supported by a data structure and the semantics, or meaning, of those operations.\nAninterfacetellsusnothingabouthowthedatastructureimplementstheseoperations,it\nonly provides the list of supported operations along with specifications about what types\nofargumentseachoperationacceptsandthevaluereturnedbyeachoperation.\nAdatastructureimplementationontheotherhand,includestheinternalrepresen-\ntationofthedatastructureaswellasthedefinitionsofthealgorithmsthatimplementthe\noperationssupportedbythedatastructure. Thus,therecanbemanyimplementationsofa\nsingleinterface. Forexample,inChapter2,wewillseeimplementationsoftheListinter-\nfaceusingarraysandinChapter3wewillseeimplementationsoftheListinterfaceusing\npointer-baseddatastructures. Eachimplementsthesameinterface, List, butindifferent\nways.\n1.1.1 TheQueue,Stack,andDequeInterfaces\nTheQueueinterfacerepresentsacollectionofelementstowhichwecanaddelementsand\nremovethenextelement. Moreprecisely,theoperationssupportedbytheQueueinterface\nare\n\u2022 add(x): addthevaluextotheQueue\n\u2022 remove(): removethenext(previouslyadded)value,y,fromtheQueueandreturny\nNotice that the remove() operation takes no argument. The Queue\u2019s queueing discipline\ndecideswhichelementshouldberemoved. Therearemanypossiblequeueingdisciplines,\nthemostcommonofwhichincludeFIFO,priority,andLIFO.\nAFIFO(first-in-first-out)Queue,illustratedinFigure1.1,removesitemsinthesame\norder they were added, much in the same way a queue (or line-up) works when checking\nout at a cash register in a grocery store. This is the most common kind of Queue so the\nqualifier FIFO is often ommitted. In other texts, the add(x) and remove() operations on a\nFIFOQueueareoftencalledenqueue(x)anddequeue(),respectively.\nA priority Queue, illustrated in Figure 1.2, always removes the smallest element\nfrom the Queue, breaking ties arbitrarily. This is similar to the way patients are triaged\n4\n1.Introduction 1.1.Interfaces\nx\n\u00b7\u00b7\u00b7\nadd(x)/enqueue(x) remove()/dequeue()\nFigure1.1: AFIFOQueue.\nremove()/deleteMin()\nadd(x)\n3\n6\nx\n13\n16\nFigure1.2: ApriorityQueue.\nin a hospital emergency room. As patients arrive they are evaluated and then placed in a\nwaiting room. When a doctor becomes available they first treat the patient with the most\nlife-threatening condition. The remove(x) operation on a priority Queue is usually called\ndeleteMin()inothertexts.\nAverycommonqueueingdisciplineistheLIFO(last-in-first-out)discipline,illus-\ntrated in Figure 1.3. In a LIFO Queue, the most recently added element is the next one\nremoved. Thisisbestvisualizedintermsofastackofplates; platesareplacedonthetop\nofthestackandalsoremovedfromthetopofthestack. Thisstructureissocommonthat\nit gets its own name: Stack. Often, when discussing a Stack, the names of add(x) and\nremove() are changed to push(x) and pop(); this is to avoid confusing the LIFO and FIFO\nqueueingdisciplines.\nA Deque is a generalization of both the FIFO Queue and LIFO Queue (Stack). A\nDequerepresentsasequenceofelements,withafrontandaback. Elementscanbeadded\nat the front of the sequence or the back of the sequence. The names of the operations on\nadd(x)/push(x)\nx\n\u00b7\u00b7\u00b7\nremove()/pop()\nFigure1.3: Astack.\n5\n1.Introduction 1.1.Interfaces\n0 1 2 3 4 5 6 7 n 1\n\u00b7\u00b7\u00b7 \u2212\na b c d e f b k c\n\u00b7\u00b7\u00b7\nFigure 1.4: A List represents a sequence indexed by 0,1,2,...,n. In this List a call to\nget(2)wouldreturnthevaluec.\naDequeareself-explanatory: addFirst(x),removeFirst(),addLast(x),andremoveLast().\nNoticethataStackcanbeimplementedusingonlyaddFirst(x)andremoveFirst()while\naFIFOQueuecanbeimplementedusingonlyaddLast(x)andremoveFirst().\n1.1.2 TheListInterface: LinearSequences\nThis book will talk very little about the FIFO Queue, Stack, or Deque interfaces. This\nis because these interfaces are subsumed by the List interface. A List, illustrated in\nFigure 1.4, represents a sequence, x ,...,x , of values. The List interface includes the\n0 n 1\n\u2212\nfollowingoperations:\n1. size(): returnn,thelengthofthelist\n2. get(i): returnthevaluex\ni\n3. set(i,x): setthevalueofx equaltox\ni\n4. add(i,x): addxatpositioni,displacingx ,...,x ;\ni n 1\n\u2212\nSetx =x ,forallj n 1,...,i ,incrementn,andsetx =x\nj+1 j i\n\u2208{ \u2212 }\n5. remove(i)removethevaluex ,displacingx ,...,x ;\ni i+1 n 1\n\u2212\nSetx =x ,forallj i,...,n 2 anddecrementn\nj j+1\n\u2208{ \u2212 }\nNoticethattheseoperationsareeasilysufficienttoimplementtheDequeinterface:\naddFirst(x) add(0,x)\n\u21d2\nremoveFirst() remove(0)\n\u21d2\naddLast(x) add(size(),x)\n\u21d2\nremoveLast() remove(size() 1)\n\u21d2 \u2212\nAlthoughwewillnormallynotdiscusstheStack,DequeandFIFOQueueinterfaces\nin subsequent chapters, the terms Stack and Deque are sometimes used in the names of\ndatastructuresthatimplementtheListinterface. Whenthishappens,itistohighlightthe\n6\n1.Introduction 1.1.Interfaces\nfactthatthesedatastructurescanbeusedtoimplementtheStackorDequeinterfacevery\nefficiently. For example, the ArrayDeque class is an implementation of the List interface\nthatimplementsalltheDequeoperationsinconstanttimeperoperation.\n1.1.3 TheUSetInterface: UnorderedSets\nThe USet interface represents an unordered set of unique elements, mimicking a mathe-\nmatical set. A USet contains n distinct elements; no element appears more than once; the\nelementsareinnospecificorder. AUSetsupportsthefollowingoperations:\n1. size(): returnthenumber,n,ofelementsintheset\n2. add(x): addtheelementxtothesetifnotalreadypresent;\nAdd x to the set provided that there is no element y in the set such that x equals y.\nReturntrueifxwasaddedtothesetandfalseotherwise.\n3. remove(x): removexfromtheset;\nFindanelementyinthesetsuchthatxequalsyandremovey. Returny, ornullif\nnosuchelementexists.\n4. find(x): findxinthesetifitexists;\nFindanelementyinthesetsuchthatyequalsx. Returny,ornullifnosuchelement\nexists.\nThesedefinitionsareabitfussyaboutdistinguishingx,theelementweareremov-\ning or finding, from y, the element we remove or find. This is because x and y might\nactuallybedistinctobjectsthatareneverthelesstreatedasequal. Thisisaveryusefuldis-\ntinction since it allows for the creation of dictionaries or maps that map keys onto values.\nThis is done by creating a compound object called a Pair that contains a key and a value.\nTwo Pairs are treated as equal if their keys are equal. By storing Pairs in a USet, we can\nfind the value associated with any key k by creating a Pair, x, with key k and using the\nfind(x)method.\n1.1.4 TheSSetInterface: SortedSets\nTheSSetinterfacerepresentsasortedsetofelements. AnSSetstoreselementsfromsome\ntotalorder,sothatanytwoelementsxandycanbecompared. Incodeexamples,thiswill\n7\n1.Introduction 1.2.MathematicalBackground\nbedonewithamethodcalledcompare(x,y)inwhich\n<0 ifx<y\ncompare(x,y)\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2 >\n=\n0\n0 i\ni\nf\nfx\nx\n>\n=\ny\ny\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nAnSSetsupportsthesize(),add(x),andremove(x)methodswithexactlythesameseman-\ntics as in the USet interface. The difference between a USet and an SSet is in the find(x)\nmethod:\n4. find(x): locatexinthesortedset;\nFind the smallest element y in the set such that y x. Return y or null if no such\n\u2265\nelementexists.\nThisversionofthefind(x)operationissometimesreferredtoasasuccessorsearch.\nItdiffersinafundamentalwayfromUSet.find(x)sinceitreturnsameaningfulresulteven\nwhenthereisnoelementinthesetthatisequaltox.\nThe distinction between the USet and SSet find(x) operations is very important\nandisveryoftenmissed. TheextrafunctionalityprovidedbyanSSetusuallycomeswith\napricethatincludesbothalargerrunningtimeandahigherimplementationcomplexity.\nFor example, most of the SSet implementations discussed in this book all have find(x)\noperations with running times that are logarithmic in the size of the set. On the other\nhand, the implementation of a USet as a ChainedHashTable in Chapter 5 has a find(x)\noperation that runs in constant expected time. When choosing which of these structures\nto use, one should always use a USet unless the extra functionality offered by an SSet is\nreallyneeded.\n1.2 Mathematical Background\nIn this section, we review some mathematical notations and tools used throughout this\nbook, including logarithms, big-Oh notation, and probability theory. This is intended to\nbe a review of these topics, not an introduction. Any reader who feels they are missing\nthisbackgroundisencouragedtoread,anddoexercisesfrom,theappropriatesectionsof\ntheverygood(andfree)textbookonmathematicsforcomputerscience[45].\n8\n1.Introduction 1.2.MathematicalBackground\n1.2.1 ExponentialsandLogarithms\nTheexpressionbx denotesthenumberb raisedtothepowerofx. Ifx isapositiveinteger,\nthenthisisjustthevalueofb multipliedbyitselfx 1times:\n\u2212\nbx =b b b .\n\u00d7 \u00d7\u00b7\u00b7\u00b7\u00d7\nx\nWhenx isanegativeinteger,bx =1/b x.(cid:124)Whe(cid:123)n(cid:122)x=(cid:125)0,bx =1. Whenb isnotaninteger,we\n\u2212\n(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)\ncan still define exponentiation in terms of the exponential function ex (see below), which\nisitselfdefinedintermsoftheexponentialseries,butthisisbestlefttoacalculustext.\nIn this book, the expression log k denotes the base-b logarithm of k. That is, the\nb\nuniquevaluex thatsatisfies\nbx =k .\nMost of the logarithms in this book are base 2 (binary logarithms), in which case we drop\nthebase,sothatlogk isshorthandforlog k.\n2\nAn informal, but useful, way to think about logarithms is to think of log k as the\nb\nnumber of times we have to divide k by b before the result is less than or equal to 1. For\nexample, when one does binary search, each comparison reduces the number of possible\nanswersbyafactorof2. Thisisrepeateduntilthereisatmostonepossibleanswer. There-\nfore,thenumberofcomparisondonebybinarysearchwhenthereareinitiallyatmostn+1\npossibleanswersisatmost log (n+1) .\n(cid:100) 2 (cid:101)\nAnotherlogarithmthatcomesupseveraltimesinthisbookisthenaturallogarithm.\nHereweusethenotationlnk todenotelog k,wheree\u2014Euler\u2019sconstant\u2014isgivenby\ne\n1 n\ne= lim 1+ 2.71828 .\nn n \u2248\n\u2192\u221e\n(cid:18) (cid:19)\nThe natural logarithm comes up frequently because it is the value of a particularly com-\nmonintegral:\nk\n1/xdx=lnk .\n1\n(cid:90)\nTwo of the most common manipulations we do with logarithms are removing them from\nanexponent:\nblog b k =k\nandchangingthebaseofalogarithm:\nlog k\nlog k= a .\nb log b\na\n9\n1.Introduction 1.2.MathematicalBackground\nForexample, wecanusethesetwomanipulationsto comparethenaturalandbinarylog-\narithms\nlogk logk\nlnk= = =(ln2)(logk) 0.693147logk .\nloge (lne)/(ln2) \u2248\n1.2.2 Factorials\nInoneortwoplacesinthisbook,thefactorialfunctionisused. Foranon-negativeinteger\nn,thenotationn!(pronounced\u201cnfactorial\u201d)isdefinedtomean\nn!=1 2 3 n .\n\u00b7 \u00b7 \u00b7\u00b7\u00b7\u00b7\u00b7\nFactorials appear because n! counts the number of distinct permutations, i.e., orderings,\nofndistinctelements. Forthespecialcasen=0,0!isdefinedas1.\nThequantityn!canbeapproximatedusingStirling\u2019sApproximation:\nn n\nn!=\u221a2\u03c0n e\u03b1(n) ,\ne\n(cid:18) (cid:19)\nwhere\n1 1\n<\u03b1(n)< .\n12n+1 12n\nStirling\u2019sApproximationalsoapproximatesln(n!):\n1\nln(n!)=nlnn n+ ln(2\u03c0n)+\u03b1(n)\n\u2212 2\n(In fact, Stirling\u2019s Approximation is most easily proven by approximating ln(n!) = ln1+\nn\nln2+ +lnnbytheintegral lnndn=nlnn n+1.)\n\u00b7\u00b7\u00b7 1 \u2212\nRelated to the factoria(cid:82)l function are the binomial coefficients. For a non-negative\nintegernandanintegerk 0,...,n ,thenotation n denotes:\n\u2208{ } k\nn n! (cid:0) (cid:1)\n= .\nk k!(n k)!\n(cid:32) (cid:33) \u2212\nThebinomialcoefficient n (pronounced\u201cnchoosek\u201d)countsthenumberofsubsetsofan\nk\nnelementsetthathavesizek,i.e.,thenumberofwaysofchoosingk distinctintegersfrom\n(cid:0) (cid:1)\ntheset 1,...,n .\n{ }\n1.2.3 AsymptoticNotation\nWhenanalyzingdatastructuresinthisbook,wewillwanttotalkabouttherunningtimes\nof various operations. The exact running times will, of course, vary from computer to\ncomputer and even from run to run on an individual computer. When we talk about the\n10\n1.Introduction 1.2.MathematicalBackground\nrunning time of an operation we are referring to the number of computer instructions\nperformed during the operation. Even for simple code, this quantity can be difficult to\ncompute exactly. Therefore, instead of analyzing running times exactly, we will use the\nso-calledbig-Ohnotation: Forafunctionf(n),O(f(n))denotesasetoffunctions,\nO(f(n))= g(n):thereexistsc>0,andn suchthatg(n) c f(n)foralln n .\n0 0\n{ \u2264 \u00b7 \u2265 }\nThinkinggraphically,thissetconsistsofthefunctionsg(n)wherec f(n)startstodominate\n\u00b7\ng(n)whennissufficientlylarge.\nWegenerallyuseasymptoticnotationtosimplifyfunctions. Forexample,inplace\nof5nlogn+8n 200wecanwrite,simply,O(nlogn). Thisisprovenasfollows:\n\u2212\n5nlogn+8n 200 5nlogn+8n\n\u2212 \u2264\n5nlogn+8nlogn forn 2(sothatlogn 1)\n\u2264 \u2265 \u2265\n13nlogn\n\u2264\nwhichdemonstratesthatthefunctionf(n)=5nlogn+8n 200isinthesetO(nlogn)using\n\u2212\ntheconstantsc=13andn =2.\n0\nThereareanumberofusefulshortcutswhenusingasymptoticnotation. First:\nO(nc 1) O(nc 2) ,\n\u2282\nforanyc <c . Second: Foranyconstantsa,b,c>0,\n1 2\nO(a) O(logn) O(nb) O(cn) .\n\u2282 \u2282 \u2282\nTheseinclusionrelationscanbemultipliedbyanypositivevalue, andtheystillhold. For\nexample,multiplyingbynyields:\nO(n) O(nlogn) O(n1+b) O(ncn) .\n\u2282 \u2282 \u2282\nContinuing in a long and distinguished tradition, we will abuse this notation by\nwriting things like f (n) = O(f(n)) when what we really mean is f (n) O(f(n)). We\n1 1\n\u2208\nwill also make statements like \u201cthe running time of this operation is O(f(n))\u201d when this\nstatement should be \u201cthe running time of this operation is a member of O(f(n)).\u201d These\nshortcuts are mainly to avoid awkward language and to make it easier to use asymptotic\nnotationwithinstringsofequations.\nAparticularlystrangeexampleofthiscomeswhenwewritestatementslike\nT(n)=2logn+O(1) .\n11\n1.Introduction 1.2.MathematicalBackground\nAgain,thiswouldbemorecorrectlywrittenas\nT(n) 2logn+[somememberofO(1)] .\n\u2264\nTheexpressionO(1)alsobringsupanotherissue. Sincethereisnovariableinthis\nexpression,itmaynotbeclearwhichvariableisgettingarbitrarilylarge. Withoutcontext,\nthere is no way to tell. In the example above, since the only variable in the rest of the\nequation is n, we can assume that this should be read as T(n) = 2logn+O(f(n)), where\nf(n)=1.\nBig-Ohnotationisnotneworuniquetocomputerscience. Itwasusedbynumber\ntheorist Paul Bachmann as early as 1894, but it is immensely useful in describing the\nrunningtimesofcomputeralgorithms. Considerapieceofcodelikethefollowing:\nSimple\nvoid snippet() {\nfor (int i = 0; i < n; i++)\na[i] = i;\n}\nOneexecutionofthismethodinvolves\n\u2022 1assignment(inti=0),\n\u2022 n+1comparisons(i<n),\n\u2022 nincrements(i++),\n\u2022 narrayoffsetcalculations(a[i]),and\n\u2022 nindirectassignments(a[i]=i).\nSowecouldwritethisrunningtimeas\nT(n)=a+b(n+1)+cn+dn+en ,\nwhere a, b, c, d, and e are constants that depend on the the machine running the code\nandrepresentthetimetoperformassignments,comparisons,incrementoperations,array\noffset calculations, and indirect assignments, respectively. If this is the expression for the\nrunningtimeoftwolinesofcode,thenclearlythiskindofanalysiswillnotbetractableto\ncomplicatedcodeoralgorithms. Usingbig-Ohnotation,therunningtimesimplifiesdown\nto\nT(n)=O(n) .\n12\n1.Introduction 1.2.MathematicalBackground\nNotonlyisthismorecompact,butitalsogivesnearlyasmuchinformation. Thefactthat\nthe running time depends on the constants a, b, c, d, and e in the above example means\nthat, in general, it will not be possible to compare two running times to know which is\nfaster without knowing the values of these constants. Even if we go through the effort of\ndetermining these constants (say, through timing tests) then our conclusion will only be\nvalidforthemachinewerunourtestson.\nBig-Oh notation allows us to reason at a much higher level, making it possible\nto analyze much more complicated functions. If two algorithms have the same big-Oh\nrunning time, then we won\u2019t know which is faster, and there may not be a clear winner.\nOne may be faster on one machine and the other may be faster on a different machine.\nHowever, if the two algorithms have a demonstrably different big-Oh running time, then\nwecanbecertainthattheonewiththesmallerrunningtimewillbefasterforlargeenough\nvaluesofn.\nAn example of how big-Oh notation allows us to compare two different functions\nis shown in Figure 1.5, which compares the rate of grown of f (n) = 15n versus f (n) =\n1 2\n2nlogn. It might be that f (n) is the running time of a complicated linear time algorithm\n1\nwhile f (n) is the running time of a considerably simpler algorithm based on the divide-\n2\nand-conquer paradigm. This illustrates that, although f (n) is greater than f (n) for small\n1 2\nvalues of n, the opposite is true for large values of n. Eventually f (n) wins out, by an\n1\nincreasinglywidemargin. Analysisusingbig-Ohnotationtoldusthatthiswouldhappen,\nsinceO(n) O(nlogn).\n\u2282\nIn a few cases, we will use asymptotic notation on functions with more than one\nvariable. There seems to be no standard for this, but for our purposes, the following\ndefinitionissufficient:\ng(n ,...,n ):thereexistsc>0,andzsuchthat\n1 k\nO(f(n 1 ,...,n k ))=\uf8f1 g(n 1 ,...,n k ) c f(n 1 ,...,n k ) \uf8fc .\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\nforalln\n1\n,\n\u2264\n...,n\n\u00b7\nk\nsuchthatg(n\n1\n,...,n\nk\n)\n\u2265\nz\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 \uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe\nThis definition captures the situation we really care about: when the arguments n ,...,n\n1 k\nmake g take on large values. This agrees with the univariate definition of O(f(n)) when\nf(n)isanincreasingfunctionofn. Thereadershouldbewarnedthat,althoughthisworks\nfor our purposes, other texts may treat multivariate functions and asymptotic notation\ndifferently.\n13\n1.Introduction 1.2.MathematicalBackground\n1600\n1400\n1200\n1000\n800\n600\n400\n200\n0\n10 20 30 40 50 60 70 80 90 100\n)n(f\n15n\n2nlogn\nn\n300000\n250000\n200000\n150000\n100000\n50000\n0\n0 1000 2000 3000 4000 5000 6000 7000 8000 900010000\n)n(f\n15n\n2nlogn\nn\nFigure1.5: Plotsof15nversus2nlogn.\n14\n1.Introduction 1.2.MathematicalBackground\n1.2.4 RandomizationandProbability\nSome of the data structures presented in this book are randomized; they make random\nchoices that are independent of the data being stored in them or the operations being\nperformed on them. For this reason, performing the same set of operations more than\nonceusingthesestructurescouldresultindifferentrunningtimes. Whenanalyzingthese\ndatastructuresweareinterestedintheiraverageorexpectedrunningtimes.\nFormally, the running time of an operation on a randomized data structure is a\nrandomvariableandwewanttostudyitsexpectedvalue. ForadiscreterandomvariableX\ntakingonvaluesinsomecountableuniverseU,theexpectedvalueofX,denotedbyE[X],\nisgiventhebytheformula\nE[X]= x Pr X =x .\n\u00b7 { }\nx U\n(cid:88)\u2208\nHere Pr denotes the probability that the event occurs. In all the examples in this\n{E} E\nbook, these probabilities are only with respect to whatever random choices are made by\ntherandomizeddatastructure;thereisnoassumptionthatthedatastoredinthestructure\nisrandomorthatthesequenceofoperationsperformedonthedatastructureisrandom.\nOne of the most important properties of expected values is linearity of expectation:\nForanytworandomvariablesX andY,\nE[X+Y]=E[X]+E[Y] .\nMoregenerally,foranyrandomvariablesX ,...,X ,\n1 k\nk k\nE X = E[X ] .\nk i\n\uf8ee \uf8f9\ni=1 i=1\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 (cid:88) \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb (cid:88)\nLinearity of expectation allows us to break down complicated random variables (like the\nleft hand sides of the above equations) into sums of simpler random variables (the right\nhandsides).\nA useful trick, that we will use repeatedly, is that of defining indicator random\nvariables. Thesebinaryvariablesareusefulwhenwewanttocountsomethingandarebest\nillustrated by an example. Suppose we toss a fair coin k times and we want to know the\nexpected number of times the coin comes up heads. Intuitively, we know the answer is\n15\n1.Introduction 1.2.MathematicalBackground\nk/2,butifwetrytoproveitusingthedefinitionofexpectedvalue,weget\nk\nE[X]= i Pr X =i\n\u00b7 { }\ni=0\n(cid:88)\nk\nk\n= i /2k\n\u00b7 i\ni=0 (cid:32) (cid:33)\n(cid:88)\nk 1\n\u2212 k 1\n=k \u2212 /2k\n\u00b7 i\ni=0(cid:32) (cid:33)\n(cid:88)\n=k/2 .\nThis requires that we know enough to calculate that Pr X =i = k /2k, that we know the\n{ } i\nbinomialidentityi k\ni\n=k k\n\u2212i\n1 ,andthatweknowthebinomialide\n(cid:0)\nn\n(cid:1)\ntity k\ni=0\nk\ni\n=2k.\nUsing indicator variables and linearity of expectation makes things much easier:\n(cid:0) (cid:1) (cid:0) (cid:1) (cid:80) (cid:0) (cid:1)\nForeachi 1,...,k ,definetheindicatorrandomvariable\n\u2208{ }\n1 iftheithcointossisheads\nI =\ni\n\uf8f1\n0 otherwise.\n\uf8f4\uf8f4\uf8f4\uf8f2\nThen\n\uf8f4\uf8f4\uf8f4\uf8f3\nE[I ]=(1/2)1+(1/2)0=1/2 .\ni\nNow,X = k I ,so\ni=1 i\n(cid:80)\nk\nE[X]=E I\ni\n\uf8ee \uf8f9\ni=1\nk\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 (cid:88) \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n= E[I ]\ni\ni=1\n(cid:88)\nk\n= 1/2\ni=1\n(cid:88)\n=k/2 .\nThis is a bit more long-winded, but doesn\u2019t require that we know any magical identities\nor compute any non-trivial probabilities. Even better: It agrees with the intuition that\nwe expect half the coins to come up heads precisely because each individual coin has\nprobability1/2ofcomingupheads.\n16\n1.Introduction 1.3.TheModelofComputation\n1.3 The Model of Computation\nInthisbook,wewillanalyzethetheoreticalrunningtimesofoperationsonthedatastruc-\ntures we study. To do this precisely, we need a mathematical model of computation. For\nthis,weusethew-bitword-RAM model. RAMstandsforRandomAccessMachine. Inthis\nmodel,wehaveaccesstoarandomaccessmemoryconsistingofcells,eachofwhichstores\naw-bitword. Thisimpliesamemorycellcanrepresent,forexample,anyintegerintheset\n0,...,2w 1 .\n{ \u2212 }\nIn the word-RAM model, basic operations on words take constant time. This in-\ncludesarithmeticoperations(+, , ,/,%),comparisons(<,>,=, , ),andbitwiseboolean\n\u2212 \u2217 \u2264 \u2265\noperations(bitwise-AND,OR,andexclusive-OR).\nAnycellcanbereadorwritteninconstanttime. Ourcomputer\u2019smemoryisman-\naged by a memory management system from which we can allocate or deallocate a block\nofmemoryofanysizewelike. Allocatingablockofmemoryofsizek takesO(k)timeand\nreturns a reference to the newly-allocated memory block. This reference is small enough\ntoberepresentedbyasingleword.\nTheword-size,w,isaveryimportantparameterofthismodel. Theonlyassumption\nwe will make on w is that it is at least w logn, where n is the number of elements stored\n\u2265\nin any of our data structures. This is a fairly modest assumption, since otherwise a word\nisnotevenbigenoughtocountthenumberofelementsstoredinthedatastructure.\nSpaceismeasuredinwordssothat,whenwetalkabouttheamountofspaceused\nbyadatastructure,wearereferringtothenumberofwordsofmemoryusedbythestruc-\nture. AllourdatastructuresstorevaluesofagenerictypeTandweassumeanelementof\ntypeToccupiesonewordofmemory.\nThe w-bit word-RAM model is a fairly close match for modern desktop computers\nwhen w = 32 or w = 64. The data structures presented in this book don\u2019t use any special\ntricksthatarenotimplementableinC++onmostarchitectures.\n1.4 Correctness, Time Complexity, and Space Complexity\nWhen studying the performance of a data structure, there are three things that matter\nmost:\nCorrectness: Thedatastructureshouldcorrectlyimplementitsinterface.\nTimecomplexity: The running times of operations on the data structure should be as\nsmallaspossible.\n17\n1.Introduction 1.4.Correctness,TimeComplexity,andSpaceComplexity\nSpacecomplexity: Thedatastructureshoulduseaslittlememoryaspossible.\nInthisintroductorytext,wewilltakecorrectnessasagiven;wewon\u2019tconsiderdata\nstructures that give incorrect answers to queries or that don\u2019t properly perform updates.\nWe will, however, see data structures that make an extra effort to keep space usage to a\nminimum. Thiswon\u2019tusuallyaffectthe(asymptotic)runningtimesofoperations,butcan\nmakethedatastructuresalittleslowerinpractice.\nWhen studying running times in the context of data structure we tend to come\nacrossthreedifferentkindsofrunningtimeguarantees:\nWorst-caserunningtimes: These are the strongest kind of running time guarantees. If\na data structure operation has a worst-case running time of f(n), then one of these\noperationsnevertakeslongerthanf(n)time.\nAmortizedrunningtimes: Ifwesaythattheamortizedrunningtimeofanoperationina\ndata structure is f(n), this means that the cost of a typical operation is at most f(n).\nMore precisely, if a data structure has an amortized running time of f(n), then a\nsequenceofmoperationstakesatmostmf(n)time. Someindividualoperationsmay\ntake more than f(n) time but the average, over the entire sequence of operations, is\natmostf(n).\nExpectedrunningtimes: If we say that the expected running time of an operation on a\ndata structure is f(n), this means that the actual running time is a random variable\n(seeSection1.2.4)andtheexpectedvalueofthisrandomvariableisatmostf(n). The\nrandomizationhereiswithrespecttorandomchoicesmadebythedatastructure.\nTounderstandthedifferencebetweenworst-case,amortized,andexpectedrunning\ntimes,ithelpstoconsiderafinancialexample. Considerthecostbuyingahouse.\nWorst-case versus amortized cost: Suppose that a home costs $120000. In order to\nbuy this home, we might get a 120 month (10 year) mortgage with monthly payments of\n$1200 per month. In this case, the worst-case monthly cost of paying this mortgage is\n$1200permonth.\nIf we have enough cash on hand, we might choose to buy the house outright, with\none payment of $120000. In this case, over a period of 10 years, the amortized monthly\ncostofbuyingthishouseis\n$120000/120months=$1000permonth .\n18\n1.Introduction 1.5.CodeSamples\nThis is much less than the $1200 per month we would have to pay if we took out a mort-\ngage.\nWorst-case versus expected cost: Next, consider the issue of fire insurance on our\n$120000 home. By studying hundreds of thousands of cases, insurance companies have\ndeterminedthattheexpectedamountoffiredamagecausedtoahomelikeoursis$10per\nmonth. Thisisaverysmallnumber, sincemosthomesneverhavefires,afewhomesmay\nhavesomesmallfiresthatcauseabitofsmokedamage,andatinynumberofhomesburn\nrighttotheirfoundations. Basedonthisinformation,theinsurancecompanycharges$15\npermonthforfireinsurance.\nNow it\u2019s decision time. Should we pay the $15 worst-case monthly cost for fire\ninsurance or should we gamble and self-insure at an expected cost of $10 per month?\nClearly, the $10 per month cost less in expectation, but we have to be able to accept the\npossibilitythatactualcostmaybemuchhigher. Intheunlikelyeventthattheentirehouse\nburnsdown,theactualcostwillbe$120000.\nThese financial examples also offer insight into why we sometimes settle for an\namortizedorexpectedrunningtimeoveraworst-caserunningtime. Itisoftenpossibleto\ngetalowerexpectedoramortizedrunningtimethanworst-caserunningtime. Atthevery\nleast,itisveryoftenpossibletogetamuchsimplerdatastructureifoneiswillingtosettle\nforamortizedorexpectedrunningtimes.\n1.5 Code Samples\nThe code samples in this book are written in the C++ programming language. However\nto make the book accessible even to readers not familiar with all of C++\u2019s constructs and\nkeywords,thecodesampleshavebeensimplified. Forexample,areaderwon\u2019tfindanyof\nthekeywordspublic,protected,private,orstatic. Areaderalsowon\u2019tfindmuchdis-\ncussion about class hierarchies. Which interfaces a particular class implements or which\nclassitextends,ifrelevanttothediscussion,willbeclearfromtheaccompanyingtext.\nTheseconventionsshouldmakemostofthecodesamplesunderstandablebyany-\none with a background in any of the languages from the ALGOL tradition, including B,\nC,C++,C#,D,Java,JavaScript,andsoon. Readerswhowantthefulldetailsofallimple-\nmentationsareencouragedtolookattheC++sourcecodethataccompaniesthisbook.\nThisbookmixesmathematicalanalysisofrunningtimeswithC++sourcecodefor\nthe algorithms being analyzed. This means that some equations contain variables also\nfound in the source code. These variables are typeset consistently, both within the source\n19\n1.Introduction 1.6.ListofDataStructures\nListimplementations\nget(i)/set(i,x) add(i,x)/remove(i)\nArrayStack O(1) O(1+n i)A \u00a72.1\n\u2212\nArrayDeque O(1) O(1+min i,n i )A \u00a72.4\n{ \u2212 }\nDualArrayDeque O(1) O(1+min i,n i )A \u00a72.5\n{ \u2212 }\nRootishArrayStack O(1) O(1+n i)A \u00a72.6\n\u2212\nDLList O(1+min i,n i ) O(1+min i,n i ) \u00a73.2\n{ \u2212 } { \u2212 }\nSEList O(1+min i,n i /b) O(b+min i,n i /b)A \u00a73.3\n{ \u2212 } { \u2212 }\nSkiplistList O(logn)E O(logn)E \u00a74.3\nUSetimplementations\nfind(x) add(x)/remove(x)\nChainedHashTable O(1)E O(1)A,E \u00a75.1\nLinearHashTable O(1)E O(1)A,E \u00a75.2\nADenotesanamortizedrunningtime.\nEDenotesanexpectedrunningtime.\nTable1.1: SummaryofListandUSetimplementations.\ncodeandwithinequations. Themostcommonsuchvariableisthevariablenthat,without\nexception,alwaysreferstothenumberofitemscurrentlystoredinthedatastructure.\n1.6 List of Data Structures\nTables 1.1 and 1.2 summarizes the performance of data structures in this book that im-\nplement each of the interfaces, List, USet, and SSet, described in Section 1.1. Figure 1.6\nshowsthedependenciesbetweenvariouschaptersinthisbook. Adashedarrowindicates\nonlyaweak-dependency,inwhichonlyasmallpartofthechapterdependsonaprevious\nchapteroronlythemainresultsofthepreviouschapter.\n1.7 Discussion and Exercises\nTheList,USet,andSSetinterfacesdescribedinSection1.1areinfluencedbytheJavaCol-\nlections Framework [49]. These are essentially simplified versions of the List, Set/Map,\nandSortedSet/SortedMapinterfacesfoundintheJavaCollectionsFramework.\nForasuperb(andfree)treatmentofthemathematicsdiscussedinthischapter,in-\n20\n1.Introduction 1.7.DiscussionandExercises\nSSetimplementations\nfind(x) add(x)/remove(x)\nSkiplistSSet O(logn)E O(logn)E \u00a74.2\nTreap O(logn)E O(logn)E \u00a77.2\nScapegoatTree O(logn) O(logn)A \u00a78.1\nRedBlackTree O(logn) O(logn) \u00a79.2\nBinaryTrieI O(w) O(w) \u00a713.1\nXFastTrieI O(logw)A,E O(w)A,E \u00a713.2\nYFastTrieI O(logw)A,E O(logw)A,E \u00a713.3\n(Priority)Queueimplementations\nfindMin() add(x)/remove()\nBinaryHeap O(1) O(logn)A \u00a710.1\nMeldableHeap O(1) O(logn)E \u00a710.2\nI Thisstructurecanonlystorew-bitintegerdata.\nTable1.2: SummaryofSSetandpriorityQueueimplementations.\n21\n1.Introduction 1.7.DiscussionandExercises\n1.Introduction\n2.Array-basedlists 3.Linkedlists\n3.3Space-efficientlinkedlists\n5.Hashtables\n4.Skiplists\n6.Binarytrees 7.Randombinarysearchtrees 11.Sortingalgorithms\n11.1.2.Quicksort\n8.Scapegoattrees 11.1.3.Heapsort\n9.Red-blacktrees\n10.Heaps\n12.Graphs\n13.Datastructuresforintegers\n14.External-memorysearching\nFigure1.6: Thedependenciesbetweenchaptersinthisbook.\n22\n1.Introduction 1.7.DiscussionandExercises\ncluding asymptotic notation, logarithms, factorials, Stirling\u2019s approximation, basic prob-\nability, and lots more, see the textbook by Leyman, Leighton, and Meyer [45]. For a gen-\ntle calculus text that includes formal definitions of exponentials and logarithms, see the\n(freelyavailable)classictextbyThompson[64].\nFormore informationonbasicprobability, especially asitrelatesto computersci-\nence, see the textbook by Ross [58]. Another good reference, that covers both asymptotic\nnotationandprobability,isthetextbookbyGraham,Knuth,andPatashnik[33].\nExercise 1.1. This exercise is designed to help get the reader familiar with choosing the\nrightdatastructurefortherightproblem. Ifimplemented,thepartsofthisexerciseshould\nbe done by making use of an implementation of the relevant interface (Stack, Queue,\nDeque,USet,orSSet)providedbytheC++StandardTemplateLibrary.\nSolvethefollowingproblemsbyreadingatextfileonelineatatimeandperform-\ning operations on each line in the appropriate data structure(s). Your implementations\nshould be fast enough that even files containing a million lines can be processed in a few\nseconds.\n1. Readtheinputonelineatatimeandthenwritethelinesoutinreverseorder,sothat\nthelastinputlineisprintedfirst,thenthesecondlastinputline,andsoon.\n2. Read the first 50 lines of input and then write them out in reverse order. Read the\nnext 50 lines and then write them out in reverse order. Do this until there are no\nmore lines left to read, at which point any remaining lines should be output in re-\nverseorder.\nInotherwords,youroutputwillstartwiththe50thline,thenthe49th,thenthe48th,\nandsoondowntothefirstline. Thiswillbefollowedbythe100thline,followedby\nthe99th,andsoondowntothe51stline. Andsoon.\nYourcodeshouldneverhavetostoremorethan50linesatanygiventime.\n3. Readtheinputonelineatatime. Atanypointafterreadingthefirst42lines,ifsome\nlineisblank(i.e.,astringoflength0)thenoutputthelinethatoccured42linesprior\ntothatone. Forexample,ifLine242isblank,thenyourprogramshouldoutputline\n200. Thisprogramshouldbeimplementedsothatitneverstoresmorethan43lines\noftheinputatanygiventime.\n4. Read the input one line at a time and write each line to the output if it is not a\nduplicate of some previous input line. Take special care so that a file with a lot of\n23\n1.Introduction 1.7.DiscussionandExercises\nduplicate lines does not use more memory than what is required for the number of\nuniquelines.\n5. Read the input one line at a time and write each line to the output only if you have\nalreadyreadthislinebefore. (Theendresultisthatyouremovethefirstoccurrence\nofeachline.) Takespecialcaresothatafilewithalotofduplicatelinesdoesnotuse\nmorememorythanwhatisrequiredforthenumberofuniquelines.\n6. Readtheentireinputonelineatatime. Thenoutputalllinessortedbylength,with\ntheshortestlinesfirst. Inthecasewheretwolineshavethesamelength,resolvetheir\norderusingtheusual\u201csortedorder.\u201d Duplicatelinesshouldbeprintedonlyonce.\n7. Do the same as the previous question except that duplicate lines should be printed\nthesamenumberoftimesthattheyappearintheinput.\n8. Read the entire input one line at a time and then output the even numbered lines\n(startingwiththefirstline,line0)followedbytheodd-numberedlines.\n9. Readtheentireinputonelineatatimeandrandomlypermutethelinesbeforeout-\nputting them. To be clear: You should not modify the contents of any line. Instead,\nthesamecollectionoflinesshouldbeprinted,butinarandomorder.\nExercise1.2. ADyckwordisasequenceof+1\u2019sand-1\u2019swiththepropertythatsumofany\nprefix of the sequence is never negative. Describe any relationship between Dyck words\nandStackpush(x)andpop()operations.\nExercise 1.3. A matched string is a sequence of , (, and [ characters that are properly\n{\nmatched. Forexample,\u201c ()[] \u201disamatchedstring,butthis\u201c ()] \u201disnot,sincethesecond\n{{ }} {{ }\nis matched with a ]. Show how to use a stack so that, given a string of length n, you can\n{\ndetermineifitisamatchedstringinO(n)time.\nExercise1.4. SupposeyouhaveaStack,s,thatsupportsonlythepush(x)andpop()oper-\nations. Showhow,usingonlyaFIFOQueue,q,youcanreversetheorderofallelementsin\ns.\nExercise 1.5. Using a USet, implement a Bag. A Bag is like a USet\u2014it supports the\nadd(x), remove(x) and find(x) methods\u2014but it allows duplicate elements to be stored.\nThe find(x) operation in a Bag returns some element (if any) that is equal to x. In addi-\ntion,aBagsupportsthefindAll(x)operationthatreturnsalistofallelementsintheBag\nthatareequaltox.\n24\n1.Introduction 1.7.DiscussionandExercises\nExercise 1.6. From scratch, write and test implementations of the List, USet and SSet\ninterfaces. Thesedonothavetobeefficient. Theycanbeusedlatertotestthecorrectness\nandperformanceofmoreefficientimplementations. (Theeasiestwaytodothisistostore\ntheelementsinanarray.)\nExercise1.7. Worktoimprovetheperformanceofyourimplementationsfromtheprevi-\nousquestionusinganytricksyoucanthinkof. Experimentandthinkabouthowyoucould\nimprove the performance of add(i,x) and remove(i) in your List implementation. Think\nabouthowyoucouldimprovetheperformanceofthefind(x)operationinyourUSetand\nSSet implementations. This exercise is designed to give you a feel for how difficult it can\nbetoobtainefficientimplementationsoftheseinterfaces.\n25\n1.Introduction 1.7.DiscussionandExercises\n26\nChapter 2\nArray-Based Lists\nIn this chapter, we study implementations of the List and Queue interfaces where the\nunderlyingdataisstoredinanarray,calledthebackingarray. Thefollowingtablesumma-\nrizestherunningtimesofoperationsforthedatastructurespresentedinthischapter:\nget(i)/set(i,x) add(i,x)/remove(i)\nArrayStack O(1) O(n i)\n\u2212\nArrayDeque O(1) O(min i,n i )\n{ \u2212 }\nDualArrayDeque O(1) O(min i,n i )\n{ \u2212 }\nRootishArrayStack O(1) O(n i)\n\u2212\nData structures that work by storing data in a single array have many advantages\nandlimitationsincommon:\n\u2022 Arraysofferconstanttimeaccesstoanyvalueinthearray. Thisiswhatallowsget(i)\nandset(i,x)toruninconstanttime.\n\u2022 Arrays are not very dynamic. Adding or removing an element near the middle of a\nlist means that a large number of elements in the array need to be shifted to make\nroomforthenewlyaddedelementortofillinthegapcreatedbythedeletedelement.\nThis is why the operations add(i,x) and remove(i) have running times that depend\nonnandi.\n\u2022 Arrays cannot expand or shrink. When the number of elements in the data struc-\ntureexceedsthesizeofthebackingarray, anewarrayneedstobeallocatedandthe\ndata from the old array needs to be copied into the new array. This is an expensive\noperation.\n27\n2.Array-BasedLists\nThethirdpointisimportant. Therunningtimescitedinthetableabovedonotincludethe\ncost of growing and shrinking the backing array. We will see that, if carefully managed,\nthe cost of growing and shrinking the backing array does not add much to the cost of an\naverage operation. More precisely, if we start with an empty data structure, and perform\nany sequence of m add(i,x) or remove(i) operations, then the total cost of growing and\nshrinking the backing array, over the entire sequence of m operations is O(m). Although\nsomeindividualoperationsaremoreexpensive,theamortizedcost, whenamortizedover\nallmoperations,isonlyO(1)peroperation.\nInthischapter,andthroughoutthisbook,itwillbeconvenienttohavearraysthat\nkeep track of their size. The usual C++ arrays do not do this, so we have defined a class,\narray, that keeps track of its length. The implementation of this class is straightforward.\nItisimplementedasastandardC++array,a,andaninteger,length:\narray\nT *a;\nint length;\nThesizeofanarrayisspecifiedatthetimeofcreation:\narray\narray(int len) {\nlength = len;\na = new T[length];\n}\nTheelementsofanarraycanbeindexed:\narray\nT& operator[](int i) {\nassert(i >= 0 && i < length);\nreturn a[i];\n}\nFinally,whenonearrayisassignedtoanother,thisisjustapointermanipulationthattakes\nconstanttime:\narray\narray<T>& operator=(array<T> &b) {\nif (a != NULL) delete[] a;\na = b.a;\nb.a = NULL;\nlength = b.length;\nreturn *this;\n}\n28\n2.Array-BasedLists 2.1.ArrayStack: FastStackOperationsUsinganArray\n2.1 ArrayStack: Fast Stack Operations Using an Array\nAnArrayStackimplementsthelistinterfaceusinganarraya,calledthebackingarray. The\nlistelementwithindexiisstoredina[i]. Atmosttimes,aislargerthanstrictlynecessary,\nsoanintegernisusedtokeeptrackofthenumberofelementsactuallystoredina. Inthis\nway,thelistelementsarestoredina[0],...,a[n 1]and,atalltimes,a.length n.\n\u2212 \u2265\nArrayStack\narray<T> a;\nint n;\nint size() {\nreturn n;\n}\n2.1.1 TheBasics\nAccessingandmodifyingtheelementsofanArrayStackusingget(i)andset(i,x)istriv-\nial. Afterperforminganynecessarybounds-checkingwesimplyreturnorset,respectively,\na[i].\nArrayStack\nT get(int i) {\nreturn a[i];\n}\nT set(int i, T x) {\nT y = a[i];\na[i] = x;\nreturn y;\n}\nThe operations of adding and removing elements from an ArrayStack are illus-\ntrated in Figure 2.1. To implement the add(i,x) operation, we first check if a is already\nfull. If so, we call the method resize() to increase the size of a. How resize() is im-\nplemented will be discussed later. For now, it is sufficient to know that, after a call to\nresize(), we can be sure that a.length > n. With this out of the way, we now shift the\nelementsa[i],...,a[n 1]rightbyonepositiontomakeroomforx,seta[i]equaltox,and\n\u2212\nincrementn.\nArrayStack\nvoid add(int i, T x) {\nif (n + 1 > a.length) resize();\nfor (int j = n; j > i; j--)\na[j] = a[j - 1];\n29\n2.Array-BasedLists 2.1.ArrayStack: FastStackOperationsUsinganArray\nb r e d\nadd(2,e)\nb r e e d\nadd(5,r)\nb r e e d r\nadd(5,e)\n\u2217\nb r e e d r\nb r e e d e r\nremove(4)\nb r e e e r\nremove(4)\nb r e e r\nremove(4)\n\u2217\nb r e e\nb r e e\nset(2,i)\nb r i e\n0 1 2 3 4 5 6 7 8 9 10 11\nFigure 2.1: A sequence of add(i,x) and remove(i) operations on an ArrayStack. Arrows\ndenoteelementsbeingcopied. Operationsthatresultinacalltoresize()aremarkedwith\nanasterisk.\n30\n2.Array-BasedLists 2.1.ArrayStack: FastStackOperationsUsinganArray\na[i] = x;\nn++;\n}\nIf we ignore the cost of the potential call to resize(), the cost of the add(i,x) operation is\nproportionaltothenumberofelementswehavetoshifttomakeroomforx. Thereforethe\ncostofthisoperation(ignoringthecostofresizinga)isO(n i+1).\n\u2212\nImplementingtheremove(i)operationissimilar. Weshifttheelementsa[i+1],...,a[n 1]\n\u2212\nleft by one position (overwriting a[i]) and decrease the value of n. After doing this, we\ncheck if n is getting much smaller than a.length by checking if a.length 3n. If so, we\n\u2265\ncallresize()toreducethesizeofa.\nArrayStack\nT remove(int i) {\nT x = a[i];\nfor (int j = i; j < n - 1; j++)\na[j] = a[j + 1];\nn--;\nif (a.length >= 3 * n) resize();\nreturn x;\n}\nIf we ignore the cost of the resize() method, the cost of a remove(i) operation is propor-\ntionaltothenumberofelementsweshift,whichisO(n i).\n\u2212\n2.1.2 GrowingandShrinking\nThe resize() method is fairly straightforward; it allocates a new array b whose size is 2n\nand copies the n elements of a into the first n positions in b, and then sets a to b. Thus,\nafteracalltoresize(),a.length=2n.\nArrayStack\nvoid resize() {\narray<T> b(max(2 * n, 1));\nfor (int i = 0; i < n; i++)\nb[i] = a[i];\na = b;\n}\nAnalyzing the actual cost of the resize() operation is easy. It allocates an array b\nofsize2nandcopiesthenelementsofaintob. ThistakesO(n)time.\n31\n2.Array-BasedLists 2.1.ArrayStack: FastStackOperationsUsinganArray\nThe running time analysis from the previous section ignored the cost of calls to\nresize(). In this section we analyze this cost using a technique known as amortized anal-\nysis. This technique does not try to determine the cost of resizing during each individual\nadd(i,x) and remove(i) operation. Instead, it considers the cost of all calls to resize()\nduringasequenceofmcallstoadd(i,x)orremove(i). Inparticular,wewillshow:\nLemma2.1. IfanemptyArrayListiscreatedandanysequenceofm 1callstoadd(i,x)and\n\u2265\nremove(i)areperformed,thenthetotaltimespentduringallcallstoresize()isO(m).\nProof. Wewillshowthatanytimeresize()iscalled,thenumberofcallstoaddorremove\nsince the last call to resize() is at least n/2 1. Therefore, if n denotes the value of n\ni\n\u2212\nduringtheithcalltoresize()andr denotesthenumberofcallstoresize(),thenthetotal\nnumberofcallstoadd(i,x)orremove(i)isatleast\nr\n(n /2 1) m ,\ni\n\u2212 \u2264\ni=1\n(cid:88)\nwhichisequivalentto\nr\nn 2m+2r .\ni\n\u2264\ni=1\n(cid:88)\nOntheotherhand,thetotaltimespentduringallcallstoresize()is\nr\nO(n ) O(m+r)=O(m) ,\ni\n\u2264\ni=1\n(cid:88)\nsincer isnotmorethanm. Allthatremainsistoshowthatthenumberofcallstoadd(i,x)\norremove(i)betweenthe(i 1)thandtheithcalltoresize()isatleastn /2.\ni\n\u2212\nThere are two cases to consider. In the first case, resize() is being called by\nadd(i,x) because the backing array a is full, i.e., a.length = n = n . Consider the previ-\ni\nous call to resize(): After this previous call, the size of a was a.length, but the number\nof elements stored in a was at most a.length/2 = n /2. But now the number of elements\ni\nstoredinaisn =a.length,sotheremusthavebeenatleastn /2callstoadd(i,x)sincethe\ni i\npreviouscalltoresize().\nThesecondcasetoconsideriswhenresize()isbeingcalledbyremove(i)because\na.length 3n = 3n . Again, after the previous call to resize() the number of elements\ni\n\u2265\nstoredinawasatleasta.length/2 1.1 Nowtherearen a.length/3elementsstoredin\ni\n\u2212 \u2264\na. Therefore,thenumberofremove(i)operationssincethelastcalltoresize()isatleast\na.length/2 1 a.length/3=a.length/6 1=(a.length/3)/2 1 n /2 1 .\ni\n\u2212 \u2212 \u2212 \u2212 \u2265 \u2212\n1The 1inthisformulaaccountsforthespecialcasethatoccurswhenn=0anda.length=1.\n\u2212\n32\n2.Array-BasedLists 2.2.FastArrayStack: AnOptimizedArrayStack\nIneithercase,thenumberofcallstoadd(i,x)orremove(i)thatoccurbetweenthe(i 1)th\n\u2212\ncalltoresize()andtheithcalltoresize()isatleastn /2 1,asrequiredtocompletethe\ni\n\u2212\nproof.\n2.1.3 Summary\nThefollowingtheoremsummarizestheperformanceofanArrayStack:\nTheorem 2.1. An ArrayStack implements the List interface. Ignoring the cost of calls to\nresize(),anArrayStacksupportstheoperations\n\u2022 get(i)andset(i,x)inO(1)timeperoperation;and\n\u2022 add(i,x)andremove(i)inO(1+n i)timeperoperation.\n\u2212\nFurthermore,beginningwithanemptyArrayStack,anysequenceofmadd(i,x)andremove(i)\noperationsresultsinatotalofO(m)timespentduringallcallstoresize().\nThe ArrayStack is an efficient way to implement a Stack. In particular, we can\nimplement push(x) as add(n,x) and pop() as remove(n 1), in which case these operations\n\u2212\nwillruninO(1)amortizedtime.\n2.2 FastArrayStack: An Optimized ArrayStack\nMuch of the work done by an ArrayStack involves shifting (by add(i,x) and remove(i))\nand copying (by resize()) of data. In the implementations shown above, this was done\nusing for loops. It turns out that many programming environments have specific func-\ntions that are very efficient at copying and moving blocks of data. In the C program-\nminglanguage,therearethememcpy(d,s,n)andmemmove(d,s,n)functions. InC++thereis\nthe std::copy(a0,a1,b) and algorithm. In Java there is the System.arraycopy(s,i,d,j,n)\nmethod.\nFastArrayStack\nvoid resize() {\narray<T> b(max(1, 2*n));\nstd::copy(a+0, a+n, b+0);\na = b;\n}\nvoid add(int i, T x) {\nif (n + 1 > a.length) resize();\nstd::copy_backward(a+i, a+n, a+n);\na[i] = x;\nn++;\n}\n33\n2.Array-BasedLists 2.3.ArrayQueue: AnArray-BasedQueue\nThese functions are usually highly optimized and may even use special machine\ninstructions that can do this copying much faster than we could do using a for loop.\nAlthough using these functions does not asymptotically decrease the running times, it\ncan still be a worthwhile optimization. In the C++ implementations here, the use of\nstd::copy(a0,a1,b) resulted in speedups of a factor between 2 and 3, depending on the\ntypesofoperationsperformed. Yourmileagemayvary.\n2.3 ArrayQueue: An Array-Based Queue\nInthissection,wepresenttheArrayQueuedatastructure,whichimplementsaFIFO(first-\nin-first-out) queue; elements are removed (using the remove() operation) from the queue\ninthesameordertheyareadded(usingtheadd(x)operation).\nNoticethatanArrayStackisapoorchoiceforanimplementationofaFIFOqueue.\nThereasonisthatwemustchooseoneendofthelisttoaddtoandthenremovefromthe\nother end. One of the two operations must work on the head of the list, which involves\ncallingadd(i,x)orremove(i)withavalueofi=0. Thisgivesarunningtimeproportional\nton.\nTo obtain an efficient array-based implementation of a queue, we first notice that\nthe problem would be easy if we had an infinite array a. We could maintain one index j\nthatkeepstrackofthenextelementtoremoveandanintegernthatcountsthenumberof\nelementsinthequeue. Thequeueelementswouldalwaysbestoredin\na[j],a[j+1],...,a[j+n 1] .\n\u2212\nInitially, both j and n would be set to 0. To add an element, we would place it in a[j+n]\nand increment n. To remove an element, we would remove it from a[j], increment j, and\ndecrementn.\nOf course, the problem with this solution is that it requires an infinite array. An\nArrayQueuesimulatesthisbyusingafinitearrayaandmodulararithmetic. Thisisthekind\nofarithmeticusedwhenwearetalkingaboutthetimeofday. Forexample10o\u2019clockplus\n5hoursgives3o\u2019clock. Formally,wesaythat\n10+5=15 3 (mod 12) .\n\u2261\nWe read the latter part of this equation as \u201c15 is congruent to 3 modulo 12.\u201d We can also\ntreat mod asabinaryoperator,sothat\n15 mod 12=3 .\n34\n2.Array-BasedLists 2.3.ArrayQueue: AnArray-BasedQueue\nMore generally, for an integer a and positive integer m, a mod m is the unique\nintegerr 0,...,m 1 suchthata=r+kmforsomeintegerk. Lessformally,thevaluer is\n\u2208{ \u2212 }\ntheremainderwegetwhenwedivideabym. Inmanyprogramminglanguages,including\nC++,the mod operatorisrepresentedusingthe%symbol.2\nModulararithmeticisusefulforsimulatinganinfinitearray,sincei mod a.length\nalwaysgivesavalueintherange0,...,a.length 1. Usingmodulararithmeticwecanstore\n\u2212\nthequeueelementsatarraylocations\na[j%a.length],a[(j+1)%a.length],...,a[(j+n 1)%a.length] .\n\u2212\nThis treats a like a circular array in which array indices exceeding a.length 1 \u201cwrap\n\u2212\naround\u201dtothebeginningofthearray.\nTheonlyremainingthingtoworryaboutistakingcarethatthenumberofelements\nintheArrayQueuedoesnotexceedthesizeofa.\nArrayQueue\narray<T> a;\nint j;\nint n;\nA sequence of add(x) and remove() operations on an ArrayQueue is illustrated in\nFigure 2.2. To implement add(x), we first check if a is full and, if necessary, call resize()\ntoincreasethesizeofa. Next,westorexina[(j+n)%a.length]andincrementn.\nArrayQueue\nbool add(T x) {\nif (n + 1 > a.length) resize();\na[(j+n) % a.length] = x;\nn++;\nreturn true;\n}\nTo implement remove() we first store a[j] so that we can return it later. Next,\nwe decrement n and increment j (modulo a.length) by setting j=(j+1) mod a.length.\nFinally, we return the stored value of a[j]. If necessary, we may call resize() to decrease\nthesizeofa.\nArrayQueue\nT remove() {\nT x = a[j];\n2This is sometimes referred to as the brain-dead mod operator since it does not correctly implement the\nmathematicalmodoperatorwhenthefirstargumentisnegative.\n35\n2.Array-BasedLists 2.3.ArrayQueue: AnArray-BasedQueue\nj=2,n=3 a b c\nadd(d)\nj=2,n=4 a b c d\nadd(e)\nj=2,n=5 e a b c d\nremove()\nj=3,n=4 e b c d\nadd(f)\nj=3,n=5 e f b c d\nadd(g)\nj=3,n=6 e f g b c d\nadd(h)\n\u2217\nj=0,n=6 b c d e f g\nj=0,n=7 b c d e f g h\nremove()\nj=1,n=6 c d e f g h\n0 1 2 3 4 5 6 7 8 9 10 11\nFigure 2.2: A sequence of add(x) and remove(i) operations on an ArrayQueue. Arrows\ndenoteelementsbeingcopied. Operationsthatresultinacalltoresize()aremarkedwith\nanasterisk.\n36\n2.Array-BasedLists 2.4.ArrayDeque: FastDequeOperationsUsinganArray\nj = (j + 1) % a.length;\nn--;\nif (a.length >= 3*n) resize();\nreturn x;\n}\nFinally,theresize()operationisverysimilartotheresize()operationofArrayStack.\nItallocatesanewarraybofsize2nandcopies\na[j],a[(j+1)%a.length],...,a[(j+n 1)%a.length]\n\u2212\nonto\nb[0],b[1],...,b[n 1]\n\u2212\nandsetsj=0.\nArrayQueue\nvoid resize() {\narray<T> b(max(1, 2*n));\nfor (int k = 0; k < n; k++)\nb[k] = a[(j+k)%a.length];\na = b;\n}\n2.3.1 Summary\nThefollowingtheoremsummarizestheperformanceoftheArrayQueuedatastructure:\nTheorem 2.2. An ArrayQueue implements the (FIFO) Queue interface. Ignoring the cost of\ncalls to resize(), an ArrayQueue supports the operations add(x) and remove() in O(1) time\nperoperation. Furthermore,beginningwithanemptyArrayQueue,anysequenceofmadd(i,x)\nandremove(i)operationsresultsinatotalofO(m)timespentduringallcallstoresize().\n2.4 ArrayDeque: Fast Deque Operations Using an Array\nTheArrayQueuefromtheprevioussectionisadatastructureforrepresentingasequence\nthatallowsustoefficientlyaddtooneendofthesequenceandremovefromtheotherend.\nTheArrayDequedatastructureallowsforefficientadditionandremovalatbothends. This\nstructure implements the List interface using the same circular array technique used to\nrepresentanArrayQueue.\nArrayDeque\narray<T> a;\nint j;\nint n;\n37\n2.Array-BasedLists 2.4.ArrayDeque: FastDequeOperationsUsinganArray\nj=0,n=8 a b c d e f g h\nremove(2)\nj=1,n=7 a b d e f g h\nadd(4,x)\nj=1,n=8 a b d e x f g h\nadd(3,y)\nj=0,n=9 a b d y e x f g h\nadd(4,z)\nj=11,n=10 b d y z e x f g h a\n0 1 2 3 4 5 6 7 8 9 10 11\nFigure 2.3: A sequence of add(i,x) and remove(i) operations on an ArrayDeque. Arrows\ndenoteelementsbeingcopied.\nThe get(i) and set(i,x) operations on an ArrayDeque are straightforward. They\ngetorsetthearrayelementa[(j+i) mod a.length].\nArrayDeque\nT get(int i) {\nreturn a[(j + i) % a.length];\n}\nT set(int i, T x) {\nT y = a[(j + i) % a.length];\na[(j + i) % a.length] = x;\nreturn y;\n}\nTheimplementationofadd(i,x)isalittlemoreinteresting. Asusual,wefirstcheck\nifaisfulland,ifnecessary,callresize()toresizea. Rememberthatwewantthisoperation\nto be fast when i is small (close to 0) or when i is large (close to n). Therefore, we check\nif i < n/2. If so, we shift the elements a[0],...,a[i 1] left by one position. Otherwise\n\u2212\n(i n/2),weshifttheelementsa[i],...,a[n 1]rightbyoneposition. SeeFigure2.3foran\n\u2265 \u2212\nillustrationofadd(i,x)andremove(x)operationsonanArrayDeque.\nArrayDeque\nvoid add(int i, T x) {\nif (n + 1 > a.length) resize();\nif (i < n/2) { // shift a[0],..,a[i-1] left one position\nj = (j == 0) ? a.length - 1 : j - 1;\nfor (int k = 0; k <= i-1; k++)\na[(j+k)%a.length] = a[(j+k+1)%a.length];\n38\n2.Array-BasedLists 2.4.ArrayDeque: FastDequeOperationsUsinganArray\n} else { // shift a[i],..,a[n-1] right one position\nfor (int k = n; k > i; k--)\na[(j+k)%a.length] = a[(j+k-1)%a.length];\n}\na[(j+i)%a.length] = x;\nn++;\n}\nBydoingtheshiftinginthisway,weguaranteethatadd(i,x)neverhastoshiftmore\nthanmin i,n i elements. Thus,therunningtimeoftheadd(i,x)operation(ignoringthe\n{ \u2212 }\ncostofaresize()operation)isO(1+min i,n i ).\n{ \u2212 }\nThe remove(i) operation is similar. It either shifts elements a[0],...,a[i 1] right\n\u2212\nbyonepositionorshiftstheelementsa[i+1],...,a[n 1]leftbyonepositiondependingon\n\u2212\nwhetheri<n/2. Again,thismeansthatremove(i)neverspendsmorethanO(1+min i,n\n{ \u2212\ni )timetoshiftelements.\n}\nArrayDeque\nT remove(int i) {\nT x = a[(j+i)%a.length];\nif (i < n/2) { // shift a[0],..,[i-1] right one position\nfor (int k = i; k > 0; k--)\na[(j+k)%a.length] = a[(j+k-1)%a.length];\nj = (j + 1) % a.length;\n} else { // shift a[i+1],..,a[n-1] left one position\nfor (int k = i; k < n-1; k++)\na[(j+k)%a.length] = a[(j+k+1)%a.length];\n}\nn--;\nif (3*n < a.length) resize();\nreturn x;\n}\n2.4.1 Summary\nThefollowingtheoremsummarizestheperformanceoftheArrayDequedatastructure:\nTheorem 2.3. An ArrayDeque implements the List interface. Ignoring the cost of calls to\nresize(),anArrayDequesupportstheoperations\n\u2022 get(i)andset(i,x)inO(1)timeperoperation;and\n\u2022 add(i,x)andremove(i)inO(1+min i,n i )timeperoperation.\n{ \u2212 }\n39\n2.Array-BasedLists 2.5.DualArrayDeque: BuildingaDequefromTwoStacks\nFurthermore,beginningwithanemptyArrayDeque,anysequenceofmadd(i,x)andremove(i)\noperationsresultsinatotalofO(m)timespentduringallcallstoresize().\n2.5 DualArrayDeque: Building a Deque from Two Stacks\nNext, wepresentanotherdatastructure, theDualArrayDequethatachievesthesameper-\nformance bounds as an ArrayDeque by using two ArrayStacks. Although the asymptotic\nperformance of the DualArrayDeque is no better than that of the ArrayDeque, it is still\nworth studying since it offers a good example of how to make a sophisticated data struc-\nturebycombiningtwosimplerdatastructures.\nADualArrayDequerepresentsalistusingtwoArrayStacks. RecallthatanArrayStack\nisfastwhentheoperationsonitmodifyelementsneartheend. ADualArrayDequeplaces\ntwoArrayStacks,calledfrontandback,back-to-backsothatoperationsarefastateither\nend.\nDualArrayDeque\nArrayStack<T> front;\nArrayStack<T> back;\nADualArrayDequedoesnotexplicitlystorethenumber,n,ofelementsitcontains.\nItdoesn\u2019tneedto,sinceitcontainsn=front.size()+back.size()elements. Nevertheless,\nwhenanalyzingtheDualArrayDequewewillstillusentodenotethenumberofelements\nitcontains.\nDualArrayDeque\nint size() {\nreturn front.size() + back.size();\n}\nThe front ArrayStack contains list elements with indices 0,...,front.size() 1,\n\u2212\nbutstorestheminreverseorder. ThebackArrayStackcontainslistelementswithindices\nfront.size(),...,size() 1 in the normal order. In this way, get(i) and set(i,x) translate\n\u2212\nintoappropriatecallstoget(i)orset(i,x)oneitherfrontorback,whichtakeO(1)time\nperoperation.\nDualArrayDeque\nT get(int i) {\nif (i < front.size()) {\nreturn front.get(front.size() - i - 1);\n} else {\nreturn back.get(i - front.size());\n40\n2.Array-BasedLists 2.5.DualArrayDeque: BuildingaDequefromTwoStacks\nfront back\na b c d\nadd(3,x)\na b c x d\nadd(4,y)\na b c x y d\nremove(0)\n\u2217\nb c x y d\nb c x y d\n4 3 2 1 0 0 1 2 3 4\nFigure 2.4: A sequence of add(i,x) and remove(i) operations on a DualArrayDeque. Ar-\nrows denote elements being copied. Operations that result in a rebalancing by balance()\naremarkedwithanasterisk.\n}\n}\nT set(int i, T x) {\nif (i < front.size()) {\nreturn front.set(front.size() - i - 1, x);\n} else {\nreturn back.set(i - front.size(), x);\n}\n}\nNotethat,ifanindexi<front.size(),thenitcorrespondstotheelementoffront\natpositionfront.size() i 1,sincetheelementsoffrontarestoredinreverseorder.\n\u2212 \u2212\nAddingandremovingelementsfromaDualArrayDequeisillustratedinFigure2.4.\nTheadd(i,x)operationmanipulateseitherfrontorback,asappropriate:\nDualArrayDeque\nvoid add(int i, T x) {\nif (i < front.size()) {\nfront.add(front.size() - i, x);\n} else {\nback.add(i - front.size(), x);\n}\nbalance();\n41\n2.Array-BasedLists 2.5.DualArrayDeque: BuildingaDequefromTwoStacks\n}\nThe add(i,x) method performs rebalancing of the two ArrayStacks front and\nback,bycallingthebalance()method. Theimplementationofbalance()isdescribedbe-\nlow, but for now it is sufficient to know that balance() ensures that, unless size() < 2,\nfront.size() and back.size() do not differ by more than a factor of 3. In particular,\n3 front.size() back.size()and3 back.size() front.size().\n\u00b7 \u2265 \u00b7 \u2265\nNextweanalyzethecostofadd(i,x), ignoringthecostofthebalance()operation.\nIfi<front.size(),thenadd(i,x)becomesfront.add(front.size() i 1,x). Sincefront\n\u2212 \u2212\nisanArrayStack,thecostofthisis\nO(front.size() (front.size() i 1)+1)=O(i+1) . (2.1)\n\u2212 \u2212 \u2212\nOntheotherhand,ifi front.size(),thenadd(i,x)becomesback.add(i front.size(),x).\n\u2265 \u2212\nThecostofthisis\nO(back.size() (i front.size())+1)=O(n i+1) . (2.2)\n\u2212 \u2212 \u2212\nNotice that the first case (2.1) occurs when i < n/4. The second case (2.2) occurs\nwheni 3n/4. Whenn/4 i<3n/4,wecan\u2019tbesurewhethertheoperationaffectsfront\n\u2265 \u2264\nor back, but in either case, the operation takes O(n)=O(i)=O(n i) time, since i n/4\n\u2212 \u2265\nandn i>n/4. Summarizingthesituation,wehave\n\u2212\nO(1+i) ifi<n/4\nRunningtimeofadd(i,x) \uf8f1 O(n) ifn/4 i<3n/4\n\u2264\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\nO(1+n\n\u2212\ni) ifi\n\u2265\n3\n\u2264\nn/4\nThus, the running time of add(i,x) (ignor\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\ning the cost of the call to balance()) is O(1+\nmin i,n i ).\n{ \u2212 }\nTheremove(i)operation,anditsanalysis,issimilartotheadd(i,x)operation.\nDualArrayDeque\nT remove(int i) {\nT x;\nif (i < front.size()) {\nx = front.remove(front.size()-i-1);\n} else {\nx = back.remove(i-front.size());\n}\nbalance();\nreturn x;\n}\n42\n2.Array-BasedLists 2.5.DualArrayDeque: BuildingaDequefromTwoStacks\n2.5.1 Balancing\nFinally, we study the balance() operation performed by add(i,x) and remove(i). This\noperation is used to ensure that neither front nor back gets too big (or too small). It\nensures that, unless there are fewer than 2 elements, each of front and back contain at\nleast n/4 elements. If this is not the case, then it moves elements between them so that\nfrontandbackcontainexactly n/2 elementsand n/2 elements,respectively.\n(cid:98) (cid:99) (cid:100) (cid:101)\nDualArrayDeque\nvoid balance() {\nif (3*front.size() < back.size()\n|| 3*back.size() < front.size()) {\nint n = front.size() + back.size();\nint nf = n/2;\narray<T> af(max(2*nf, 1));\nfor (int i = 0; i < nf; i++) {\naf[nf-i-1] = get(i);\n}\nint nb = n - nf;\narray<T> ab(max(2*nb, 1));\nfor (int i = 0; i < nb; i++) {\nab[i] = get(nf+i);\n}\nfront.a = af;\nfront.n = nf;\nback.a = ab;\nback.n = nb;\n}\n}\nThere is not much to analyze. If the balance() operation does rebalancing, then it\nmoves O(n) elements and this takes O(n) time. This is bad, since balance() is called with\neachcalltoadd(i,x)andremove(i). However,thefollowinglemmashowsthat,onaverage,\nbalance()onlyspendsaconstantamountoftimeperoperation.\nLemma 2.2. If an empty DualArrayDeque is created and any sequence of m 1 calls to\n\u2265\nadd(i,x) and remove(i) are performed, then the total time spent during all calls to balance()\nisO(m).\nProof. We will show that, if balance() is forced to shift elements, then the number of\nadd(i,x)andremove(i)operationssincethelasttimebalance()shiftedanyelementsisat\n43\n2.Array-BasedLists 2.5.DualArrayDeque: BuildingaDequefromTwoStacks\nleast n/2 1. As in the proof of Lemma 2.1, this is sufficient to prove that the total time\n\u2212\nspentbybalance()isO(m).\nWewillperformouranalysisusingatechniqueknowsasthepotentialmethod. De-\nfine the potential, \u03a6, of the DualArrayDeque as the difference in size between front and\nback:\n\u03a6 = front.size() back.size() .\n| \u2212 |\nThe interesting thing about this potential is that a call to add(i,x) or remove(i) that does\nnotdoanybalancingcanincreasethepotentialbyatmost1.\nObservethat,immediatelyafteracalltobalance()thatshiftselements,thepoten-\ntial,\u03a6 ,isatmost1,since\n0\n\u03a6 = n/2 n/2 1 .\n0\n|(cid:98) (cid:99)\u2212(cid:100) (cid:101)|\u2264\nConsider the situation immediately before a call to balance() that shifts elements\nandsuppose,withoutlossofgenerality,thatbalance()isshiftingelementsbecause3front.size()<\nback.size(). Noticethat,inthiscase,\nn = front.size()+back.size()\n< back.size()/3+back.size()\n4\n= back.size()\n3\nFurthermore,thepotentialatthispointintimeis\n\u03a6 = back.size() front.size()\n1\n\u2212\n> back.size() back.size()/3\n\u2212\n2\n= back.size()\n3\n2 3\n> n\n3 \u00d7 4\n= n/2\nTherefore, the number of calls to add(i,x) or remove(i) since the last time balance()\nshiftedelementsisatleast\u03a6 \u03a6 >n/2 1. Thiscompletestheproof.\n1 0\n\u2212 \u2212\n2.5.2 Summary\nThefollowingtheoremsummarizestheperformanceofaDualArrayStack\nTheorem2.4. ADualArrayDequeimplementstheListinterface. Ignoringthecostofcallsto\nresize()andbalance(),aDualArrayDequesupportstheoperations\n44\n2.Array-BasedLists 2.6.RootishArrayStack: ASpace-EfficientArrayStack\n\u2022 get(i)andset(i,x)inO(1)timeperoperation;and\n\u2022 add(i,x)andremove(i)inO(1+min i,n i )timeperoperation.\n{ \u2212 }\nFurthermore, beginning with an empty DualArrayDeque, any sequence of m add(i,x) and\nremove(i) operations results in a total of O(m) time spent during all calls to resize() and\nbalance().\n2.6 RootishArrayStack: A Space-Efficient Array Stack\nOne of the drawbacks of all previous data structures in this chapter is that, because they\nstore their data in one or two arrays, and they avoid resizing these arrays too often, the\narrays are frequently not very full. For example, immediately after a resize() operation\nonanArrayStack,thebackingarrayaisonlyhalffull. Evenworse,therearetimeswhen\nonly1/3ofacontainsdata.\nInthissection,wediscussadatastructure,theRootishArrayStack,thataddresses\nthe problem of wasted space. The RootishArrayStack stores n elements using O(\u221an)\narrays. Inthesearrays,atmostO(\u221an)arraylocationsareunusedatanytime. Allremaining\narraylocationsareusedtostoredata. Therefore,thesedatastructureswasteatmostO(\u221an)\nspacewhenstoringnelements.\nARootishArrayStackstoresitselementsinalistofrarrayscalledblocksthatare\nnumbered 0,1,...,r 1. See Figure 2.5. Block b contains b+1 elements. Therefore, all r\n\u2212\nblockscontainatotalof\n1+2+3+ +r=r(r+1)/2\n\u00b7\u00b7\u00b7\nelements. TheaboveformulacanbeobtainedasshowninFigure2.6.\nRootishArrayStack\nArrayStack<T*> blocks;\nint n;\nThe elements of the list are laid out in the blocks as we might expect. The list\nelementwithindex0isstoredinblock0,theelementswithlistindices1and2arestored\nin block 1, the elements with list indices 3, 4, and 5 are stored in block 2, and so on. The\nmain problem we have to address is that of determining, given an index i, which block\ncontainsiaswellastheindexcorrespondingtoiwithinthatblock.\nDetermining the index of i within its block turns out to be easy. If index i is in\nblockb,thenthenumberofelementsinblocks0,...,b 1isb(b+1)/2. Therefore,iisstored\n\u2212\n45\n2.Array-BasedLists 2.6.RootishArrayStack: ASpace-EfficientArrayStack\nblocks\na b c d e f g h\nadd(2,x)\na b x c d e f g h\nremove(1)\na x c d e f g h\nremove(7)\na x c d e f g\nremove(6)\na x c d e f\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nFigure 2.5: A sequence of add(i,x) and remove(i) operations on a RootishArrayStack.\nArrowsdenoteelementsbeingcopied.\n...\n.\n.\n.\nr ...\n.\n.\n.\n...\nr+1\nFigure2.6: Thenumberofwhitesquaresis1+2+3+ +r. Thenumberofshadedsquares\n\u00b7\u00b7\u00b7\nisthesame. Togetherthewhiteandshadedsquaresmakearectangleconsistingofr(r+1)\nsquares.\n46\n2.Array-BasedLists 2.6.RootishArrayStack: ASpace-EfficientArrayStack\natlocation\nj=i b(b+1)/2\n\u2212\nwithin block b. Somewhat more challenging is the problem of determining the value of\nb. The number of elements that have indices less than or equal to i is i+1. On the other\nhand,thenumberofelementsinblocks0,...,bis(b+1)(b+2)/2. Therefore,bisthesmallest\nintegersuchthat\n(b+1)(b+2)/2 i+1 .\n\u2265\nWecanrewritethisequationas\nb2+3b 2i 0 .\n\u2212 \u2265\nThecorrespondingquadraticequationb2+3b 2i=0hastwosolutions: b=( 3+\u221a9+8i)/2\n\u2212 \u2212\nand b = ( 3 \u221a9+8i)/2. The second solution makes no sense in our application since it\n\u2212 \u2212\nalways gives a negative value. Therefore, we obtain the solution b = ( 3+\u221a9+8i)/2. In\n\u2212\ngeneral,thissolutionisnotaninteger,butgoingbacktoourinequality,wewantthesmall-\nestintegerbsuchthatb ( 3+\u221a9+8i)/2. Thisissimply\n\u2265 \u2212\nb= ( 3+\u221a9+8i)/2 .\n\u2212\nRootishArrayStack\n(cid:108) (cid:109)\nint i2b(int i) {\ndouble db = (-3.0 + sqrt(9 + 8*i)) / 2.0;\nint b = (int)ceil(db);\nreturn b;\n}\nWiththisoutoftheway,theget(i)andset(i,x)methodsarestraightforward. We\nfirst compute the appropriate block b and the appropriate index j within the block and\nthenperformtheappropriateoperation:\nRootishArrayStack\nT get(int i) {\nint b = i2b(i);\nint j = i - b*(b+1)/2;\nreturn blocks.get(b)[j];\n}\nT set(int i, T x) {\nint b = i2b(i);\nint j = i - b*(b+1)/2;\nT y = blocks.get(b)[j];\nblocks.get(b)[j] = x;\nreturn y;\n}\n47\n2.Array-BasedLists 2.6.RootishArrayStack: ASpace-EfficientArrayStack\nIfweuseanyofthedatastructuresinthischapterforrepresentingtheblockslist,\nthenget(i)andset(i,x)willeachruninconstanttime.\nTheadd(i,x)methodwill,bynow,lookfamiliar. Wefirstcheckifourdatastructure\nis full, by checking if the number of blocks r is such that r(r+1)/2=n and, if so, we call\ngrow() to add another block. With this done, we shift elements with indices i,...,n 1 to\n\u2212\ntherightbyonepositiontomakeroomforthenewelementwithindexi:\nRootishArrayStack\nvoid add(int i, T x) {\nint r = blocks.size();\nif (r*(r+1)/2 < n + 1) grow();\nn++;\nfor (int j = n-1; j > i; j--)\nset(j, get(j-1));\nset(i, x);\n}\nThegrow()methoddoeswhatweexpect. Itaddsanewblock:\nRootishArrayStack\nvoid grow() {\nblocks.add(blocks.size(), new T[blocks.size()+1]);\n}\nIgnoringthecostofthegrow()operation,thecostofanadd(i,x)operationisdom-\ninatedbythecostofshiftingandisthereforeO(1+n i),justlikeanArrayStack.\n\u2212\nThe remove(i) operation is similar to add(i,x). It shifts the elements with indices\ni+1,...,nleftbyonepositionandthen,ifthereismorethanoneemptyblock,itcallsthe\nshrink()methodtoremoveallbutoneoftheunusedblocks:\nRootishArrayStack\nT remove(int i) {\nT x = get(i);\nfor (int j = i; j < n-1; j++)\nset(j, get(j+1));\nn--;\nint r = blocks.size();\nif ((r-2)*(r-1)/2 >= n) shrink();\nreturn x;\n}\nRootishArrayStack\nvoid shrink() {\nint r = blocks.size();\n48\n2.Array-BasedLists 2.6.RootishArrayStack: ASpace-EfficientArrayStack\nwhile (r > 0 && (r-2)*(r-1)/2 >= n) {\ndelete [] blocks.remove(blocks.size()-1);\nr--;\n}\n}\nOnce again, ignoring the cost of the shrink() operation, the cost of a remove(i)\noperationisdominatedbythecostofshiftingandisthereforeO(n i).\n\u2212\n2.6.1 AnalysisofGrowingandShrinking\nThe above analysis of add(i,x) and remove(i) does not account for the cost of grow() and\nshrink(). Notethat,unliketheArrayStack.resize()operation,grow()andshrink()donot\ndoanycopyingofdata. Theyonlyallocateorfreeanarrayofsizer. Insomeenvironments,\nthistakesonlyconstanttime,whileinothers,itmayrequiretimeproportionaltor.\nWe note that, immediately after a call to grow() or shrink(), the situation is clear.\nThe final block is completely empty and all other blocks are completely full. Another\ncalltogrow()orshrink()willnothappenuntilatleastr 1elementshavebeenaddedor\n\u2212\nremoved. Therefore,evenifgrow()andshrink()takeO(r)time,thiscostcanbeamortized\noveratleastr 1add(i,x)orremove(i)operations,sothattheamortizedcostofgrow()and\n\u2212\nshrink()isO(1)peroperation.\n2.6.2 SpaceUsage\nNext, we analyze the amount of extra space used by a RootishArrayStack. In particular,\nwe want to count any space used by a RootishArrayStack that is not an array element\ncurrentlyusedtoholdalistelement. Wecallallsuchspacewastedspace.\nTheremove(i)operationensuresthataRootishArrayStackneverhasmorethan2\nblocksthatarenotcompletelyfull. Thenumberofblocks,r,usedbyaRootishArrayStack\nthatstoresnelementsthereforesatisfies\n(r 2)(r 1) n .\n\u2212 \u2212 \u2264\nAgain,usingthequadraticequationonthisgives\nr (3+\u221a1+4n)/2=O(\u221an) .\n\u2264\nThe last two blocks have sizes r and r 1, so the space wasted by these two blocks is at\n\u2212\nmost2r 1=O(\u221an). Ifwestoretheblocksin(forexample)anArrayList,thentheamount\n\u2212\nofspacewastedbytheListthatstoresthoserblocksisalsoO(r)=O(\u221an). Theotherspace\n49\n2.Array-BasedLists 2.6.RootishArrayStack: ASpace-EfficientArrayStack\nneededforstoringnandotheraccountinginformationisO(1). Therefore,thetotalamount\nofwastedspaceinaRootishArrayStackisO(\u221an).\nNext, we argue that this space usage is optimal for any data structure that starts\nout empty and can support the addition of one item at a time. More precisely, we will\nshow that, at some point during the addition of n items, the data structure is wasting an\namountofspaceatleastin\u221an(thoughitmaybeonlywastedforamoment).\nSuppose we start with an empty data structure and we add n items one at a time.\nAt the end of this process, all n items are stored in the structure and they are distributed\namong a collection of r memory blocks. If r \u221an, then the data structure must be using\n\u2265\nrpointers(orreferences)tokeeptrackoftheserblocks,andthisiswastedspace. Onthe\notherhand,ifr<\u221anthen,bythepigeonholeprinciple,someblockmusthavesizeatleast\nn/r>\u221an. Considerthemomentatwhichthisblockwasfirstallocated. Immediatelyafter\nit was allocated, this block was empty, and was therefore wasting \u221an space. Therefore, at\nsome point in time during the insertion of n elements, the data structure was wasting \u221an\nspace.\n2.6.3 Summary\nThefollowingtheoremsummarizestheperformanceoftheRootishArrayStackdatastruc-\nture:\nTheorem2.5. ARootishArrayStackimplementstheListinterface. Ignoringthecostofcalls\ntogrow()andshrink(),aRootishArrayStacksupportstheoperations\n\u2022 get(i)andset(i,x)inO(1)timeperoperation;and\n\u2022 add(i,x)andremove(i)inO(1+n i)timeperoperation.\n\u2212\nFurthermore, beginning with an empty RootishArrayStack, any sequence of m add(i,x) and\nremove(i)operationsresultsinatotalofO(m)timespentduringallcallstogrow()andshrink().\nThe space (measured in words)3 used by a RootishArrayStack that stores n elements\nisn+O(\u221an).\n2.6.4 ComputingSquareRoots\nAreaderwhohashadsomeexposuretomodelsofcomputationmaynoticethattheRootishArrayStack,\nas described above, does not fit into the usual word-RAM model of computation (Sec-\n3RecallSection1.3foradiscussionofhowmemoryismeasured.\n50\n2.Array-BasedLists 2.6.RootishArrayStack: ASpace-EfficientArrayStack\ntion1.3)becauseitrequirestakingsquareroots. Thesquarerootoperationisgenerallynot\nconsideredabasicoperationandisthereforenotusuallypartoftheword-RAMmodel.\nIn this section, we take time to show that the square root operation can be imple-\nmented efficiently. In particular, we show that for any integer x 0,...,n , \u221ax can be\n\u2208 { } (cid:98) (cid:99)\ncomputed in constant-time, after O(\u221an) preprocessing that creates two arrays of length\nO(\u221an). The following lemma shows that we can reduce the problem of computing the\nsquarerootofxtothesquarerootofarelatedvaluex .\n(cid:48)\nLemma2.3. Letx 1andletx =x a,where0 a \u221ax. Then\u221ax \u221ax 1.\n(cid:48) (cid:48)\n\u2265 \u2212 \u2264 \u2264 \u2265 \u2212\nProof. Itsufficestoshowthat\nx \u221ax \u221ax 1 .\n\u2212 \u2265 \u2212\n(cid:113)\nSquarebothsidesofthisinequalitytoget\nx \u221ax x 2\u221ax+1\n\u2212 \u2265 \u2212\nandgathertermstoget\n\u221ax 1\n\u2265\nwhichisclearlytrueforanyx 1.\n\u2265\nStart by restricting the problem a little, and assume that 2r x < 2r+1, so that\n\u2264\nlogx = r, i.e., x is an integer having r+1 bits in its binary representation. We can take\n(cid:98) (cid:99)\nx = x (x mod 2 r/2 ). Now, x satisfies the conditions of Lemma 2.3, so \u221ax \u221ax 1.\n(cid:48) (cid:98) (cid:99) (cid:48) (cid:48)\n\u2212 \u2212 \u2264\nFurthermore,x hasallofitslower-order r/2 bitsequalto0,sothereareonly\n(cid:48)\n(cid:98) (cid:99)\n2r+1 r/2 4 2r/2 4\u221ax\n\u2212(cid:98) (cid:99)\n\u2264 \u00b7 \u2264\npossible values of x . This means that we can use an array, sqrttab, that stores the value\n(cid:48)\nof \u221ax foreachpossiblevalueofx . Alittlemoreprecisely,wehave\n(cid:48) (cid:48)\n(cid:98) (cid:99)\nsqrttab[i]= \u221a i2 r/2 .\n(cid:98) (cid:99)\nInthisway,sqrttab[i]iswithin2of\u221axforall (cid:106)x i2 r/(cid:107)2 ,...,(i+1)2 r/2 1 . Statedanother\n(cid:98) (cid:99) (cid:98) (cid:99)\n\u2208{ \u2212 }\nway, the array entry s = sqrttab[x>> r/2 ] is either equal to \u221ax , \u221ax 1, or \u221ax 2.\n(cid:98) (cid:99) (cid:98) (cid:99) (cid:98) (cid:99)\u2212 (cid:98) (cid:99)\u2212\nFromswecandeterminethevalueof \u221ax byincrementingsuntil(s+1)2>x.\n(cid:98) Fa(cid:99)stSqrt\nint sqrt(int x, int r) {\nint s = sqrtab[x>>r/2];\nwhile ((s+1)*(s+1) <= x) s++; // executes at most twice\nreturn s;\n}\n51\n2.Array-BasedLists 2.6.RootishArrayStack: ASpace-EfficientArrayStack\nNow, this only works for x 2r,...,2r+1 1 and sqrttab is a special table that\n\u2208 { \u2212 }\nonlyworksforaparticularvalueofr= logx . Toovercomethis,wecouldcompute logn\n(cid:98) (cid:99) (cid:98) (cid:99)\ndifferent sqrttab arrays, one for each possible value of logx . The sizes of these tables\n(cid:98) (cid:99)\nform an exponential sequence whose largest value is at most 4\u221an, so the total size of all\ntablesisO(\u221an).\nHowever, it turns out that more than one sqrttab array is unnecessary; we only\nneed one sqrttab array for the value r = logn . Any value x with logx = r < r can be\n(cid:48)\n(cid:98) (cid:99)\nupgradedbymultiplyingxby2r \u2212 r (cid:48) andusingtheequation\n\u221a2r \u2212 r (cid:48)x=2(r \u2212 r (cid:48) )/2\u221ax .\nThe quantity 2r\n\u2212\nr (cid:48)x is in the range 2r,...,2r+1 1 so we can look up its square root in\n{ \u2212 }\nsqrttab. The following code implements this idea to compute \u221ax for all non-negative\n(cid:98) (cid:99)\nintegersxintherange 0,...,230 1 usinganarray,sqrttab,ofsize216.\n{ \u2212 }\nFastSqrt\nint sqrt(int x) {\nint rp = log(x);\nint upgrade = ((r-rp)/2) * 2;\nint xp = x << upgrade; // xp has r or r-1 bits\nint s = sqrtab[xp>>(r/2)] >> (upgrade/2);\nwhile ((s+1)*(s+1) <= x) s++; // executes at most twice\nreturn s;\n}\nSomething we have taken for granted thus far is the question of how to compute\nr = logx . Again, this is a problem that can be solved with an array, logtab, of size\n(cid:48)\n(cid:98) (cid:99)\n2r/2. In this case, the code is particularly simple, since logx is just the index of the\n(cid:98) (cid:99)\nmost significant 1 bit in the binary representation of x. This means that, for x > 2r/2, we\ncan right-shift the bits of x by r/2 positions before using it as an index into logtab. The\nfollowing code does this using an array logtab of size 216 to compute logx for all x in\n(cid:98) (cid:99)\ntherange 1,...,232 1\n{ \u2212 }\nFastSqrt\nint log(int x) {\nif (x >= halfint)\nreturn 16 + logtab[x>>16];\nreturn logtab[x];\n}\nFinally,forcompleteness,weincludethefollowingcodethatinitializeslogtaband\nsqrttab:\n52\n2.Array-BasedLists 2.7.DiscussionandExercises\nFastSqrt\nvoid inittabs() {\nsqrtab = new int[1<<(r/2)];\nlogtab = new int[1<<(r/2)];\nfor (int d = 0; d < r/2; d++)\nfor (int k = 0; k < 1<<d; k++)\nlogtab[1<<d+k] = d;\nint s = 1<<(r/4); // sqrt(2\u02c6(r/2))\nfor (int i = 0; i < 1<<(r/2); i++) {\nif ((s+1)*(s+1) <= i << (r/2)) s++; // sqrt increases\nsqrtab[i] = s;\n}\n}\nTo summarize, the computations done by the i2b(i) method can be implemented\nin constant time on the word-RAM using O(\u221an) extra memory to store the sqrttab and\nlogtab arrays. These arrays can be rebuilt when n increases or decreases by a factor of 2,\nandthecostofthisrebuildingcanbeamortizedoverthenumberofadd(i,x)andremove(i)\noperationsthatcausedthechangeinninthesamewaythatthecostofresize()isanalyzed\nintheArrayStackimplementation.\n2.7 Discussion and Exercises\nMostofthedatastructuresdescribedinthischapterarefolklore. Theycanbefoundinim-\nplementationsdatingbackover30years. Forexample,implementationsofstacks,queues,\nanddequeswhichgeneralizeeasilytotheArrayStack,ArrayQueueandArrayDequestruc-\nturesdescribedherearediscussedbyKnuth[41,Section2.2.2].\nBrodnik et al. [11] seem to have been the first to describe the RootishArrayStack\nandprovea\u221anlower-boundlikethatinSection2.6.2. Theyalsopresentadifferentstruc-\nturethatusesamoresophisticatedchoiceofblocksizesinordertoavoidcomputingsquare\nrootsinthei2b(i)method. Withtheirscheme,theblockcontainingiisblock log(i+1) ,\n(cid:98) (cid:99)\nwhichisjusttheindexoftheleading1bitinthebinaryrepresentationofi+1. Somecom-\nputerarchitecturesprovideaninstructionforcomputingtheindexoftheleading1-bitin\naninteger.\nAstructurerelatedtotheRootishArrayStackisthe2-leveltiered-vectorofGoodrich\nandKloss[32]. Thisstructuresupportsget(i,x)andset(i,x)inconstanttimeandadd(i,x)\nand remove(i) in O(\u221an) time. These running times are similar to what can be achieved\nwiththemorecarefulimplementationofaRootishArrayStackdiscussedinExercise2.10.\n53\n2.Array-BasedLists 2.7.DiscussionandExercises\nExercise2.1. The List method addAll(i,c) inserts all elements of the Collection c into\nthelistatpositioni. (Theadd(i,x)methodisaspecialcasewherec= x .) Explainwhy,for\n{ }\nthedatastructuresinthischapter,itisnotefficienttoimplementaddAll(i,c)byrepeated\ncallstoadd(i,x). Designandimplementamoreefficientimplementation.\nExercise 2.2. Design and implement a RandomQueue. This is an implementation of the\nQueue interface in which the remove() operation removes an element that is chosen uni-\nformlyatrandomamongalltheelementscurrentlyinthequeue. (ThinkofaRandomQueue\nas a bag in which we can add elements or reach in and blindly remove some random ele-\nment.) Theadd(x)andremove()operationsinaRandomQueueshouldruninconstanttime\nperoperation.\nExercise2.3. DesignandimplementaTreque(triple-endedqueue). ThisisaListimple-\nmentation in which get(i) and set(i,x) run in constant time and add(i,x) and remove(i)\nrunintime\nO(1+min i,n i, n/2 i ) .\n{ \u2212 | \u2212 |}\nInotherwords,modificationsarefastiftheyareneareitherendornearthemiddleofthe\nlist.\nExercise2.4. Implementamethodrotate(a,r)that\u201crotates\u201dthearrayasothata[i]moves\ntoa[(i+r) mod a.length],foralli 0,...,a.length .\n\u2208{ }\nExercise 2.5. Implement a method rotate(r) that \u201crotates\u201d a List so that list item i\nbecomes list item (i+r) mod n. When run on an ArrayDeque, or a DualArrayDeque,\nrotate(r)shouldruninO(1+min r,n r )time.\n{ \u2212 }\nExercise2.6. ModifytheArrayDequeimplementationsothattheshiftingdonebyadd(i,x),\nremove(i),andresize()isdoneusingSystem.arraycopy(s,i,d,j,n).\nExercise 2.7. Modify the ArrayDeque implementation so that it does not use the % oper-\nator (which is expensive on some systems). Instead, it should make use of the fact that, if\na.length is a power of 2, then k%a.length=k&(a.length 1). (Here, & is the bitwise-and\n\u2212\noperator.)\nExercise 2.8. Design and implement a variant of ArrayDeque that does not do any mod-\nular arithmetic at all. Instead, all the data sits in a consecutive block, in order, inside an\narray. Whenthedataoverrunsthebeginningortheendofthisarray,amodifiedrebuild()\noperation is performed. The amortized cost of all operations should be the same as in an\nArrayDeque.\n54\n2.Array-BasedLists 2.7.DiscussionandExercises\nHint: Making this work is really all about how a rebuild() operation is performed. You\nwould like rebuild() to put the data structure into a state where the data cannot run off\neitherenduntilatleastn/2operationshavebeenperformed.\nTest the performance of your implementation against the ArrayDeque. Optimize\nyour implementation (by using System.arraycopy(a,i,b,i,n)) and see if you can get it to\noutperformtheArrayDequeimplementation.\nExercise 2.9. Design and implement a version of a RootishArrayStack that has only\nO(\u221an) wasted space, but that can perform add(i,x) and remove(i,x) operations in O(1+\nmin i,n i )time.\n{ \u2212 }\nExercise 2.10. Design and implement a version of a RootishArrayStack that has only\nO(\u221an) wasted space, but that can perform add(i,x) and remove(i,x) operations in O(1+\nmin \u221an,n i )time. (Foranideaonhowtodothis,seeSection3.3.)\n{ \u2212 }\nExercise 2.11. Design and implement a version of a RootishArrayStack that has only\nO(\u221an) wasted space, but that can perform add(i,x) and remove(i,x) operations in O(1+\nmin i,\u221an,n i )time. (SeeSection3.3forideasonhowtoachievethis.)\n{ \u2212 }\nExercise2.12. DesignandimplementaCubishArrayStack. Thisthreelevelstructureim-\nplements the List interface using at most O(n2/3) wasted space. In this structure, get(i)\nand set(i,x) take constant time; while add(i,x) and remove(i) take O(n1/3) amortized\ntime.\n55\n2.Array-BasedLists 2.7.DiscussionandExercises\n56\nChapter 3\nLinked Lists\nIn this chapter, we continue to study implementations of the List interface, this time\nusing pointer-based data structures rather than arrays. The structures in this chapter are\nmadeupofnodesthatcontainthelistitems. Thenodesarelinkedtogetherintoasequence\nusingreferences(pointers). Wefirststudysingly-linkedlists,whichcanimplementStack\nand(FIFO)Queueoperationsinconstanttimeperoperation.\nLinkedlistshaveadvantagesanddisadvantagesrelativetoarray-basedimplemen-\ntationsoftheListinterface. Theprimarydisadvantageisthatwelosetheabilitytoaccess\nany element using get(i) or set(i,x) in constant time. Instead, we have to walk through\nthe list, one element at a time, until we reach the ith element. The primary advantage is\nthattheyaremoredynamic: Givenareferencetoanylistnodeu,wecandeleteuorinsert\nanodeadjacenttouinconstanttime. Thisistruenomatterwhereuisinthelist.\n3.1 SLList: A Singly-Linked List\nAn SLList (singly-linked list) is a sequence of Nodes. Each node u stores a data value\nu.x and a reference u.next to the next node in the sequence. For the last node w in the\nsequence,w.next=null\nSLList\nclass Node {\npublic:\nT x;\nNode *next;\nNode(T x0) {\nx = 0;\nnext = NULL;\n}\n};\n57\n3.LinkedLists 3.1.SLList: ASingly-LinkedList\nhead tail\na b c d e\nhead tail add(x)\na b c d e x\nhead tail remove()\nb c d e x\nhead tail pop()\nc d e x\nhead tail push(y)\ny c d e x\nFigure 3.1: A sequence of Queue (add(x) and remove()) and Stack (push(x) and pop()) op-\nerationsonanSLList.\nFor efficiency, an SLList uses variables head and tail to keep track of the first\nand last node in the sequence, as well as an integer n to keep track of the length of the\nsequence:\nSLList\nNode *head;\nNode *tail;\nint n;\nAsequenceofStackandQueueoperationsonanSLListisillustratedinFigure3.1.\nAn SLList can efficiently implement the Stack operations push() and pop() by\nadding and removing elements at the head of the sequence. The push() operation sim-\nply creates a new node u with data value x, sets u.next to the old head of the list and\nmakes u the new head of the list. Finally, it increments n since the size of the SLList has\nincreasedbyone:\nSLList\nT push(T x) {\nNode *u = new Node(x);\nu->next = head;\nhead = u;\nif (n == 0)\ntail = u;\nn++;\nreturn x;\n}\n58\n3.LinkedLists 3.1.SLList: ASingly-LinkedList\nThepop()operation,aftercheckingthattheSLListisnotempty,removesthehead\nby setting head = head.next and decrementing n. A special case occurs when the last\nelementisbeingremoved,inwhichcasetailissettonull:\nSLList\nT pop() {\nif (n == 0) return NULL;\nT x = head->x;\nNode *u = head;\nhead = head->next;\ndelete u;\nif (--n == 0) tail = NULL;\nreturn x;\n}\nClearly,boththepush(x)andpop()operationsruninO(1)time.\n3.1.1 QueueOperations\nAnSLListcanalsoefficientlyimplementtheFIFOqueueoperationsadd(x)andremove().\nRemovalsaredonefromtheheadofthelist,andareidenticaltothepop()operation:\nSLList\nT remove() {\nif (n == 0) return NULL;\nT x = head->x;\nNode *u = head;\nhead = head->next;\ndelete u;\nif (--n == 0) tail = NULL;\nreturn x;\n}\nAdditions, on the other hand, are done at the tail of the list. In most cases, this is\ndonebysettingtail.next=u,whereuisthenewlycreatednodethatcontainsx. However,\naspecialcaseoccurswhenn=0,inwhichcasetail=head=null. Inthiscase,bothtail\nandheadaresettou.\nSLList\nbool add(T x) {\nNode *u = new Node(x);\nif (n == 0) {\nhead = u;\n} else {\ntail->next = u;\n59\n3.LinkedLists 3.2.DLList: ADoubly-LinkedList\n}\ntail = u;\nn++;\nreturn true;\n}\nClearly,bothadd(x)andremove()takeconstanttime.\n3.1.2 Summary\nThefollowingtheoremsummarizestheperformanceofanSLList:\nTheorem 3.1. An SLList implements the Stack and (FIFO) Queue interfaces. The push(x),\npop(),add(x)andremove()operationsruninO(1)timeperoperation.\nAnSLListcomesveryclosetoimplementingthefullsetofDequeoperations. The\nonly missing operation is removal from the tail of an SLList. Removing from the tail of\nan SLList is difficult because it requires updating the value of tail so that it points to\nthe node w that precedes tail in the SLList; this is the node w such that w.next = tail.\nUnfortunately, the only way to get to w is by traversing the SLList starting at head and\ntakingn 2steps.\n\u2212\n3.2 DLList: A Doubly-Linked List\nA DLList (doubly-linked list) is very similar to an SLList except that each node u in a\nDLList has references to both the node u.next that follows it and the node u.prev that\nprecedesit.\nDLList\nstruct Node {\nT x;\nNode *prev, *next;\n};\nWhenimplementinganSLList,wesawthattherewerealwayssomespecialcases\nto worry about. For example, removing the last element from an SLList or adding an\nelement to an empty SLList requires special care so that head and tail are correctly\nupdated. In a DLList, the number of these special cases increases considerably. Perhaps\nthe cleanest way to take care of all these special cases in a DLList is to introduce a dummy\nnode. Thisisanodethatdoesnotcontainanydata,butactsasaplaceholdersothatthere\narenospecialnodes;everynodehasbothanextandaprev,withdummyactingasthenode\n60\n3.LinkedLists 3.2.DLList: ADoubly-LinkedList\ndummy\na b c d e\nFigure3.2: ADLListcontaininga,b,c,d,e.\nthatfollowsthelastnodeinthelistandthatprecedesthefirstnodeinthelist. Inthisway,\nthenodesofthelistare(doubly-)linkedintoacycle,asillustratedinFigure3.2.\nDLList\nNode dummy;\nint n;\nDLList() {\ndummy.next = &dummy;\ndummy.prev = &dummy;\nn = 0;\n}\nFindingthenodewithaparticularindexinaDLListiseasy;wecaneitherstartat\ntheheadofthelist(dummy.next)andworkforward,orstartatthetailofthelist(dummy.prev)\nandworkbackward. ThisallowsustoreachtheithnodeinO(1+min i,n i )time:\n{ \u2212 }\nDLList\nNode* getNode(int i) {\nNode* p;\nif (i < n / 2) {\np = dummy.next;\nfor (int j = 0; j < i; j++)\np = p->next;\n} else {\np = &dummy;\nfor (int j = n; j > i; j--)\np = p->prev;\n}\nreturn (p);\n}\nThe get(i) and set(i,x) operations are now also easy. We first find the ith node\nandthengetorsetitsxvalue:\n61\n3.LinkedLists 3.2.DLList: ADoubly-LinkedList\nu\nu.prev u.next\np w\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\nFigure3.3: AddingthenodeubeforethenodewinaDLList.\nDLList\nT get(int i) {\nreturn getNode(i)->x;\n}\nT set(int i, T x) {\nNode* u = getNode(i);\nT y = u->x;\nu->x = x;\nreturn y;\n}\nThe running time of these operations is dominated by the time it takes to find the\nithnode,andisthereforeO(1+min i,n i ).\n{ \u2212 }\n3.2.1 AddingandRemoving\nIfwehaveareferencetoanodewinaDLListandwewanttoinsertanodeubeforew,then\nthisisjustamatterofsettingu.next=w,u.prev=w.prev,andthenadjustingu.prev.next\nand u.next.prev. (See Figure 3.3.) Thanks to the dummy node, there is no need to worry\naboutw.prevorw.nextnotexisting.\nDLList\nNode* addBefore(Node *w, T x) {\nNode *u = new Node;\nu->x = x;\nu->prev = w->prev;\nu->next = w;\nu->next->prev = u;\nu->prev->next = u;\nn++;\nreturn u;\n}\n62\n3.LinkedLists 3.2.DLList: ADoubly-LinkedList\nNow, the list operation add(i,x) is trivial to implement. We find the ith node in\ntheDLListandinsertanewnodeuthatcontainsxjustbeforeit.\nDLList\nvoid add(int i, T x) {\naddBefore(getNode(i), x);\n}\nThe only non-constant part of the running time of add(i,x) is the time it takes to\nfindtheithnode(usinggetNode(i)). Thus,add(i,x)runsinO(1+min i,n i )time.\n{ \u2212 }\nRemoving a node w from a DLList is easy. We need only adjust pointers at w.next\nandw.prevsothattheyskipoverw. Again,theuseofthedummynodeeliminatestheneed\ntoconsideranyspecialcases:\nDLList\nvoid remove(Node *w) {\nw->prev->next = w->next;\nw->next->prev = w->prev;\ndelete w;\nn--;\n}\nNowtheremove(i)operationistrivial. Wefindthenodewithindexiandremove\nit:\nDLList\nT remove(int i) {\nNode *w = getNode(i);\nT x = w->x;\nremove(w);\nreturn x;\n}\nAgain, the only expensive part of this operation is finding the ith node using\ngetNode(i),soremove(i)runsinO(1+min i,n i )time.\n{ \u2212 }\n3.2.2 Summary\nThefollowingtheoremsummarizestheperformanceofaDLList:\nTheorem 3.2. A DLList implements the List interface. The get(i), set(i,x), add(i,x) and\nremove(i)operationsruninO(1+min i,n i )timeperoperation.\n{ \u2212 }\n63\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\nIt is worth noting that, if we ignore the cost of the getNode(i) operation, then all\noperationsonaDLListtakeconstanttime. Thus,theonlyexpensivepartofoperationson\naDLListisfindingtherelevantnode. Oncewehavetherelevantnode,adding,removing,\noraccessingthedataatthatnodetakesonlyconstanttime.\nThisisinsharpcontrasttothearray-basedListimplementationsofChapter2;in\nthose implementations, the relevant array item can be found in constant time. However,\naddition or removal requires shifting elements in the array and, in general, takes non-\nconstanttime.\nFor this reason, linked list structures are well-suited to applications where refer-\nencestolistnodescanbeobtainedthroughexternalmeans. Forexample,pointerstothe\nnodesofalinkedlistcouldbestoredinaUSet. Then,toremoveanitemxfromthelinked\nlist, the node that contains x can be found quickly using the Uset and the node can be\nremovedfromthelistinconstanttime.\n3.3 SEList: A Space-Efficient Linked List\nOne of the drawbacks of linked lists (besides the time it takes to access elements that are\ndeep within the list) is their space usage. Each node in a DLList requires an additional\ntwo references to the next and previous nodes in the list. Two of the fields in a Node are\ndedicatedtomaintainingthelistandonlyoneofthefieldsisforstoringdata!\nAn SEList (space-efficient list) reduces this wasted space using a simple idea:\nRather than store individual elements in a DLList, we store a block (array) containing\nseveral items. More precisely, an SEList is parameterized by a block size b. Each individ-\nualnodeinanSEListstoresablockthatcanholduptob+1elements.\nIt will turn out, for reasons that become clear later, that it will be helpful if we\ncan do Deque operations on each block. The data structure we choose for this is a BDeque\n(bounded deque), derived from the ArrayDeque structure described in Section 2.4. The\nBDequediffersfromtheArrayDequeinonesmallway: WhenanewBDequeiscreated,the\nsize of the backing array a is fixed at b+1 and it never grows or shrinks. The important\npropertyofaBDequeisthatitallowsfortheadditionorremovalofelementsateitherthe\nfront or back in constant time. This will be useful as elements are shifted from one block\ntoanother.\nSEList\nclass BDeque : public ArrayDeque<T> {\npublic:\nBDeque(int b) {\nn = 0;\n64\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\nj = 0;\narray<int> z(b+1);\na = z;\n}\n\u02dcBDeque() { }\n// C++ Question: Why is this necessary?\nvoid add(int i, T x) {\nArrayDeque<T>::add(i, x);\n}\nbool add(T x) {\nArrayDeque<T>::add(size(), x);\nreturn true;\n}\nvoid resize() {}\n};\nAnSELististhenadoubly-linkedlistofblocks:\nSEList\nclass Node {\npublic:\nBDeque d;\nNode *prev, *next;\nNode(int b) : d(b) { }\n};\nSEList\nint n;\nNode dummy;\n3.3.1 SpaceRequirements\nAn SEList places very tight restrictions on the number of elements in a block: Unless a\nblock is the last block, then that block contains at least b 1 and at most b+1 elements.\n\u2212\nThismeansthat,ifanSEListcontainsnelements,thenithasatmost\nn/(b 1)+1=O(n/b)\n\u2212\nblocks. The BDeque for each block contains an array of length b+1 but, for all blocks\nexceptthelast,atmostaconstantamountofspaceiswastedinthisarray. Theremaining\nmemory used by a block is also constant. This means that the wasted space in an SEList\nisonlyO(b+n/b). Bychoosingavalueofbwithinaconstantfactorof\u221anwecanmakethe\nspace-overheadofanSEListapproachthe\u221anlowerboundgiveninSection2.6.2.\n65\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\n3.3.2 FindingElements\nThe first challenge we face with an SEList is finding the list item with a given index i.\nNote that the location of an element consists of two parts: The node u that contains the\nblockthatcontainstheelementaswellastheindexjoftheelementwithinitsblock.\nSEList\nclass Location {\npublic:\nNode *u;\nint j;\nLocation() { }\nLocation(Node *u, int j) {\nthis->u = u;\nthis->j = j;\n}\n};\nTo find the block that contains a particular element, we proceed in the same way\nasinaDLList. Weeitherstartatthefrontofthelistandtraverseintheforwarddirection\nor at the back of the list and traverse backwards until we reach the node we want. The\nonlydifferenceisthat,eachtimewemovefromonenodetothenext,weskipoverawhole\nblockofelements.\nSEList\nvoid getLocation(int i, Location &ell) {\nif (i < n / 2) {\nNode *u = dummy.next;\nwhile (i >= u->d.size()) {\ni -= u->d.size();\nu = u->next;\n}\nell.u = u;\nell.j = i;\n} else {\nNode *u = &dummy;\nint idx = n;\nwhile (i < idx) {\nu = u->prev;\nidx -= u->d.size();\n}\nell.u = u;\nell.j = i - idx;\n}\n}\n66\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\nRemember that, with the exception of at most one block, each block contains at\nleastb 1elements,soeachstepinoursearchgetsusb 1elementsclosertotheelement\n\u2212 \u2212\nwearelookingfor. Ifwearesearchingforward,thismeanswereachthenodewewantafter\nO(1+i/b)steps. Ifwesearchbackwards,wereachthenodewewantafterO(1+(n i)/b)\n\u2212\nsteps. The algorithm takes the smaller of these two quantities depending on the value of\ni,sothetimetolocatetheitemwithindexiisO(1+min i,n i /b).\n{ \u2212 }\nOnceweknowhowtolocatetheitemwithindexi,theget(i)andset(i,x)opera-\ntionstranslateintogettingorsettingaparticularindexinthecorrectblock:\nSEList\nT get(int i) {\nLocation l;\ngetLocation(i, l);\nreturn l.u->d.get(l.j);\n}\nT set(int i, T x) {\nLocation l;\ngetLocation(i, l);\nT y = l.u->d.get(l.j);\nl.u->d.set(l.j, x);\nreturn y;\n}\nTherunningtimesoftheseoperationsaredominatedbythetimeittakestolocate\ntheitem,sotheyalsoruninO(1+min i,n i /b)time.\n{ \u2212 }\n3.3.3 AddinganElement\nThings start to get complicated when adding elements to an SEList. Before considering\nthe general case, we consider the easier operation, add(x), in which x is added to the end\nof the list. If the last block is full (or does not exist because there are no blocks yet), then\nwefirstallocateanewblockandappendittothelistofblocks. Nowthatwearesurethat\nthelastblockexistsandisnotfull,weappendxtothelastblock.\nSEList\nvoid add(T x) {\nNode *last = dummy.prev;\nif (last == &dummy || last->d.size() == b+1) {\nlast = addBefore(&dummy);\n}\nlast->d.add(x);\nn++;\n}\n67\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\na b c d e f g h i j\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\na x b c d e f g h i j\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\na b c d e f g h\n\u00b7\u00b7\u00b7\na x b c d e f g h\n\u00b7\u00b7\u00b7\na b c d e f g h i j k l\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\na x b c d e f g h i j k l\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\nFigure3.4: Thethreecasesthatoccurduringtheadditionofanitemxintheinteriorofan\nSEList. (ThisSEListhasblocksizeb=3.)\nThingsgetmorecomplicatedwhenweaddtotheinteriorofthelistusingadd(i,x).\nWe first locate i to get the node u whose block contains the ith list item. The problem is\nthat we want to insert x into u\u2019s block, but we have to be prepared for the case where u\u2019s\nblockalreadycontainsb+1elements,sothatitisfullandthereisnoroomforx.\nLet u ,u ,u ,... denote u, u.next, u.next.next, and so on. We explore u ,u ,u ,...\n0 1 2 0 1 2\nlooking for a node that can provide space for x. Three cases can occur during our space\nexploration(seeFigure3.4):\n1. Wequickly(inr+1 bsteps)findanodeu whoseblockisnotfull. Inthiscase,we\nr\n\u2264\nperformr shiftsofanelementfromoneblockintothenext,sothatthefreespacein\nu becomesafreespaceinu . Wecantheninsertxintou \u2019sblock.\nr 0 0\n2. Wequickly(inr+1 bsteps)runofftheendofthelistofblocks. Inthiscase,weadd\n\u2264\nanewemptyblocktotheendofthelistofblocksandproceedasinthefirstcase.\n3. After b steps we do not find any block that is not full. In this case, u ,...,u is a\n0 b 1\n\u2212\nsequence of b blocks that each contain b+1 elements. We insert a new block u at\nb\nthe end of this sequence and spread the original b(b+1) elements so that each block\nofu ,...,u containsexactlybelements. Nowu \u2019sblockcontainsonlybelementsso\n0 b 0\nithasroomforustoinsertx.\n68\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\nSEList\nvoid add(int i, T x) {\nif (i == n) {\nadd(x);\nreturn;\n}\nLocation l; getLocation(i, l);\nNode *u = l.u;\nint r = 0;\nwhile (r < b && u != &dummy && u->d.size() == b+1) {\nu = u->next;\nr++;\n}\nif (r == b) { // found b blocks each with b+1 elements\nspread(l.u);\nu = l.u;\n}\nif (u == &dummy) { // ran off the end of the list - add new node\nu = addBefore(u);\n}\nwhile (u != l.u) { // work backwards, shifting an element at each step\nu->d.add(0, u->prev->d.remove(u->prev->d.size()-1));\nu = u->prev;\n}\nu->d.add(l.j, x);\nn++;\n}\nThe running time of the add(i,x) operation depends on which of the three cases\nabove occurs. Cases 1 and 2 involve examining and shifting elements through at most b\nblocks and take O(b) time. Case 3 involves calling the spread(u) method, which moves\nb(b+1) elements and takes O(b2) time. If we ignore the cost of Case 3 (which we will\naccountforlaterwithamortization)thismeansthatthetotalrunningtimetolocateiand\nperformtheinsertionofxisO(b+min i,n i /b).\n{ \u2212 }\n3.3.4 RemovinganElement\nRemovinganelement,usingtheremove(i)methodfromanSEListissimilartoaddingan\nelement. We first locate the node u that contains the element with index i. Now, we have\nto be prepared for the case where we cannot remove an element from u without causing\nu\u2019sblocktohavesizelessthanb 1,whichisnotallowed.\n\u2212\nAgain, let u ,u ,u ,... denote u, u.next, u.next.next, We examine u ,u ,u ,... in\n0 1 2 0 1 2\n69\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\na b c d e f g\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\na c d e f g\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\na b c d e f\n\u00b7\u00b7\u00b7\na c d e f\n\u00b7\u00b7\u00b7\na b c d e f\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\na c d e f\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\nFigure3.5: Thethreecasesthatoccurduringtheremovalofanitemxintheinteriorofan\nSEList. (ThisSEListhasblocksizeb=3.)\norder looking for a node from which we can borrow an element to make the size of u \u2019s\n0\nblocklargerthanb 1. Therearethreecasestoconsider(seeFigure3.5):\n\u2212\n1. We quickly (in r +1 b steps) find a node whose block contains more than b 1\n\u2264 \u2212\nelements. In this case, we perform r shifts of an element from one block into the\nprevious, so that the extra element in u becomes an extra element in u . We can\nr 0\nthenremovetheappropriateelementfromu \u2019sblock.\n0\n2. We quickly (in r+1 b steps) run off the end of the list of blocks. In this case, u\nr\n\u2264\nis the last block, and there is no requirement that u \u2019s block contain at least b 1\nr\n\u2212\nelements. Therefore,weproceedasabove,borrowinganelementfromu tomakean\nr\nextraelementinu . Ifthiscausesu \u2019sblocktobecomeempty,thenweremoveit.\n0 r\n3. After b steps we do not find any block containing more than b 1 elements. In\n\u2212\nthis case, u ,...,u is a sequence of b blocks that each contain b 1 elements. We\n0 b 1\n\u2212 \u2212\ngathertheseb(b 1)elementsintou ,...,u sothateachoftheseb 1blockscontains\n0 b 2\n\u2212 \u2212 \u2212\nexactlybelementsandweremoveu ,whichisnowempty. Nowu \u2019sblockcontains\nb 1 0\n\u2212\nbelementssowecanremovetheappropriateelementfromit.\nSEList\nT remove(int i) {\nLocation l; getLocation(i, l);\n70\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\nT y = l.u->d.get(l.j);\nNode *u = l.u;\nint r = 0;\nwhile (r < b && u != &dummy && u->d.size() == b - 1) {\nu = u->next;\nr++;\n}\nif (r == b) { // found b blocks each with b-1 elements\ngather(l.u);\n}\nu = l.u;\nu->d.remove(l.j);\nwhile (u->d.size() < b - 1 && u->next != &dummy) {\nu->d.add(u->next->d.remove(0));\nu = u->next;\n}\nif (u->d.size() == 0)\nremove(u);\nn--;\nreturn y;\n}\nLike the add(i,x) operation, the running time of the remove(i) operation is O(b+\nmin i,n i /b)ifweignorethecostofthegather(u)methodthatoccursinCase3.\n{ \u2212 }\n3.3.5 AmortizedAnalysisofSpreadingandGathering\nNext, we consider the cost of the gather(u) and spread(u) methods that may be executed\nbytheadd(i,x)andremove(i)methods. Forcompleteness,heretheyare:\nSEList\nvoid spread(Node *u) {\nNode *w = u;\nfor (int j = 0; j < b; j++) {\nw = w->next;\n}\nw = addBefore(w);\nwhile (w != u) {\nwhile (w->d.size() < b)\nw->d.add(0, w->prev->d.remove(w->prev->d.size()-1));\nw = w->prev;\n}\n}\n71\n3.LinkedLists 3.3.SEList: ASpace-EfficientLinkedList\nSEList\nvoid gather(Node *u) {\nNode *w = u;\nfor (int j = 0; j < b-1; j++) {\nwhile (w->d.size() < b)\nw->d.add(w->next->d.remove(0));\nw = w->next;\n}\nremove(w);\n}\nThe running time of each of these methods is dominated by the two nested loops.\nBoth the inner loop and outer loop execute at most b+1 times, so the total running time\nofeachofthesemethodsisO((b+1)2)=O(b2). However,thefollowinglemmashowsthat\nthesemethodsexecuteonatmostoneoutofeverybcallstoadd(i,x)orremove(i).\nLemma 3.1. If an empty SEList is created and any sequence of m 1 calls to add(i,x) and\n\u2265\nremove(i)areperformed,thenthetotaltimespentduringallcallstospread()andgather()is\nO(bm).\nProof. We will use the potential method of amortized analysis. We say that a node u is\nfragileifu\u2019sblockdoesnotcontainbelements(sothatuiseitherthelastnode,orcontains\nb 1 or b+1 elements). Any node whose block contains b elements is rugged. Define the\n\u2212\npotential of an SEList as the number of fragile nodes it contains. We will consider only\ntheadd(i,x)operationanditsrelationtothenumberofcallstospread(u). Theanalysisof\nremove(i)andgather(u)isidentical.\nNotice that, if Case 1 occurs during the add(i,x) method, then only one node, u\nr\nhasthesizeofitsblockchanged. Therefore,atmostonenode,namelyu ,goesfrombeing\nr\nrugged to being fragile. If Case 2 occurs, then a new node is created, and this node is\nfragile, but no other node changes sizes, so the number of fragile nodes increases by one.\nThus,ineitherCase1orCase2thepotentialoftheSEListincreasesbyatmost1.\nFinally,ifCase3occurs,itisbecauseu ,...,u areallfragilenodes. Thenspread(u )\n0 b 1 0\n\u2212\niscalledandthesebfragilenodesarereplacedwithb+1ruggednodes. Finally,xisadded\ntou \u2019sblock,makingu fragile. Intotalthepotentialdecreasesbyb 1.\n0 0\n\u2212\nIn summary, the potential starts at 0 (there are no nodes in the list). Each time\nCase1orCase2occurs,thepotentialincreasesbyatmost1. EachtimeCase3occurs,the\npotential decreases by b 1. The potential (which counts the number of fragile nodes) is\n\u2212\n72\n3.LinkedLists 3.4.DiscussionandExercises\nneverlessthan0. Weconcludethat,foreveryoccurrenceofCase3,thereareatleastb 1\n\u2212\noccurrencesofCase1orCase2. Thus,foreverycalltospread(u)thereareatleastbcalls\ntoadd(i,x). Thiscompletestheproof.\n3.3.6 Summary\nThefollowingtheoremsummarizestheperformanceoftheSEListdatastructure:\nTheorem3.3. AnSEListimplementstheListinterface. Ignoringthecostofcallstospread(u)\nandgather(u),anSEListwithblocksizebsupportstheoperations\n\u2022 get(i)andset(i,x)inO(1+min i,n i /b)timeperoperation;and\n{ \u2212 }\n\u2022 add(i,x)andremove(i)inO(b+min i,n i /b)timeperoperation.\n{ \u2212 }\nFurthermore, beginning with an empty SEList, any sequence of m add(i,x) and remove(i)\noperationsresultsinatotalofO(bm)timespentduringallcallstospread(u)andgather(u).\nThe space (measured in words)1 used by an SEList that stores n elements is n+O(b+\nn/b).\nThe SEList is a tradeoff between an ArrayList and a DLList where the relative\nmixofthesetwostructuresdependsontheblocksizeb. Attheextremeb=2,eachSEList\nnodestoresatmost3values,whichisreallynotmuchdifferentthanaDLList. Attheother\nextreme, b>n, all the elements are stored in a single array, just like in an ArrayList. In\nbetween these two extremes lies a tradeoff between the time it takes to add or remove a\nlistitemandthetimeittakestolocateaparticularlistitem.\n3.4 Discussion and Exercises\nBothsingly-linkedanddoubly-linkedlistsarefolklore,havingbeenusedinprogramsfor\nover 40 years. They are discussed, for example, by Knuth [41, Sections 2.2.3\u20132.2.5]. Even\ntheSEListdatastructureseemstobeawell-knowndatastructuresexercise.\nExercise 3.1. Why is it not possible, in an SLList to use a dummy node to avoid all the\nspecialcasesthatoccurintheoperationspush(x),pop(),add(x),andremove()?\nExercise 3.2. Design and implement an SLList method, secondLast(), that returns the\nsecond-last element of an SLList. Do this without using the member variable, n, that\nkeepstrackofthesizeofthelist.\n1RecallSection1.3foradiscussionofhowmemoryismeasured.\n73\n3.LinkedLists 3.4.DiscussionandExercises\nExercise3.3. Describe and implement the List operations get(i), set(i,x), add(i,x) and\nremove(i)onanSLList. EachoftheseoperationsshouldruninO(1+i)time.\nExercise3.4. DesignandimplementanSLListmethod,reverse()thatreversestheorder\nofelementsinanSLList. ThismethodshouldruninO(n)time,shouldnotuserecursion,\nshouldnotuseanysecondarydatastructures,andshouldnotcreateanynewnodes.\nExercise 3.5. Design and implement SLList and DLList methods called checkSize().\nThesemethodswalkthroughthelistandcountthenumberofnodestoseeifthismatches\nthe value, n, stored in the list. These methods return nothing, but throw an exception if\nthesizetheycomputedoesnotmatchthevalueofn.\nExercise3.6. Withoutreferringtothischapter,trytorecreatethecodefortheaddBefore(w)\noperation, that creates a node, u, and adds it just before the node w in a DLList. If your\ncodedoesnotexactlymatchthecodegiveninthisbookitmaystillbecorrect. Testitand\nseeifitworks.\nThenextfewexercisesinvolveperformingmanipulationsonDLLists. Theseshould\nallbedonewithoutallocatinganynewnodesortemporaryarrays. Morespecifically,they\ncanallbedoneonlybychangingtheprevandnextvaluesofexistingnodes.\nExercise 3.7. Write a DLList method isPalindrome() that returns true if the list is a\npalindrome, i.e., the element at position i is equal to the element at position n i 1 for\n\u2212 \u2212\nalli 0,...,n 1 . YourcodeshouldruninO(n)time.\n\u2208{ \u2212 }\nExercise 3.8. Implement a method rotate(r) that \u201crotates\u201d a DLList so that list item i\nbecomes list item (i+r) mod n. This method should run in O(1+min r,n r ) time and\n{ \u2212 }\nshouldnotmodifyanynodesinthelist.\nExercise 3.9. Write a method, truncate(i), that truncates a DLList at position i. After\nthe execution of this method, the size of the list is i and it contains only the elements at\nindices0,...,i 1. ThereturnvalueisanotherDLListthatcontainstheelementsatindices\n\u2212\ni,...,n 1. ThismethodshouldruninO(min i,n i )time.\n\u2212 { \u2212 }\nExercise3.10. WriteaDLListmethod,absorb(l2),thattakesasanargumentaDLList,l2,\nempties it and appends its contents, in order, to the receiver. For example, if l1 contains\na,b,c and l2 contains d,e,f, then after calling l1.absorb(l2), l1 will contain a,b,c,d,e,f\nandl2willbeempty.\n74\n3.LinkedLists 3.4.DiscussionandExercises\nExercise 3.11. Write a method deal() that removes all the elements with odd-numbered\nindicesfromaDLListandreturnaDLListcontainingtheseelements. Forexample,ifl1,\ncontainstheelementsa,b,c,d,e,f,thenaftercallingl1.deal(),l1shouldcontaina,c,eand\nalistcontainingb,d,f shouldbereturned.\nExercise3.12. Writeamethod,reverse(),thatreversestheorderofelementsinaDLList.\nExercise3.13. This exercises walks you through an implementation of the merge sort al-\ngorithmforsortingaDLList,asdiscussedinSection11.1.1.\n1. WriteaDLListmethodcalledtakeFirst(l2). Thismethodtakesthefirstnodefrom\nl2andappendsittothethereceivinglist. Thisisequivalenttoadd(size(),l2.remove(0)),\nexceptthatitshouldnotcreateanewnode.\n2. Write a DLList static method, merge(l1,l2), that takes two sorted lists l1 and l2,\nmergesthem,andreturnsanewsortedlistcontainingtheresult. Thiscausesl1and\nl2 to be emptied in the proces. For example, if l1 contains a,c,d and l2 contains\nb,e,f,thenthismethodreturnsanewlistcontaininga,b,c,d,e,f.\n3. WriteaDLListmethodsort()thatsortstheelementscontainedinthelistusingthe\nmergesortalgorithm. Thisrecursivealgorithmworksasfollowing:\n(a) Ifthelistcontains0or1elementsthenthereisnothingtodo. Otherwise,\n(b) Split the list into two approximately equal length lists l1 and l2 using the\ntruncate(size()/2)method;\n(c) Recursivelysortl1;\n(d) Recursivelysortl2;and,finally,\n(e) Mergel1andl2intoasinglesortedlist.\nThe next few exercises are more advanced and require a clear understanding of\nwhat happens to the minimum value stored in a Stack or Queue as items are added and\nremoved.\nExercise3.14. DesignandimplementaMinStackdatastructurethatcanstorecomparable\nelementsandsupportsthestackoperationspush(x),pop(),andsize(),aswellasthemin()\noperation, which returns the minimum value currently stored in the data structure. All\noperationsshouldruninconstanttime.\n75\n3.LinkedLists 3.4.DiscussionandExercises\nExercise3.15. DesignanimplementaMinQueuedatastructurethatcanstorecomparable\nelements and supports the queue operations add(x), remove(), and size(), as well as the\nmin()operation, whichreturnstheminimumvaluecurrentlystoredinthedatastructure.\nAlloperationsshouldruninconstantamortizedtime.\nExercise 3.16. Design an implement a MinDeque data structure that can store compara-\nble elements and supports the queue operations addFirst(x), addLast(x) removeFirst(),\nremoveLast()andsize(),aswellasthemin()operation,whichreturnstheminimumvalue\ncurrently stored in the data structure. All operations should run in constant amortized\ntime.\nThe next exercises are designed to test the reader\u2019s understanding of the imple-\nmentationananalysisofthespace-efficientSEList:\nExercise3.17. Prove that, if an SEList is used like a Stack (so that the modifications are\ndone using push(x) add(size(),x) and pop() remove(size() 1)) then these operations\n\u2261 \u2261 \u2212\nruninconstantamortizedtime,independentofthevalueofb.\nExercise3.18. DesignanimplementofaversionofanSEListthatsupportsalltheDeque\noperationsinconstantamortizedtimeperoperation,independentofthevalueofb.\n76\nChapter 4\nSkiplists\nIn this chapter, we discuss a beautiful data structure: the skiplist, that has a variety of\napplications. Using a skiplist we can implement a List that is fast for all the operations\nget(i), set(i,x), add(i,x), and remove(i). We can also implement an SSet in which all\noperationsruninO(logn)expectedtime.\nSkiplists rely on randomization for their efficiency. In particular, a skiplist uses\nrandom coin tosses when an element is inserted to determine the height of that element.\nThe performance of skiplists is expressed in terms of expected running times and lengths\nofpaths. Thisexpectationistakenovertherandomcointossesusedbytheskiplist. Inthe\nimplementation, the random coin tosses used by a skiplist are simulated using a pseudo-\nrandomnumber(orbit)generator.\n4.1 The Basic Structure\nConceptually, a skiplist is a sequence of singly-linked lists L ,...,L , where each L con-\n0 h r\ntains a subset of the items in L . We start with the input list L that contains n items\nr 1 0\n\u2212\nand construct L from L , L from L , and so on. The items in L are obtained by tossing\n1 0 2 1 r\na coin for each element, x, in L and including x in L if the coin comes up heads. This\nr 1 r\n\u2212\nprocess ends when we create a list L that is empty. An example of a skiplist is shown in\nr\nFigure4.1.\nFor an element, x, in a skiplist, we call the height of x the largest value r such that\nx appears in L . Thus, for example, elements that only appear in L have height 0. If we\nr 0\nspend a few moments thinking about it, we notice that the height of x corresponds to the\nfollowing experiment: Toss a coin repeatedly until the first time it comes up tails. How\nmanytimesdiditcomeupheads? Theanswer,notsurprisingly,isthattheexpectedheight\nofanodeis1. (Weexpecttotossthecointwicebeforegettingtails,butwedon\u2019tcountthe\nlasttoss.) Theheightofaskiplististheheightofitstallestnode.\n77\n4.Skiplists 4.1.TheBasicStructure\nL\n5\nL\n4\nL\n3\nL\n2\nL\n1\nL 0 1 2 3 4 5 6\n0\nsentinel\nFigure4.1: Askiplistcontainingsevenelements.\nL\n5\nL\n4\nL\n3\nL\n2\nL\n1\nL 0 1 2 3 4 5 6\n0\nsentinel\nFigure4.2: Thesearchpathforthenodecontaining4inaskiplist.\nAttheheadofeverylistisaspecialnode,calledthesentinel,thatactsasadummy\nnodeforthelist. Thekeypropertyofskiplistsisthatthereisashortpath,calledthesearch\npath,fromthesentinelinL toeverynodeinL . Rememberinghowtoconstructasearch\nh 0\npath for a node, u, is easy (see Figure 4.2) : Start at the top left corner of your skiplist\n(the sentinel in L ) and always go right unless that would overshoot u, in which case you\nh\nshouldtakeastepdownintothelistbelow.\nMore precisely, to construct the search path for the node u in L we start at the\n0\nsentinel,w,inL . Next,weexaminew.next. Ifw.nextcontainsanitemthatappearsbefore\nh\nu in L , then we set w=w.next. Otherwise, we move down and continue the search at the\n0\noccurrence of w in the list L . We continue this way until we reach the predecessor of u\nh 1\n\u2212\ninL .\n0\nThefollowingresult,whichwewillproveinSection4.4,showsthatthesearchpath\nisquiteshort:\nLemma 4.1. The expected length of the search path for any node, u, in L is at most 2logn+\n0\nO(1)=O(logn).\n78\n4.Skiplists 4.2.SkiplistSSet: AnEfficientSSetImplementation\nAspace-efficientwaytoimplementaSkiplististodefineaNode,u,asconsisting\nof a data value, x, and an array, next, of pointers, where u.next[i] points to u\u2019s successor\nin the list L . In this way, the data, x, in a node is stored only once, even though x may\ni\nappearinseverallists.\nSkiplistSSet\nstruct Node {\nT x;\nint height; // length of next\nNode *next[];\n};\nThenexttwosectionsofthischapterdiscusstwodifferentapplicationsofskiplists.\nIn each of these applications, L stores the main structure (a list of elements or a sorted\n0\nset of elements). The primary difference between these structures is in how a search path\nisnavigated;inparticular,theydifferinhowtheydecideifasearchpathshouldgodown\nintoL orgorightwithinL .\nr 1 r\n\u2212\n4.2 SkiplistSSet: An Efficient SSet Implementation\nASkiplistSSetusesaskipliststructuretoimplementtheSSetinterface. Whenusedthis\nway,thelistL storestheelementsoftheSSetinsortedorder. Thefind(x)methodworks\n0\nbyfollowingthesearchpathforthesmallestvalueysuchthaty x:\n\u2265\nSkiplistSSet\nNode* findPredNode(T x) {\nNode *u = sentinel;\nint r = h;\nwhile (r >= 0) {\nwhile ( u->next[r] != NULL && compare(u->next[r]->x, x) < 0)\nu = u->next[r]; // go right in list r\nr--; // go down into list r-1\n}\nreturn u;\n}\nT find(T x) {\nNode *u = findPredNode(x);\nreturn u->next[0] == NULL ? NULL : u->next[0]->x;\n}\nFollowing the search path for y is easy: when situated at some node, u, in L , we\nr\nlookrighttou.next[r].x. Ifx>u.next[r].x,thenwetakeasteptotherightinL ,otherwise\nr\n79\n4.Skiplists 4.2.SkiplistSSet: AnEfficientSSetImplementation\nwemovedownintoL . Eachstep(rightordown)inthissearchtakesonlyconstanttime\nr 1\n\u2212\nso,byLemma4.1,theexpectedrunningtimeoffind(x)isO(logn).\nBefore we can add an element to a SkipListSSet, we need a method to simulate\ntossing coins to determine the height, k, of a new node. We do this by picking a random\ninteger,z,andcountingthenumberoftrailing1sinthebinaryrepresentationofz:1\nSkiplistSSet\nint pickHeight() {\nint z = rand();\nint k = 0;\nint m = 1;\nwhile ((z & m) != 0) {\nk++;\nm <<= 1;\n}\nreturn k;\n}\nToimplementtheadd(x)methodinaSkiplistSSetwesearchforxandthensplice\nxintoafewlistsL ,...,L ,wherekisselectedusingthepickHeight()method. Theeasiest\n0 k\nway to do this is to use an array, stack, that keeps track of the nodes at which the search\npath goes down from some list L into L . More precisely, stack[r] is the node in L\nr r 1 r\n\u2212\nwherethesearchpathproceededdownintoL . Thenodesthatwemodifytoinsertxare\nr 1\n\u2212\nprecisely the nodes stack[0],...,stack[k]. The following code implements this algorithm\nforadd(x):\nSkiplistSSet\nbool add(T x) {\nNode *u = sentinel;\nint r = h;\nint comp = 0;\nwhile (r >= 0) {\nwhile (u->next[r] != NULL && (comp = compare(u->next[r]->x, x)) < 0)\nu = u->next[r];\nif (u->next[r] != NULL && comp == 0)\nreturn false;\nstack[r--] = u; // going down, store u\n}\nNode *w = newNode(x, pickHeight());\nwhile (h < w->height)\n1Thismethoddoesnotexactlyreplicatethecoin-tossingexperimentsincethevalueofkwillalwaysbeless\nthanthenumberofbitsinanint. However,thiswillhavenegligibleimpactunlessthenumberofelements\ninthestructureismuchgreaterthan232=4294967296.\n80\n4.Skiplists 4.2.SkiplistSSet: AnEfficientSSetImplementation\n0 1 2 3 3.5 4 5 6\nsentinel add(3.5)\nFigure 4.3: Adding the node containing 3.5 to a skiplist. The nodes stored in stack are\nhighlighted.\nstack[++h] = sentinel; // increasing height of skiplist\nfor (int i = 0; i < w->height; i++) {\nw->next[i] = stack[i]->next[i];\nstack[i]->next[i] = w;\n}\nn++;\nreturn true;\n}\nRemoving an element, x, is done in a similar way, except that there is no need for\nstack to keep track of the search path. The removal can be done as we are following the\nsearchpath. Wesearchforxandeachtimethesearchmovesdownwardfromanodeu,we\ncheckifu.next.x=xandifso,wespliceuoutofthelist:\nSkiplistSSet\nbool remove(T x) {\nbool removed = false;\nNode *u = sentinel, *del;\nint r = h;\nint comp = 0;\nwhile (r >= 0) {\nwhile (u->next[r] != NULL && (comp = compare(u->next[r]->x, x)) < 0) {\nu = u->next[r];\n}\nif (u->next[r] != NULL && comp == 0) {\nremoved = true;\ndel = u->next[r];\nu->next[r] = u->next[r]->next[r];\nif (u == sentinel && u->next[r] == NULL)\nh--; // skiplist height has gone down\n}\n81\n4.Skiplists 4.3.SkiplistList: AnEfficientRandom-AccessListImplementation\n0 1 2 3 4 5 6\nsentinel remove(3)\nFigure4.4: Removingthenodecontaining3fromaskiplist.\nr--;\n}\nif (removed) {\ndelete del;\nn--;\n}\nreturn removed;\n}\n4.2.1 Summary\nThefollowingtheoremsummarizestheperformanceofskiplistswhenusedtoimplement\nsortedsets:\nTheorem4.1. ASkiplistSSetimplementstheSSetinterface. ASkiplistSSetsupportsthe\noperationsadd(x),remove(x),andfind(x)inO(logn)expectedtimeperoperation.\n4.3 SkiplistList: An Efficient Random-Access List Implementation\nASkiplistListimplementstheListinterfaceontopofaskipliststructure. InaSkiplistList,\nL contains the elements of the list in the order they appear in the list. Just like with a\n0\nSkiplistSSet,elementscanbeadded,removed,andaccessedinO(logn)time.\nForthistobepossible,weneedawaytofollowthesearchpathfortheithelement\ninL . Theeasiestwaytodothisistodefinethenotionofthelengthofanedgeinsomelist,\n0\nL . We define the length of every edge in L as 1. The length of an edge, e, in L , r > 0,\nr 0 r\nisdefinedasthesumofthelengthsoftheedgesbeloweinL . Equivalently, thelength\nr 1\n\u2212\nofeisthenumberofedgesinL belowe. SeeFigure4.5foranexampleofaskiplistwith\n0\nthelengthsofitsedgesshown. Sincetheedgesofskiplistsarestoredinarrays,thelengths\ncanbestoredthesameway:\n82\n4.Skiplists 4.3.SkiplistList: AnEfficientRandom-AccessListImplementation\n5\nL\n5\n5\nL\n4\n3 2\nL\n3\n3 1 1\nL\n2\n3 1 1 1 1\nL\n1\n1 1 1 1 1 1 1\nL 0 1 2 3 4 5 6\n0\nsentinel\nFigure4.5: Thelengthsoftheedgesinaskiplist.\nSkiplistList\nstruct Node {\nT x;\nint height; // length of next\nint *length;\nNode **next;\n};\nTheusefulpropertyofthisdefinitionoflengthisthat,ifwearecurrentlyatanode\nthatisatpositionjinL andwefollowanedgeoflength(cid:96),thenwemovetoanodewhose\n0\nposition,inL ,isj+(cid:96). Inthisway,whilefollowingasearchpath,wecankeeptrackofthe\n0\nposition, j, of the current node in L . When at a node, u, in L , we go right if j plus the\n0 r\nlengthoftheedgeu.next[r]islessthani,otherwisewegodownintoL .\nr 1\n\u2212\nSkiplistList\nNode* findPred(int i) {\nNode *u = sentinel;\nint r = h;\nint j = -1; // the index of the current node in list 0\nwhile (r >= 0) {\nwhile (u->next[r] != NULL && j + u->length[r] < i) {\nj += u->length[r];\nu = u->next[r];\n}\nr--;\n}\nreturn u;\n}\nSkiplistList\nT get(int i) {\nreturn findPred(i)->next[0]->x;\n83\n4.Skiplists 4.3.SkiplistList: AnEfficientRandom-AccessListImplementation\n56\n56\n3 232 1\n3 1 121 1\n3 1 121 1 1 1\n1 1 1 1 121 1 1 1\n0 1 2 3 x 4 5 6\nsentinel add(4,x)\nFigure4.6: AddinganelementtoaSkiplistList.\n}\nT set(int i, T x) {\nNode *u = findPred(i)->next[0];\nT y = u->x;\nu->x = x;\nreturn y;\n}\nSincethehardestpartoftheoperationsget(i)andset(i,x)isfindingtheithnode\ninL ,theseoperationsruninO(logn)time.\n0\nAdding an element to a SkiplistList at a position, i, is fairly straightforward.\nUnlike in a SkiplistSSet, we are sure that a new node will actually be added, so we can\ndotheadditionatthesametimeaswesearchforthenewnode\u2019slocation. Wefirstpickthe\nheight, k, of the newly inserted node, w, and then follow the search path for i. Anytime\nthe search path moves down from L with r k, we splice w into L . The only extra care\nr r\n\u2264\nneededistoensurethatthelengthsofedgesareupdatedproperly. SeeFigure4.6.\nNote that, each time the search path goes down at a node, u, in L , the length of\nr\nthe edge u.next[r] increases by one, since we are adding an element below that edge at\npositioni. Splicingthenodewbetweentwonodes,uandz,worksasshowninFigure4.7.\nWhilefollowingthesearchpathwearealreadykeepingtrackoftheposition,j,ofuinL .\n0\nTherefore,weknowthatthelengthoftheedgefromutowisi j. Wecanalsodeducethe\n\u2212\nlength of the edge from w to z from the length, (cid:96), of the edge from u to z. Therefore, we\ncanspliceinwandupdatethelengthsoftheedgesinconstanttime.\nThis sounds more complicated than it actually is and the code is actually quite\nsimple:\n84\n4.Skiplists 4.3.SkiplistList: AnEfficientRandom-AccessListImplementation\nu z\n\u2018\nj\n\u2018+1\nu w z\ni j \u2018+1 (i j)\n\u2212 \u2212 \u2212\nj i\nFigure4.7: Updatingthelengthsofedgeswhilesplicinganodewintoaskiplist.\nSkiplistList\nvoid add(int i, T x) {\nNode *w = newNode(x, pickHeight());\nif (w->height > h)\nh = w->height;\nadd(i, w);\n}\nSkiplistList\nNode* add(int i, Node *w) {\nNode *u = sentinel;\nint k = w->height;\nint r = h;\nint j = -1; // index of u\nwhile (r >= 0) {\nwhile (u->next[r] != NULL && j + u->length[r] < i) {\nj += u->length[r];\nu = u->next[r];\n}\nu->length[r]++; // to account for new node in list 0\nif (r <= k) {\nw->next[r] = u->next[r];\nu->next[r] = w;\nw->length[r] = u->length[r] - (i - j);\nu->length[r] = i - j;\n}\nr--;\n}\nn++;\nreturn u;\n}\nBynow,theimplementationoftheremove(i)operationinaSkiplistListshould\n85\n4.Skiplists 4.3.SkiplistList: AnEfficientRandom-AccessListImplementation\n54\nL\n5\n54\nL\n4\n3 21\nL\n3 1\n3 1 1\nL\n2 1\n3 1 1 1 1\nL\n1 1\n1 1 1 1 1 1 1\nL 0 1 2 3 4 5 6\n0\nsentinel remove(3)\nFigure4.8: RemovinganelementfromaSkiplistList.\nbeobvious. Wefollowthesearchpathforthenodeatpositioni. Eachtimethesearchpath\ntakesastepdownfromanode,u,atlevelrwedecrementthelengthoftheedgeleavingu\nat that level. We also check if u.next[r] is the element of rank i and, if so, splice it out of\nthelistatthatlevel. AnexampleisshowninFigure4.8.\nSkiplistList\nT remove(int i) {\nT x = NULL;\nNode *u = sentinel, *del;\nint r = h;\nint j = -1; // index of node u\nwhile (r >= 0) {\nwhile (u->next[r] != NULL && j + u->length[r] < i) {\nj += u->length[r];\nu = u->next[r];\n}\nu->length[r]--; // for the node we are removing\nif (j + u->length[r] + 1 == i && u->next[r] != NULL) {\nx = u->next[r]->x;\nu->length[r] += u->next[r]->length[r];\ndel = u->next[r];\nu->next[r] = u->next[r]->next[r];\nif (u == sentinel && u->next[r] == NULL)\nh--;\n}\nr--;\n}\ndeleteNode(del);\nn--;\nreturn x;\n}\n86\n4.Skiplists 4.4.AnalysisofSkiplists\n4.3.1 Summary\nThefollowingtheoremsummarizestheperformanceoftheSkiplistListdatastructure:\nTheorem4.2. ASkiplistListimplementstheListinterface. ASkiplistListsupportsthe\noperationsget(i),set(i,x),add(i,x),andremove(i)inO(logn)expectedtimeperoperation.\n4.4 Analysis of Skiplists\nIn this section, we analyze the expected height, size, and length of the search path in a\nskiplist. Thissectionrequiresabackgroundinbasicprobability. Severalproofsarebased\nonthefollowingbasicobservationaboutcointosses.\nLemma4.2. LetT bethenumberoftimesafaircoinistosseduptoandincludingthefirsttime\nthecoincomesupheads. ThenE[T]=2.\nProof. Supposewestoptossingthecointhefirsttimeitcomesupheads. Definetheindi-\ncatorvariable\n0 ifthecoinistossedlessthani times\nI =\ni\n\uf8f1 1 ifthecoinistossedi ormoretimes\n\uf8f4\uf8f4\uf8f2\nNotethatI\ni\n=1ifandonly\uf8f4\uf8f4\uf8f3 ifthefirsti\n\u2212\n1cointossesaretails,soE[I\ni\n]=Pr\n{\nI\ni\n=1\n}\n=1/2i\n\u2212\n1.\nObservethatT,thetotalnumberofcointosses,canbewrittenasT = I . Therefore,\n\u221ei=1 i\n\u221e (cid:80)\nE[T]=E I\ni\n\uf8ee \uf8f9\ni=1\n=\n\u221e\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 (cid:88)\nE[I ]\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ni\ni=1\n(cid:88)\n\u221e\n= 1/2i 1\n\u2212\ni=1\n(cid:88)\n=1+1/2+1/4+1/8+\n\u00b7\u00b7\u00b7\n=2 .\nThenexttwolemmatatellusthatskiplistshavelinearsize:\nLemma 4.3. The expected number of nodes in a skiplist containing n elements, not including\noccurrencesofthesentinel,is2n.\nProof. The probability that any particular element, x, is included in list L is 1/2r, so the\nr\nexpectednumberofnodesinL isn/2r.2 Therefore,thetotalexpectednumberofnodesin\nr\n2SeeSection1.2.4toseehowthisisderivedusingindicatorvariablesandlinearityofexpectation.\n87\n4.Skiplists 4.4.AnalysisofSkiplists\nalllistsis\n\u221e n/2r=n(1+1/2+1/4+1/8+ )=2n .\n\u00b7\u00b7\u00b7\nr=0\n(cid:88)\nLemma4.4. Theexpectedheightofaskiplistcontainingnelementsisatmostlogn+2.\nProof. Foreachr 1,2,3,..., ,definetheindicatorrandomvariable\n\u2208{ \u221e}\n0 ifL isempty\nr\nI =\nr\n\uf8f1 1 ifL r isnon-empty\n\uf8f4\uf8f4\uf8f2\nTheheight,h,oftheskiplististhen\uf8f4\uf8f4\uf8f3 givenby\n\u221e\nh= I .\nr\ni=1\n(cid:88)\nNotethatI isnevermorethanthelength, L ,ofL ,so\nr r r\n| |\nE[I ] E[L ]=n/2r .\nr r\n\u2264 | |\nTherefore,wehave\n\u221e\nE[h]=E I\nr\n\uf8ee \uf8f9\nr=1\n=\n\u221e\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 (cid:88)\nE[I ]\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nr\nr=1\n(cid:88)\nlogn\n(cid:98) (cid:99) \u221e\n= E[I ]+ E[I ]\nr r\nr=1 r= logn +1\n(cid:88) (cid:98)(cid:88)(cid:99)\nlogn\n(cid:98) (cid:99) 1+ \u221e n/2r\n\u2264\nr=1 r= logn +1\n(cid:88) (cid:98)(cid:88)(cid:99)\nlogn+ \u221e 1/2r\n\u2264\nr=0\n(cid:88)\n=logn+2 .\nLemma 4.5. The expected number of nodes in a skiplist containing n elements, including all\noccurrencesofthesentinel,is2n+O(logn).\nProof. By Lemma 4.3, the expected number of nodes, not including the sentinel, is 2n.\nThe number of occurrences of the sentinel is equal to the height, h, of the skiplist so,\nby Lemma 4.4 the expected number of occurrences of the sentinel is at most logn+2 =\nO(logn).\n88\n4.Skiplists 4.4.AnalysisofSkiplists\nLemma4.6. Theexpectedlengthofasearchpathinaskiplistisatmost2logn+O(1).\nProof. The easiest way to see this is to consider the reverse search path for a node, x. This\npath starts at the predecessor of x in L . At any point in time, if the path can go up a\n0\nlevel,thenitdoes. Ifitcannotgoupalevelthenitgoesleft. Thinkingaboutthisforafew\nmomentswillconvinceusthatthereversesearchpathforxisidenticaltothesearchpath\nforx,exceptthatitisreversed.\nThe number of nodes that the reverse search path visits at a particular level, r, is\nrelated to the following experiment: Toss a coin. If the coin comes up heads then go up\nand stop, otherwise go left and repeat the experiment. The number of coin tosses before\nthe heads then represents the number of steps to the left that a reverse search path takes\nat a particular level.3 Lemma 4.2 tells us that the expected number of coin tosses before\nthefirstheadsis1.\nLet S denote the number of steps the forward search path takes at level r that go\nr\ntotheright. WehavejustarguedthatE[S ] 1. Furthermore,S L ,sincewecan\u2019ttake\nr r r\n\u2264 \u2264| |\nmorestepsinL thanthelengthofL ,so\nr r\nE[S ] E[L ]=n/2r .\nr r\n\u2264 | |\nWecannowfinishasintheproofofLemma4.4. LetS bethelengthofthesearchpathfor\nsomenode,u,inaskiplist,andlethbetheheightoftheskiplist. Then\n\u221e\nE[S]=E h+ S\nr\n\uf8ee r=0 \uf8f9\n=E[\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0h]+ (cid:88)\n\u221e\nE[\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fbS\n]\nr\nr=0\n(cid:88)\nlogn\n(cid:98) (cid:99) \u221e\n=E[h]+ E[S ]+ E[S ]\nr r\nr=0 r= logn +1\n(cid:88) (cid:98)(cid:88)(cid:99)\nlogn\nE[h]+ (cid:98) (cid:99) 1+ \u221e n/2r\n\u2264\nr=0 r= logn +1\n(cid:88) (cid:98)(cid:88)(cid:99)\nlogn\nE[h]+ (cid:98) (cid:99) 1+ \u221e 1/2r\n\u2264\nr=0 r=0\n(cid:88) (cid:88)\n3Notethatthismightovercountthenumberofstepstotheleft,sincetheexperimentshouldendeitherat\nthefirstheadsorwhenthesearchpathreachesthesentinel,whichevercomesfirst.Thisisnotaproblemsince\nthelemmaisonlystatinganupperbound.\n89\n4.Skiplists 4.5.DiscussionandExercises\nlogn\nE[h]+ (cid:98) (cid:99) 1+ \u221e 1/2r\n\u2264\nr=0 r=0\n(cid:88) (cid:88)\nE[h]+logn+3\n\u2264\n2logn+5 .\n\u2264\nThefollowingtheoremsummarizestheresultsinthissection:\nTheorem4.3. A skiplist containing n elements has expected size O(n) and the expected length\nofthesearchpathforanyparticularelementisatmost2logn+O(1).\n4.5 Discussion and Exercises\nSkiplists were introduced by Pugh [54] who also presented a number of applications of\nskiplists [53]. Since then they have been studied extensively. Several researchers have\ndoneverypreciseanalysisoftheexpectedlengthandvarianceinlengthofthesearchpath\nfor the ith element in a skiplist [40, 39, 51]. Deterministic versions [48], biased versions\n[7, 23], and self-adjusting versions [10] of skiplists have all been developed. Skiplist im-\nplementations have been written for various languages and frameworks and have seen\nuse in open-source database systems [62, 56]. A variant of skiplists is used in the HP-UX\noperatingsystemkernel\u2019sprocessmanagementstructures[38].\nExercise4.1. Illustratethesearchpathsfor2.5and5.5ontheskiplistinFigure4.1.\nExercise 4.2. Illustrate the addition of the values 0.5 (with height 1) and then 3.5 (with\nheight2)totheskiplistinFigure4.1.\nExercise 4.3. Illustrate the removal of the values 1 and then 3 from the skiplist in Fig-\nure4.1.\nExercise4.4. Illustratetheexecutionofremove(2)ontheSkiplistListinFigure4.5.\nExercise 4.5. Illustrate the execution of add(3,x) on the SkiplistList in Figure 4.5. As-\nsumethatpickHeight()selectsaheightof4forthenewlycreatednode.\nExercise4.6. Showthat, duringanadd(x)oraremove(x)operation, theexpectednumber\nofpointersinaSkiplistSetthatgetchangedisconstant.\nExercise4.7. Supposethat, insteadofpromotinganelementfromL intoL basedona\ni 1 i\n\u2212\ncointoss,wepromoteitwithsomeprobabilityp,0<p<1.\n1. Showthattheexpectedlengthofasearchpathisatmost(1/p)log n+O(1).\n1/p\n90\n4.Skiplists 4.5.DiscussionandExercises\n2. Whatisthevalueofp thatminimizestheprecedingexpression?\n3. Whatistheexpectedheightoftheskiplist?\n4. Whatistheexpectednumberofnodesintheskiplist?\nExercise4.8. The find(x) method in a SkiplistSet sometimes performs redundant com-\nparisons;theseoccurwhenxiscomparedtothesamevaluemorethanonce. Theycanoccur\nwhen,forsomenode,u,u.next[r]=u.next[r 1]. Showhowtheseredundantcomparisons\n\u2212\nhappen and modify find(x) so that they are avoided. Analyze the expected number of\ncomparisonsdonebyyourmodifiedfind(x)method.\nExercise4.9. DesignandimplementaversionofaskiplistthatimplementstheSSetinter-\nface,butalsoallowsfastaccesstoelementsbyrank. Thatis,italsosupportsthefunction\nget(i),whichreturnstheelementwhoserankisiinO(logn)expectedtime. (Therankof\nanelementxinanSSetisthenumberofelementsintheSSetthatarelessthanx.)\nExercise 4.10. A finger in a skiplist is an array that stores the sequence of nodes on a\nsearchpathatwhichthesearchpathgoesdown. (Thevariablestackintheadd(x)codeon\npage 80 is a finger; the shaded nodes in Figure 4.3 show the contents of the finger.) One\ncanthinkofafingeraspointingoutthepathtoanodeinthelowestlist,L .\n0\nA finger search implements the find(x) operation using a finger, by walking up\nthe list using the finger until reaching a node u such that u.x < x and u.next = null or\nu.next.x > x and then performing a normal search for x starting from u. It is possible to\nprovethattheexpectednumberofstepsrequiredforafingersearchisO(1+logr),where\nr isthenumbervaluesinL betweenxandthevaluepointedtobythefinger.\n0\nImplementasubclassofSkiplistcalledSkiplistWithFingerthatdoesallfind(x)\noperations using an internal finger. This class stores a finger and does every search as a\nfinger search. During the search it also updates the finger so that each find(x) operation\nuses,asastartingpoint,afingerthatpointstotheresultofthepreviousfind(x)operation.\nExercise4.11. Writeamethod,truncate(i),thattruncatesaSkiplistListatpositioni.\nAftertheexecutionofthismethod,thesizeofthelistisianditcontainsonlytheelements\natindices0,...,i 1. ThereturnvalueisanotherSkiplistListthatcontainstheelements\n\u2212\natindicesi,...,n 1. ThismethodshouldruninO(logn)time.\n\u2212\nExercise 4.12. Write a SkiplistList method, absorb(l2), that takes as an argument a\nSkiplistList, l2, empties it and appends its contents, in order, to the receiver. For ex-\n91\n4.Skiplists 4.5.DiscussionandExercises\nample,ifl1containsa,b,c andl2containsd,e,f,thenaftercallingl1.absorb(l2),l1will\ncontaina,b,c,d,e,f andl2willbeempty. ThismethodshouldruninO(logn)time.\nExercise4.13. Usingtheideasfromthespace-efficientlinked-list,SEList,designandim-\nplementaspace-efficientSSet,SESSet. Dothisbystoringthedata,inorder,inanSEList\nandthenstoringtheblocksofthisSEListinanSSet. IftheoriginalSSetimplementation\nusesO(n)spacetostorenelements,thentheSESSetwilluseenoughspacefornelements\nplusO(n/b+b)wastedspace.\nExercise4.14. UsinganSSetasyourunderlyingstructure, designandimplementanap-\nplication that reads a (large) text file and allow you to search, interactively, for any sub-\nstring contained in the text. As the user types their query, a matching part of the text (if\nany)shouldappearasaresult.\nHint 1: Every substring is a prefix of some suffix, so it suffices to store all suffixes of the\ntextfile.\nHint 2: Any suffix can be represented compactly as a single integer indicating where the\nsuffixbeginsinthetext.\nTestyourapplicationonsomelargetextslikesomeofthebooksavailableatProjectGuten-\nberg [1]. If done correctly, your applications will be very responsive; there should be no\nnoticeablelagbetweentypingkeystrokesandtheresultsappearing.\nExercise 4.15. (This excercise is to be done after reading about binary search trees, in\nSection6.2.) Compareskiplistswithbinarysearchtreesinthefollowingways:\n1. Explain how removing some edges of a skiplist lead to a structure that looks like a\nbinarytreeandthatissimilartoabinarysearchtree.\n2. Skiplistsandbinarysearchtreeseachuseaboutthesamenumberofpointers(2per\nnode). Skiplistsmakebetteruseofthosepointers,though. Explainwhy.\n92\nChapter 5\nHash Tables\nHash tables are an efficient method of storing a small number, n, of integers from a large\nrange U = 0,...,2w 1 . The term hash table includes a broad range of data structures.\n{ \u2212 }\nThischapterfocusesononeofthemostcommonimplementationsofhashtables,namely\nhashingwithchaining.\nVeryoftenhashtablesstoredatathatarenotintegers. Inthiscase,anintegerhash\ncode is associated with each data item and this hash code is used in the hash table. The\nsecondpartofthischapterdiscusseshowsuchhashcodesaregenerated.\nSome of the methods used in this chapter require random choices of integers in\nsomespecificrange. Inthecodesamples,someofthese\u201crandom\u201dintegersarehard-coded\nconstants. Theseconstants were obtained using random bitsgenerated from atmospheric\nnoise.\n5.1 ChainedHashTable: Hashing with Chaining\nAChainedHashTabledatastructureuseshashingwithchainingtostoredataasanarray,t,\noflists. Aninteger,n,keepstrackofthetotalnumberofitemsinalllists(seeFigure5.1):\nChainedHashTable\narray<List> t;\nint n;\nThehashvalueofadataitemx,denotedhash(x)isavalueintherange 0,...,t.length 1 .\n{ \u2212 }\nAll items with hash value i are stored in the list at t[i]. To ensure that lists don\u2019t get too\nlong,wemaintaintheinvariant\nn t.length\n\u2264\nsothattheaveragenumberofelementsstoredinoneoftheselistsisn/t.length 1.\n\u2264\n93\n5.HashTables 5.1.ChainedHashTable: HashingwithChaining\nt 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nb d i x h j f m \u2018 k\nc g e\na\nFigure 5.1: An example of a ChainedHashTable with n = 14 and t.length = 16. In this\nexamplehash(x)=6\nToaddanelement,x,tothehashtable,wefirstcheckifthelengthoftneedstobe\nincreasedand,ifso,wegrowt. Withthisoutofthewaywehashxtogetaninteger,i,in\ntherange 0,...,t.length 1 andweappendxtothelistt[i]:\n{ \u2212 }\nChainedHashTable\nbool add(T x) {\nif (find(x) != null) return false;\nif (n+1 > t.length) resize();\nt[hash(x)].add(x);\nn++;\nreturn true;\n}\nGrowing the table, if necessary, involves doubling the length of t and reinserting all el-\nements into the new table. This is exactly the same strategy used in the implementation\nof ArrayStack and the same result applies: The cost of growing is only constant when\namortizedoverasequenceofinsertions(seeLemma2.1onpage32).\nBesidesgrowing,theonlyotherworkdonewhenaddingxtoaChainedHashTable\ninvolves appending x to the list t[hash(x)]. For any of the list implementations described\ninChapters2or3,thistakesonlyconstanttime.\nToremoveanelementxfromthehashtableweiterateoverthelistt[hash(x)]until\nwefindxsothatwecanremoveit:\nChainedHashTable\nT remove(T x) {\nint j = hash(x);\nfor (int i = 0; i < t[j].size(); i++) {\nT y = t[j].get(i);\nif (x == y) {\nt[j].remove(i);\nn--;\nreturn y;\n94\n5.HashTables 5.1.ChainedHashTable: HashingwithChaining\n}\n}\nreturn null;\n}\nThistakesO(n )time,wheren denotesthelengthoftheliststoredatt[i].\nhash(x) i\nSearching for the element x in a hash table is similar. We perform a linear search\nonthelistt[hash(x)]:\nChainedHashTable\nT find(T x) {\nint j = hash(x);\nfor (int i = 0; i < t[j].size(); i++)\nif (x == t[j].get(i))\nreturn t[j].get(i);\nreturn null;\n}\nAgain,thistakestimeproportionaltothelengthofthelistt[hash(x)].\nTheperformanceofahashtabledependscriticallyonthechoiceofthehashfunc-\ntion. A good hash function will spread the elements evenly among the t.length lists, so\nthat the expected size of the list t[hash(x)] is O(n/t.length)=O(1). On the other hand, a\nbad hash function will hash all values (including x) to the same table location, in which\ncase the size of the list t[hash(x)] will be n. In the next section we describe a good hash\nfunction.\n5.1.1 MultiplicativeHashing\nMultiplicativehashingisanefficientmethodofgeneratinghashvaluesbasedonmodular\narithmetic (discussed in Section 2.3) and integer division. It uses the div operator, which\ncalculates the integral part of a quotient, while discarding the remainder. Formally, for\nanyintegersa 0andb 1,adivb= a/b .\n\u2265 \u2265 (cid:98) (cid:99)\nIn multiplicative hashing, we use a hash table of size 2d for some integer d (called\nthedimension). Theformulaforhashinganintegerx 0,...,2w 1 is\n\u2208{ \u2212 }\nhash(x)=((z x) mod 2w)div2w d .\n\u2212\n\u00b7\nHere, z is a randomly chosen odd integer in 1,...,2w 1 . This hash function can be real-\n{ \u2212 }\nizedveryefficientlybyobservingthat,bydefault,operationsonintegersarealreadydone\nmodulo 2w where w is the number of bits in an integer. (See Figure 5.2.) Furthermore,\n95\n5.HashTables 5.1.ChainedHashTable: HashingwithChaining\n2w (4294967296) 100000000000000000000000000000000\nz(4102541685) 11110100100001111101000101110101\nx(42) 00000000000000000000000000101010\nz x 10100000011110010010000101110100110010\n\u00b7\n(z x) mod 2w 00011110010010000101110100110010\n\u00b7\n((z x) mod 2w)div2w d 00011110\n\u2212\n\u00b7\nFigure5.2: Theoperationofthemultiplicativehashfunctionwithw=32andd=8.\nintegerdivisionby2w d isequivalenttodroppingtherightmostw dbitsinabinaryrep-\n\u2212\n\u2212\nresentation(whichisimplementedbyshiftingthebitsrightbyw d). Inthisway,thecode\n\u2212\nthatimplementstheaboveformulaissimplerthantheformulaitself:\nChainedHashTable\nint hash(T x) {\nreturn ((unsigned)(z * hashCode(x))) >> (w-d);\n}\nThefollowinglemma,whoseproofisdeferreduntillaterinthissection,showsthat\nmultiplicativehashingdoesagoodjobofavoidingcollisions:\nLemma 5.1. Let x and y be any two values in 0,...,2w 1 with x (cid:44) y. Then Pr hash(x) =\n{ \u2212 } {\nhash(y) 2/2d.\n}\u2264\nWithLemma5.1,theperformanceofremove(x),andfind(x)areeasytoanalyze:\nLemma 5.2. For any data value x, the expected length of the list t[hash(x)] is at most n +2,\nx\nwheren isthenumberofoccurrencesofxinthehashtable.\nx\nProof. LetS bethe(multi-)setofelementsstoredinthehashtablethatarenotequaltox.\nForanelementy S,definetheindicatorvariable\n\u2208\n1 ifhash(x)=hash(y)\nI =\ny\n\uf8f1 0 otherwise\n\uf8f4\uf8f4\uf8f2\nand notice that, by Lemma 5.1, E[\n\uf8f4\uf8f4\uf8f3\nI ] 2/2d =2/t.length. The expected length of the list\ny\n\u2264\n96\n5.HashTables 5.1.ChainedHashTable: HashingwithChaining\nt[hash(x)]isgivenby\nE[t[hash(x)].size()] = E n + I\nx y\n\uf8ee \uf8f9\ny S\n= n\nx\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n+\n(cid:88)\nE\n\u2208\n[I\ny\n]\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ny S\n(cid:88)\u2208\nn + 2/t.length\nx\n\u2264\ny S\n(cid:88)\u2208\nn + 2/n\nx\n\u2264\ny S\n(cid:88)\u2208\nn +(n n )2/n\nx x\n\u2264 \u2212\nn +2 ,\nx\n\u2264\nasrequired.\nNow,wewanttoproveLemma5.1,butfirstweneedaresultfromnumbertheory.\nInthefollowingproof,weusethenotation(b ,...,b ) todenote r b 2i,whereeachb is\nr 0 2 i=0 i i\na bit, either 0 or 1. In other words, (b ,...,b ) is the integer whose binary representation\nr 0 2 (cid:80)\nisgivenbyb ,...,b . Weuse(cid:63) todenoteabitofunknownvalue.\nr 0\nLemma5.3. LetS bethesetofoddintegersin 1,...,2w 1 ,Letqandi beanytwoelementsin\n{ \u2212 }\nS. Thenthereisexactlyonevaluez S suchthatzq mod 2w=i.\n\u2208\nProof. Sincethenumberofchoicesforzandiisthesame,itissufficienttoprovethatthere\nisatmostonevaluez S thatsatisfieszq mod 2w=i.\n\u2208\nSuppose,forthesakeofcontradiction,thattherearetwosuchvalueszandz ,with\n(cid:48)\nz>z . Then\n(cid:48)\nzq mod 2w=z q mod 2w=i\n(cid:48)\nSo\n(z z )q mod 2w=0\n(cid:48)\n\u2212\nButthismeansthat\n(z z )q=k2w (5.1)\n(cid:48)\n\u2212\nforsomeintegerk. Thinkingintermsofbinarynumbers,wehave\n(z z )q=k (1,0,...,0) ,\n(cid:48) 2\n\u2212 \u00b7\nw\nsothatthewtrailingbitsinthebinaryrepresent(cid:124)at(cid:123)io(cid:122)n(cid:125)of(z z )qareall0\u2019s.\n(cid:48)\n(cid:32) (cid:32) \u2212\n97\n5.HashTables 5.1.ChainedHashTable: HashingwithChaining\nFurthermorek (cid:44)0sinceq (cid:44)0andz z (cid:44)0. Sinceq isodd,ithasnotrailing0\u2019sin\n(cid:48)\n\u2212\nitsbinaryrepresentation:\nq=((cid:63),...,(cid:63),1) .\n2\nSince z z <2w,z z hasfewerthanwtrailing0\u2019sinitsbinaryrepresentation:\n(cid:48) (cid:48)\n| \u2212 | \u2212\nz z =((cid:63),...,(cid:63),1,0,...,0) .\n(cid:48) 2\n\u2212\n<w\nTherefore,theproduct(z z )q hasfewerthanwt(cid:124)ra(cid:123)i(cid:122)lin(cid:125)g0\u2019sinitsbinaryrepresentation:\n(cid:48)\n\u2212 (cid:32) (cid:32)\n(z z )q=((cid:63), ,(cid:63),1,0,...,0) .\n(cid:48) 2\n\u2212 \u00b7\u00b7\u00b7\n<w\nTherefore(z z )q cannotsatisfy(5.1),yieldingac(cid:124)on(cid:123)t(cid:122)ra(cid:125)dictionandcompletingtheproof.\n(cid:48)\n\u2212 (cid:32) (cid:32)\nThe utility of Lemma 5.3 comes from the following observation: If z is chosen\nuniformly at random from S, then zt is uniformly distributed over S. In the following\nproof, it helps to think of the binary representation of z, which consists of w 1 random\n\u2212\nbitsfollowedbya1.\nProofofLemma5.1. Firstwenotethattheconditionhash(x)=hash(y)isequivalenttothe\nstatement\u201cthehighest-orderdbitsofzx mod 2w andthehighest-orderdbitsofzy mod 2w\nare the same.\u201d A necessary condition of that statement is that the highest-order d bits in\nthebinaryrepresentationofz(x y) mod 2w areeitherall0\u2019sorall1\u2019s. Thatis,\n\u2212\nz(x y) mod 2w=(0,...,0,(cid:63),...,(cid:63)) (5.2)\n2\n\u2212\nd w d\n\u2212\nwhenzx mod 2w>zy mod 2w or (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)\n(cid:32) (cid:32) (cid:32) (cid:32)\nz(x y) mod 2w=(1,...,1,(cid:63),...,(cid:63)) . (5.3)\n2\n\u2212\nd w d\n\u2212\nwhenzx mod 2w<zy mod 2w. Therefore,weo(cid:124)n(cid:123)ly(cid:122)h(cid:125)av(cid:124)e(cid:123)to(cid:122)b(cid:125)oundtheprobabilitythatz(x\n(cid:32) (cid:32) (cid:32) (cid:32) \u2212\ny) mod 2w lookslike(5.2)or(5.3).\nLetqbetheuniqueoddintegersuchthat(x y) mod 2w=q2r forsomeintegerr 0.\n\u2212 \u2265\nByLemma5.3,thebinaryrepresentationofzq mod 2w hasw 1randombits,followedby\n\u2212\na1:\nzq mod 2w=(b ,...,b ,1)\nw 1 1 2\n\u2212\nw 1\n\u2212\n(cid:124) (cid:123)(cid:122) (cid:125)\n(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)\n98\n5.HashTables 5.2.LinearHashTable: LinearProbing\nTherefore,thebinaryrepresentationofz(x y) mod 2w=zq2r mod 2w hasw r 1random\n\u2212 \u2212 \u2212\nbits,followedbya1,followedbyr 0\u2019s:\nz(x y) mod 2w=zq2r mod 2w=(b ,...,b ,1,0,0,...,0)\nw r 1 1 2\n\u2212 \u2212 \u2212\nw r 1 r\n\u2212 \u2212\nWe can now finish the proof: If r > w d, then th (cid:124) e d(cid:123) hi (cid:122) gher (cid:125) ord (cid:124) er (cid:123) b (cid:122) its (cid:125) of z(x y) mod 2w\n\u2212 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32) \u2212\ncontain both 0\u2019s and 1\u2019s, so the probability that z(x y) mod 2w looks like (5.2) or (5.3) is\n\u2212\n0. Ifr=w d,thentheprobabilityoflookinglike(5.2)is0,buttheprobabilityoflooking\n\u2212\nlike (5.3) is 1/2d 1 = 2/2d (since we must have b ,...,b = 1,...,1). If r < w d then we\n\u2212 1 d 1\n\u2212 \u2212\nmust have b ,...,b =0,...,0 or b ,...,b =1,...,1. The probability of each\nw r 1 w r d w r 1 w r d\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nofthesecasesis1/2d andtheyaremutuallyexclusive,sotheprobabilityofeitherofthese\ncasesis2/2d. Thiscompletestheproof.\n5.1.2 Summary\nThefollowingtheoremsummarizestheperformanceoftheChainedHashTabledatastruc-\nture:\nTheorem5.1. AChainedHashTableimplementstheUSetinterface. Ignoringthecostofcalls\nto grow(), a ChainedHashTable supports the operations add(x), remove(x), and find(x) in\nO(1)expectedtimeperoperation.\nFurthermore, beginning with an empty ChainedHashTable, any sequence of m add(x)\nandremove(x)operationsresultsinatotalofO(m)timespentduringallcallstogrow().\n5.2 LinearHashTable: Linear Probing\nThe ChainedHashTable data structure uses an array of lists, where the ith list stores all\nelements x such that hash(x) = i. An alternative, called open addressing is to store the\nelements directly in an array, t, with each array location in t storing at most one value.\nThis is the approach taken by the LinearHashTable described in this section. In some\nplaces,thisdatastructureisdescribedasopenaddressingwithlinearprobing.\nThe main idea behind a LinearHashTable is that we would, ideally, like to store\nthe element x with hash value i=hash(x) in the table location t[i]. If we can\u2019t do this\n(because some element is already stored there) then we try to store it at location t[(i+\n1) mod t.length]; if that\u2019s not possible, then we try t[(i+2) mod t.length], and so on,\nuntilwefindaplaceforx.\nTherearethreetypesofentriesstoredint:\n99\n5.HashTables 5.2.LinearHashTable: LinearProbing\n1. datavalues: actualvaluesintheUSetthatwearerepresenting;\n2. nullvalues: atarraylocationswherenodatahaseverbeenstored;and\n3. del values: at array locations where data was once stored but that has since been\ndeleted.\nInadditiontothecounter,n,thatkeepstrackofthenumberofelementsintheLinearHashTable,\na counter, q, keeps track of the number of elements of Types 1 and 3. That is, q is equal\nto n plus the number of del values in t. To make this work efficiently, we need t to be\nconsiderably larger than q, so that there are lots of null values in t. The operations on a\nLinearHashTablethereforemaintaintheinvariantthatt.length 2q.\n\u2265\nTo summarize, a LinearHashTable contains an array, t, that stores data elements,\nandintegersnandqthatkeeptrackofthenumberofdataelementsandnon-nullvalues\noft,respectively. Becausemanyhashfunctionsonlyworkfortablesizesthatareapower\nof2,wealsokeepanintegerdandmaintaintheinvariantthatt.length=2d.\nLinearHashTable\narray<T> t;\nint n; // number of values in T\nint q; // number of non-null entries in T\nint d; // t.length = 2\u02c6d\nThefind(x)operationinaLinearHashTableissimple. Westartatarrayentryt[i]\nwherei=hash(x)andsearchentriest[i],t[(i+1) mod t.length],t[(i+2) mod t.length],\nandsoon,untilwefindanindexi suchthat,either,t[i ]=x,ort[i ]=null. Intheformer\n(cid:48) (cid:48) (cid:48)\ncasewereturnt[i ]. Inthelattercase,weconcludethatxisnotcontainedinthehashtable\n(cid:48)\nandreturnnull.\nLinearHashTable\nT find(T x) {\nint i = hash(x);\nwhile (t[i] != null) {\nif (t[i] != del && t[i] == x) return t[i];\ni = (i == t.length-1) ? 0 : i + 1; // increment i (mod t.length)\n}\nreturn null;\n}\nThe add(x) operation is also fairly easy to implement. After checking that x is\nnot already stored in the table (using find(x)), we search t[i], t[(i+1) mod t.length],\n100\n5.HashTables 5.2.LinearHashTable: LinearProbing\nt[(i+2) mod t.length],andsoon,untilwefindanullordelandstorexatthatlocation,\nincrementn,andq,ifappropriate.:\nLinearHashTable\nbool add(T x) {\nif (find(x) != null) return false;\nif (2*(q+1) > t.length) resize(); // max 50% occupancy\nint i = hash(x);\nwhile (t[i] != null && t[i] != del)\ni = (i == t.length-1) ? 0 : i + 1; // increment i (mod t.length)\nif (t[i] == null) q++;\nn++;\nt[i] = x;\nreturn true;\n}\nBy now, the implementation of the remove(x) operation should be obvious. We\nsearch t[i], t[(i+1) mod t.length], t[(i+2) mod t.length], and so on until we find an\nindexi suchthatt[i ]=xort[i ]=null. Intheformercase,wesett[i ]=delandreturn\n(cid:48) (cid:48) (cid:48) (cid:48)\ntrue. Inthelattercaseweconcludethatxwasnotstoredinthetable(andthereforecannot\nbedeleted)andreturnfalse.\nLinearHashTable\nT remove(T x) {\nint i = hash(x);\nwhile (t[i] != null) {\nT y = t[i];\nif (y != del && x == y) {\nt[i] = del;\nn--;\nif (8*n < t.length) resize(); // min 12.5% occupancy\nreturn y;\n}\ni = (i == t.length-1) ? 0 : i + 1; // increment i (mod t.length)\n}\nreturn null;\n}\nThe correctness of the find(x), add(x), and remove(x) methods is easy to verify,\nthough it relies on the use of del values. Notice that none of these operations ever sets\na non-null entry to null. Therefore, when we reach an index i such that t[i ]=null,\n(cid:48) (cid:48)\nthisisaproofthattheelement,x,thatwearesearchingforisnotstoredinthetable;t[i ]\n(cid:48)\nhas always been null, so there is no reason that a previous add(x) operation would have\nproceededbeyondindexi .\n(cid:48)\n101\n5.HashTables 5.2.LinearHashTable: LinearProbing\nThe resize() method is called by add(x) when the number of non-null entries\nexceedsn/2orbyremove(x)whenthenumberofdataentriesislessthant.length/8. The\nresize() method works like the resize() methods in other array-based data structures.\nWe find the smallest non-negative integer d such that 2d 3n. We reallocate the array t\n\u2265\nso that it has size 2d and then we insert all the elements in the old version of t into the\nnewly-resized copy of t. While doing this we reset q equal to n since the newly-allocated\nthasnodelvalues.\nLinearHashTable\nvoid resize() {\nd = 1;\nwhile ((1<<d) < 3*n) d++;\narray<T> tnew(1<<d, null);\nq = n;\n// insert everything in told\nfor (int k = 0; k < t.length; k++) {\nif (t[k] != null && t[k] != del) {\nint i = hash(t[k]);\nwhile (tnew[i] != null)\ni = (i == tnew.length-1) ? 0 : i + 1;\ntnew[i] = t[k];\n}\n}\nt = tnew;\n}\n5.2.1 AnalysisofLinearProbing\nNotice that each operation, add(x), remove(x), or find(x), finishes as soon as (or before) it\ndiscovers the first null entry in t. The intuition behind the analysis of linear probing is\nthat, since at least half the elements in t are equal to null, an operation should not take\nlong to complete because it will very quickly come across a null entry. We shouldn\u2019t rely\ntooheavilyonthisintuitionthough,becauseitwouldleadusto(theincorrect)conclusion\nthattheexpectednumberoflocationsintexaminedbyanoperationisatmost2.\nFor the rest of this section, we will assume that all hash values are independently\nanduniformlydistributedin 0,...,t.length 1 . Thisisnotarealisticassumption,butit\n{ \u2212 }\nwillmakeitpossibleforustoanalyzelinearprobing. Laterinthissectionwewilldescribe\namethod,calledtabulationhashing,thatproducesahashfunctionthatis\u201cgoodenough\u201d\nfor linear probing. We will also assume that all indices into the positions of t are taken\nmodulot.length,sothatt[i]isreallyashorthandfort[i mod t.length].\nWesaythatarunoflengthk thatstartsatioccurswhent[i],t[i+1],...,t[i+k 1]\n\u2212\n102\n5.HashTables 5.2.LinearHashTable: LinearProbing\nare all non-null and t[i 1] = t[i+k] = null. The number of non-null elements of t\n\u2212\nis exactly q and the add(x) method ensures that, at all times, q t.length/2. There are q\n\u2264\nelementsx ,...,x thathavebeeninsertedintotsincethelastrebuild()operation. Byour\n1 q\nassumption, each of these has a hash value, hash(x ), that is uniform and independent of\nj\ntherest. Withthissetup,wecanprovethemainlemmarequiredtoanalyzelinearprobing.\nLemma 5.4. Fix a value i 0,...,t.length 1 . Then the probability that a run of length k\n\u2208{ \u2212 }\nstartsatiisO(ck)forsomeconstant0<c<1.\nProof. If a run of length k starts at i, then there are exactly k elements x such that\nj\nhash(x ) i,...,i+k 1 . Theprobabilitythatthisoccursisexactly\nj\n\u2208{ \u2212 }\nk q k\nq k t.length k \u2212\np = \u2212 ,\nk k t.length t.length\n(cid:32) (cid:33)(cid:32) (cid:33) (cid:32) (cid:33)\nsince, for each choice of k elements, these k elements must hash to one of the k locations\nandtheremainingq k elementsmusthashtotheothert.length k tablelocations.1\n\u2212 \u2212\nInthefollowingderivationwewillcheatalittleandreplacer!with(r/e)r. Stirling\u2019s\nApproximation (Section 1.2.2) shows that this is only a factor of O(\u221ar) from the truth.\nThis is just done to make the derivation simpler; Exercise 5.4 asks the reader to redo the\ncalculationmorerigorouslyusingStirling\u2019sApproximationinitsentirety.\nThe value of p is maximized when t.length is minimum, and the data structure\nk\nmaintainstheinvariantthatt.length 2q,so\n\u2265\nk q k\nq k 2q k \u2212\np \u2212\nk \u2264 k 2q 2q\n(cid:32) (cid:33)(cid:32) (cid:33) (cid:32) (cid:33)\nk q k\nq! k 2q k \u2212\n= \u2212\n(q k)!k! 2q 2q\n(cid:32) \u2212 (cid:33)(cid:32) (cid:33) (cid:32) (cid:33)\nqq k k 2q k q \u2212 k\n\u2212 [Stirling\u2019sapproximation]\n\u2248 (q k)q kkk 2q 2q\n(cid:32) \u2212 \u2212 (cid:33)(cid:32) (cid:33) (cid:32) (cid:33)\nqkqq \u2212 k k k 2q k q \u2212 k\n= \u2212\n(q k)q kkk 2q 2q\n(cid:32) \u2212 \u2212 (cid:33)(cid:32) (cid:33) (cid:32) (cid:33)\nk q k\nqk q(2q k) \u2212\n= \u2212\n2qk 2q(q k)\n(cid:32) (cid:33) (cid:32) \u2212 (cid:33)\n1 k (2q k) q \u2212 k\n= \u2212\n2 2(q k)\n(cid:18) (cid:19) (cid:32) \u2212 (cid:33)\n1Notethatpk isgreaterthantheprobabilitythatarunoflengthkstartsati,sincethedefinitionofpk does\nnotincludetherequirementt[i 1]=t[i+k]=null.\n\u2212\n103\n5.HashTables 5.2.LinearHashTable: LinearProbing\n1 k k q \u2212 k\n= 1+\n2 2(q k)\n(cid:18) (cid:19) (cid:32) \u2212 (cid:33)\nk\n\u221ae\n.\n\u2264 2\n(cid:32) (cid:33)\n(In the last step, we use the inequality (1+1/x)x e, which holds for all x > 0.) Since\n\u2264\n\u221ae/2<0.824360636<1,thiscompletestheproof.\nUsingLemma5.4toproveupper-boundsontheexpectedrunningtimeoffind(x),\nadd(x),andremove(x)isnowfairlystraight-forward. Considerthesimplestcase,wherewe\nexecute find(x) for some value x that has never been stored in the LinearHashTable. In\nthiscase,i=hash(x)isarandomvaluein 0,...,t.length 1 independentofthecontents\n{ \u2212 }\noft. Ifiispartofarunoflengthk thenthetimeittakestoexecutethefind(x)operation\nisatmostO(1+k). Thus,theexpectedrunningtimecanbeupper-boundedby\nt.length\n1 \u221e\nO 1+ kPr iispartofarunoflengthk .\nt.length { }\n\uf8eb \uf8f6\nNotethateach\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nrun\n(cid:18)\noflengthk\n(cid:19)\nco\n(cid:88)\nn\ni=\nt\n1\nribu\n(cid:88) k=\nte\n0\nstotheinnersumk timesforatot\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nalcontribution\nofk2,sotheabovesumcanberewrittenas\nt.length\n1 \u221e\nO 1+ k2Pr istartsarunoflengthk\nt.length { }\n\uf8eb \uf8f6\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nO 1\n(cid:18)\n+\n1\n(cid:19) (cid:88) i\nt\n=\n.\n1\nleng\n(cid:88) k\nt\n=\nh\n0\n\u221e\nk2p\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2264 t.length k\n\uf8eb \uf8f6\n=O\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed1+ (cid:18)\n\u221e\nk2p\n(cid:19) (cid:88) i=1 (cid:88) k=0 \uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nk\n\uf8eb \uf8f6\nk=0\n=O\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed1+ (cid:88)\n\u221e\nk2 O\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8(ck)\n\u00b7\n\uf8eb \uf8f6\nk=0\n=O(\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1) .\n(cid:88) \uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nThelaststepinthisderivationcomesfromthefactthat k2 O(ck)isanexponentially\n\u221ek=0\n\u00b7\ndecreasing series.2 Therefore, we conclude that the expected running time of the find(x)\n(cid:80)\noperationforavaluexthatisnotcontainedinaLinearHashTableisO(1).\nIf we ignore the cost of the resize() operation, the above analysis gives us all we\nneedtoanalyzethecostofoperationsonaLinearHashTable.\n2Intheterminologyofmanycalculustexts,thissumpassestheratiotest:Thereexistsapositiveintegerk0\n(k+1)2ck+1\nsuchthat,forallk\n\u2265\nk0,\nk2ck\n<1.\n104\n5.HashTables 5.2.LinearHashTable: LinearProbing\nFirstofall,theanalysisoffind(x)givenaboveappliestotheadd(x)operationwhen\nx is not contained in the table. To analyze the find(x) operation when x is contained in\nthe table, we need only note that this is the same as the cost of the add(x) operation that\npreviously added x to the table. Finally, the cost of a remove(x) operation is the same as\nthecostofafind(x)operation.\nInsummary,ifweignorethecostofcallstoresize(),alloperationsonaLinearHashTable\nrun in O(1) expected time. Accounting for the cost of resize can be done using the same\ntypeofamortizedanalysisperformedfortheArrayStackdatastructureinSection2.1.\n5.2.2 Summary\nThe following theorem summarizes the performance of the LinearHashTable data struc-\nture:\nTheorem 5.2. A LinearHashTable implements the USet interface. Ignoring the cost of calls\nto resize(), a LinearHashTable supports the operations add(x), remove(x), and find(x) in\nO(1)expectedtimeperoperation.\nFurthermore, beginning with an empty LinearHashTable, any sequence of m add(x)\nandremove(x)operationsresultsinatotalofO(m)timespentduringallcallstoresize().\n5.2.3 TabulationHashing\nWhileanalyzingtheLinearHashTablestructure,wemadeaverystrongassumption: That\nforanysetofelements, x ,...,x ,thehashvalueshash(x ),...,hash(x )areindependently\n1 n 1 n\n{ }\nand uniformly distributed over 0,...,t.length 1 . One way to imagine getting this is to\n{ \u2212 }\nhaveagiantarray,tab,oflength2w,whereeachentryisarandomw-bitinteger,indepen-\ndentofalltheotherentries. Inthisway,wecouldimplementhash(x)byextractingad-bit\nintegerfromtab[x.hashCode()]:\nLinearHashTable\nint idealHash(T x) {\nreturn tab[hashCode(x) >> w-d];\n}\nUnfortunately,storinganarrayofsize2w isprohibitiveintermsofmemoryusage.\nThe approach used by tabulation hashing is to, instead, treat w-bit integers as being com-\nprisedofw/rintegers, eachhavingonlyrbits. Inthisway, tabulationhashingonlyneeds\nw/r arrays each of length 2r. All the entries in these arrays are independent w-bit inte-\ngers. To obtain the value of hash(x) we split x.hashCode() up into w/r r-bit integers and\n105\n5.HashTables 5.3.HashCodes\nuse these as indices into these arrays. We then combine all these values with the bitwise\nexclusive-or operator to obtain hash(x). The following code shows how this works when\nw=32andr=4:\nLinearHashTable\nint hash(T x) {\nunsigned h = hashCode(x);\nreturn (tab[0][h&0xff]\n\u02c6 tab[1][(h>>8)&0xff]\n\u02c6 tab[2][(h>>16)&0xff]\n\u02c6 tab[3][(h>>24)&0xff])\n>> (w-d);\n}\nInthiscase,tabisa2-dimensionalarraywith4columnsand232/4=256rows.\nOnecaneasilyverifythat,foranyx,hash(x)isuniformlydistributedover 0,...,2d\n{ \u2212\n1 . With a little work, one can even verify that any pair of values have independent hash\n}\nvalues. This implies tabulation hashing could be used in place of multiplicative hashing\nfortheChainedHashTableimplementation.\nHowever,itisnottruethatanysetofndistinctvaluesgivesasetofnindependent\nhash values. Nevertheless, when tabulation hashing is used, the bound of Theorem 5.2\nstillholds. Referencesforthisareprovidedattheendofthischapter.\n5.3 Hash Codes\nThe hash tables discussed in the previous section are used to associate data with integer\nkeys consisting of w bits. In many cases, we have keys that are not integers. They may be\nstrings, objects, arrays, or other compound structures. To use hash tables for these types\nof data, we must map these data types to w-bit hash codes. Hash code mappings should\nhavethefollowingproperties:\n1. Ifxandyareequal,thenx.hashCode()andy.hashCode()areequal.\n2. Ifxandyarenotequal,thentheprobabilitythatx.hashCode()=y.hashCode()should\nbesmall(closeto1/2w).\nThe first property ensures that if we store x in a hash table and later look up a\nvalueyequaltox,thenwewillfindx\u2014asweshould. Thesecondpropertyminimizesthe\nloss from converting our objects to integers. It ensures that unequal objects usually have\ndifferenthashcodesandsoarelikelytobestoredatdifferentlocationsinourhashtable.\n106\n5.HashTables 5.3.HashCodes\n5.3.1 HashCodesforPrimitiveDataTypes\nSmall primitive data types like char, byte, int, and float are usually easy to find hash\ncodesfor. Thesedatatypesalwayshaveabinaryrepresentationandthisbinaryrepresen-\ntation usually consists of w or fewer bits. (For example, in C++ char is typically an 8-bit\ntype and float is a 32-bit type.) In these cases, we just treat these bits as the representa-\ntion of an integer in the range 0,...,2w 1 . If two values are different, they get different\n{ \u2212 }\nhashcodes. Iftheyarethesame,theygetthesamehashcode.\nA few primitive data types are made up of more than w bits, usually cw bits for\nsome constant integer c. (Java\u2019s long and double types are examples of this with c = 2.)\nThese data types can be treated as compound objects made of c parts, as described in the\nnextsection.\n5.3.2 HashCodesforCompoundObjects\nFor a compound object, we want to create a hash code by combining the individual hash\ncodes of the object\u2019s constituent parts. This is not as easy as it sounds. Although one can\nfindmanyhacksforthis(forexample,combiningthehashcodeswithbitwiseexclusive-or\noperations), many of these hacks turn out to be easy to foil (see Exercises 5.7\u20135.9). How-\never,ifoneiswillingtodoarithmeticwith2wbitsofprecision,thentherearesimpleand\nrobustmethodsavailable. SupposewehaveanobjectmadeupofseveralpartsP ,...,P\n0 r 1\n\u2212\nwhosehashcodesarex ,...,x . Thenwecanchoosemutuallyindependentrandomw-bit\n0 r 1\n\u2212\nintegers z ,...,z and a random 2w-bit odd integer z and compute a hash code for our\n0 r 1\n\u2212\nobjectwith\nr 1\nh(x ,...,x )= z \u2212 z x mod 22w div2w .\n0 r 1 i i\n\u2212 \uf8eb\uf8eb \uf8f6 \uf8f6\ni=0\nNote that this hash code has a final st\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ne\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\np\n(cid:88)\n(multip\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nlying by z\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand dividing by 2w) that uses\nthe multiplicative hash function from Section 5.1.1 to take the 2w-bit intermediate result\nandreduceittoaw-bitfinalresult. Hereisanexampleofthismethodappliedtoasimple\ncompoundobjectwith3partsx0,x1,andx2:\nPoint3D\nunsigned hashCode() {\nlong long z[] = {0x2058cc50L, 0xcb19137eL, 0x2cb6b6fdL}; // random\nlong zz = 0xbea0107e5067d19dL; // random\nlong h0 = ods::hashCode(x0);\nlong h1 = ods::hashCode(x1);\nlong h2 = ods::hashCode(x2);\nreturn (int)(((z[0]*h0 + z[1]*h1 + z[2]*h2)*zz) >> 32);\n107\n5.HashTables 5.3.HashCodes\n}\nThefollowingtheoremshowsthat,inadditiontobeingstraightforwardtoimplement,this\nmethodisprovablygood:\nTheorem5.3. Letx ,...,x andy ,...,y eachbesequencesofwbitintegersin 0,...,2w 1\n0 r 1 0 r 1\n\u2212 \u2212 { \u2212 }\nandassumex (cid:44)y foratleastoneindexi 0,...,r 1 . Then\ni i\n\u2208{ \u2212 }\nPr h(x ,...,x )=h(y ,...,y ) 3/2w .\n0 r 1 0 r 1\n{ \u2212 \u2212 }\u2264\nProof. Wewillfirstignorethefinalmultiplicativehashingstepandseehowthatstepcon-\ntributeslater. Define:\nr 1\nh (x ,...,x )= \u2212 z x mod 22w .\n(cid:48) 0 r 1 j j\n\u2212 \uf8eb \uf8f6\nj=0\nSupposethath\n(cid:48)\n(x\n0\n,...,x\nr\n\u2212\n1\n)=h\n(cid:48)\n(y\n0\n,...,y\nr\n\u2212\n1\n)\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n.\n(cid:88)\nWecan\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nrewritethisas:\nz (x y ) mod 22w=t (5.4)\ni i i\n\u2212\nwhere\ni 1 r 1\nt= \u2212 z (y x )+ \u2212 z (y x ) mod 22w\nj j j j j j\n\uf8eb \u2212 \u2212 \uf8f6\nj=0 j=i+1\nIfweassume,withoutloss\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\no\n(cid:88)\nfgeneralitytha\n(cid:88)\ntx\ni\n>y\ni\n,then\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n(5.4)becomes\nz (x y )=t , (5.5)\ni i i\n\u2212\nsinceeachofz and(x y )isatmost2w 1,sotheirproductisatmost22w 2w+1+1<22w 1.\ni i i\n\u2212 \u2212 \u2212 \u2212\nBy assumption, x y (cid:44)0, so (5.5) has at most one solution in z . Therefore, since z and\ni i i i\n\u2212\nt areindependent(z ,...,z aremutuallyindependent),theprobabilitythatweselectz\n0 r 1 i\n\u2212\nsothath (x ,...,x )=h (y ,...,y )isatmost1/2w.\n(cid:48) 0 r 1 (cid:48) 0 r 1\n\u2212 \u2212\nThe final step of the hash function is to apply multiplicative hashing to reduce\nour2w-bitintermediateresulth (x ,...,x )toaw-bitfinalresulth(x ,...,x ). ByTheo-\n(cid:48) 0 r 1 0 r 1\n\u2212 \u2212\nrem5.3,ifh (x ,...,x )(cid:44) h (y ,...,y ),thenPr h(x ,...,x )=h(y ,...,y ) 2/2w.\n(cid:48) 0 r 1 (cid:48) 0 r 1 0 r 1 0 r 1\n\u2212 \u2212 { \u2212 \u2212 }\u2264\nTosummarize,\nh(x ,...,x )\n0 r 1\nPr \u2212\n\uf8f1 =h(y 0 ,...,y r 1 ) \uf8fc\n\uf8f4\uf8f4\uf8f2 \u2212 \uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f3 h\n(cid:48)\n(x\n0\n,...,x\nr 1\n)=\uf8f4\uf8f4\uf8fe h\n(cid:48)\n(y\n0\n,...,y\nr 1\n)or\n\u2212 \u2212\n=Pr\uf8f1 h (cid:48) (x 0 ,...,x r 1 )(cid:44) h (cid:48) (y 0 ,...,y r 1 ) \uf8fc\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\nandzh\n(cid:48)\n(x\n\u2212\n0\n,...,x\nr 1\n)div2w\n\u2212\n=zh\n(cid:48)\n(y\n0\n,...,y\nr 1\n)div2w\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\u2212 \u2212\n\u2264\n1/2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nw+2/2w=3/2w .\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe\n108\n5.HashTables 5.3.HashCodes\n5.3.3 HashCodesforArraysandStrings\nThe method from the previous section works well for objects that have a fixed, constant,\nnumber of components. However, it breaks down when we want to use it with objects\nthathaveavariablenumberof componentssinceitrequiresarandomw-bitintegerz for\ni\neach component. We could use a pseudorandom sequence to generate as many z \u2019s as we\ni\nneed,butthenthez \u2019sarenotmutuallyindependent,anditbecomesdifficulttoprovethat\ni\nthe pseudorandom numbers don\u2019t interact badly with the hash function we are using. In\nparticular,thevaluesoft andz intheproofofTheorem5.3arenolongerindependent.\ni\nA more rigorous approach is to base our hash codes on polynomials over prime\nfields; these are just regular polynomials that are evaluated modulo some prime number,\np. Thismethodisbasedonthefollowingtheorem,whichsaysthatpolynomialsoverprime\nfieldsbehavepretty-muchlikeusualpolynomials:\nTheorem 5.4. Let p be a prime number, and let f(z) = x z0+x z1+ +x zr 1 be a non-\n0 1 r 1 \u2212\n\u00b7\u00b7\u00b7 \u2212\ntrivial polynomialwith coefficientsx 0,...,p 1 . Then theequationf(z) mod p=0has at\ni\n\u2208{ \u2212 }\nmostr 1solutionsforz 0,...,p 1 .\n\u2212 \u2208{ \u2212 }\nTo use Theorem 5.4, we hash a sequence of integers x ,...,x with each x\n0 r 1 i\n\u2212 \u2208\n0,...,p 2 usingarandomintegerz 0,...,p 1 viatheformula\n{ \u2212 } \u2208{ \u2212 }\nh(x ,...,x )= x z0+ +x zr 1+(p 1)zr mod p .\n0 r 1 0 r 1 \u2212\n\u2212 \u00b7\u00b7\u00b7 \u2212 \u2212\n(cid:16) (cid:17)\nNote the extra (p 1)zr term at the end of the formula. It helps to think of (p 1)\n\u2212 \u2212\nasthelastelement,x ,inthesequencex ,...,x . Notethatthiselementdiffersfromevery\nr 0 r\nother element in the sequence (each of which is in the set 0,...,p 2 ). We can think of\n{ \u2212 }\np 1asanend-of-sequencemarker.\n\u2212\nThe following theorem, which considers the case of two sequences of the same\nlength,showsthatthishashfunctiongivesagoodreturnforthesmallamountofrandom-\nizationneededtochoosez:\nTheorem 5.5. Let p > 2w+1 be a prime, let x ,...,x and y ,...,y each be sequences of\n0 r 1 0 r 1\n\u2212 \u2212\nw-bitintegersin 0,...,2w 1 ,andassumex (cid:44)y foratleastoneindexi 0,...,r 1 . Then\ni i\n{ \u2212 } \u2208{ \u2212 }\nPr h(x ,...,x )=h(y ,...,y ) (r 1)/p .\n0 r 1 0 r 1\n{ \u2212 \u2212 }\u2264 \u2212 }\nProof. Theequationh(x ,...,x )=h(y ,...,y )canberewrittenas\n0 r 1 0 r 1\n\u2212 \u2212\n(x y )z0+ +(x y )zr 1 mod p=0. (5.6)\n0 0 r 1 r 1 \u2212\n\u2212 \u00b7\u00b7\u00b7 \u2212 \u2212 \u2212\n(cid:16) (cid:17)\n109\n5.HashTables 5.3.HashCodes\nSincex (cid:44)y ,thispolynomialisnon-trivial. Therefore,byTheorem5.4,ithasatmostr 1\ni i\n\u2212\nsolutions in z. The probability that we pick z to be one of these solutions is therefore at\nmost(r 1)/p.\n\u2212\nNote that this hash function also deals with the case in which two sequences have\ndifferent lengths, even when one of the sequences is a prefix of the other. This is because\nthisfunctioneffectivelyhashestheinfinitesequence\nx ,...,x ,p 1,0,0,... .\n0 r 1\n\u2212 \u2212\nThisguaranteesthatifwehavetwosequencesoflengthr andr withr >r ,thenthesetwo\n(cid:48) (cid:48)\nsequencesdifferatindexi =r. Inthiscase,(5.6)becomes\ni=r(cid:48)\u2212 1 i=r\n\u2212\n1\n(x y )zi+(x p+1)zr(cid:48) + x zi+(p 1)zr mod p=0 ,\ni i r i\n\u2212 (cid:48) \u2212 \u2212\n\uf8eb \uf8f6\ni=0 i=r+1\nwhich, by T\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edhe (cid:88)\norem 5.4, has at most r solutio\n(cid:88)\nn\n(cid:48)\ns in z. This com\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8bined\nwith Theorem 5.5\nsufficetoprovethefollowingmoregeneraltheorem:\nTheorem5.6. Letp>2w+1beaprime,letx ,...,x andy ,...,y bedistinctsequencesof\n0 r 1 0 r 1\n\u2212 (cid:48)\u2212\nw-bitintegersin 0,...,2w 1 . Then\n{ \u2212 }\nPr h(x ,...,x )=h(y ,...,y ) max r,r /p .\n0 r 1 0 r 1 (cid:48)\n{ \u2212 \u2212 }\u2264 { }\nThe following example code shows how this hash function is applied to an object\nthatcontainsanarray,x,ofvalues:\nGeomVector\nunsigned hashCode() {\nlong p = (1L<<32)-5; // prime: 2\u02c632 - 5\nlong z = 0x64b6055aL; // 32 bits from random.org\nint z2 = 0x5067d19d; // random odd 32 bit number\nlong s = 0;\nlong zi = 1;\nfor (int i = 0; i < x.length; i++) {\nlong long xi = (ods::hashCode(x[i]) * z2) >> 1; // reduce to 31 bits\ns = (s + zi * xi) % p;\nzi = (zi * z) % p;\n}\ns = (s + zi * (p-1)) % p;\nreturn (int)s;\n}\n110\n5.HashTables 5.4.DiscussionandExercises\nThe above code sacrifices some collision probability for implementation conve-\nnience. In particular, it applies the multiplicative hash function from Section 5.1.1, with\nd=31 to reduce x[i].hashCode() to a 31-bit value. This is so that the additions and mul-\ntiplicationsthataredonemodulotheprimep=232 5canbecarriedoutusingunsigned\n\u2212\n63-bit arithmetic. This means that the probability of two different sequences, the longer\nofwhichhaslengthr,havingthesamehashcodeisatmost\n2/231+r/(232 5)\n\u2212\nratherthanther/(232 5)specifiedinTheorem5.6.\n\u2212\n5.4 Discussion and Exercises\nHashtablesandhashcodesareanenormousandactiveareaofresearchthatisjusttouched\nuponinthischapter. TheonlineBibliographyonHashing[8]containsnearly2000entries.\nA variety of different hash table implementations exist. The one described in Sec-\ntion 5.1 is known as hashing with chaining (each array entry contains a chain (List) of\nelements). Hashing with chaining dates back to an internal IBM memorandum authored\nby H. P. Luhn and dated January 1953. This memorandum also seems to be one of the\nearliestreferencestolinkedlists.\nAn alternative to hashing with chaining is that used by open addressing schemes,\nwherealldataisstoreddirectlyinanarray. TheseschemesincludetheLinearHashTable\nstructure of Section 5.2. This idea was also proposed, independently, by a group at IBM\ninthe1950s. Openaddressingschemesmustdealwiththeproblemofcollisionresolution:\nthe case where two values hash to the same array location. Different strategies exist for\ncollisionresolutionandtheseprovidedifferentperformanceguaranteesandoftenrequire\nmoresophisticatedhashfunctionsthantheonesdescribedhere.\nYetanothercategoryofhashtableimplementationsaretheso-calledperfecthashing\nmethods. Thesearemethodsinwhichfind(x)operationstakeO(1)timeintheworst-case.\nForstaticdatasets,thiscanbeaccomplishedbyfindingperfecthashfunctionsforthedata;\nthese are functions that map each piece of data to a unique array location. For data that\nchangesovertime,perfecthashingmethodsincludeFKStwo-levelhashtables[26,21]and\ncuckoohashing[50].\nThehashfunctionspresentedinthischapterareprobablyamongthemostpracti-\ncal currently known methods that can be proven to work well for any set of data. Other\nprovably good methods date back to the pioneering work of Carter and Wegman who in-\ntroducedthenotionofuniversalhashinganddescribedseveralhashfunctionsfordifferent\n111\n5.HashTables 5.4.DiscussionandExercises\nscenarios [12]. Tabulation hashing, described in Section 5.2.3, is due to Carter and Weg-\nman [12], but its analysis, when applied to linear probing (and several other hash table\nschemes)isduetoPa\u02c7tras\u00b8cuandThorup[55].\nThe idea of multiplicative hashing is very old and seems to be part of the hashing\nfolklore [43, Section 6.4]. However, the idea of choosing the multiplier z to be a random\nodd number, and the analysis in Section 5.1.1 is due to Dietzfelbinger et al. [20]. This\nversionofmultiplicativehashingisoneofthesimplest,butitscollisionprobabilityof2/2d\nis a factor of 2 larger than what one could expect with a random function from 2w 2d.\n\u2192\nThemultiply-addhashingmethodusesthefunction\nh(x)=((zx+b) mod 22w)div22w d\n\u2212\nwhere z and b are each randomly chosen from 0,...,22w 1 . Multiply-add hashing has a\n{ \u2212 }\ncollisionprobabilityofonly1/2d [18],butrequires2w-bitprecisionarithmetic.\nThere are a number of methods of obtaining hash codes from fixed-length se-\nquencesofw-bitintegers. Oneparticularlyfastmethod[9]isthefunction\nr/2 1\nh(x ,...,x )= \u2212 ((x +a ) mod 2w)((x +a ) mod 2w) mod 22w\n0 r 1 2i 2i 2i+1 2i+1\n\u2212 \uf8eb \uf8f6\ni=0\nwhere r is even and a\n0\n,.\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed.. (cid:88)\n,a\nr 1\nare randomly chosen from 0,...,2w . T\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8his\nyields a 2w-bit\n\u2212 { }\nhash code that has collision probability 1/2w. This can be reduced to a w-bit hash code\nusing multiplicative (or multiply-add) hashing. This method is fast because it requires\nonly r/2 2w-bit multiplications whereas the method described in Section 5.3.2 requires r\nmultiplications. (The mod operations occur implicitly by using w and 2w-bit arithmetic\nfortheadditionsandmultiplications,respectively.)\nThe method from Section 5.3.3 of using polynomials over prime fields to hash\nvariable-length arrays and strings is due to Dietzfelbinger et al. [19]. It is, unfortunately,\nnot very fast. This is due to its use of the mod operator which relies on a costly machine\ninstruction. Somevariantsofthismethodchoosetheprimeptobeoneoftheform2w 1,\n\u2212\nin which case the mod operator can be replaced with addition (+) and bitwise-and (&)\noperations [42, Section 3.6]. Another option is to apply one of the fast methods for fixed-\nlengthstringstoblocksoflengthc forsomeconstantc>1andthenapplytheprimefield\nmethodtotheresultingsequenceof r/c hashcodes.\n(cid:100) (cid:101)\nExercise 5.1. A certain university assigns each of its students student numbers the first\ntime they register for any course. These numbers are sequential integers that started at\n112\n5.HashTables 5.4.DiscussionandExercises\n0 many years ago and are now in the millions. Suppose we have a class of 100 first year\nstudentsandwewanttoassignthemhashcodesbasedontheirstudentnumbers. Doesit\nmakemoresensetousethefirsttwodigitsorthelasttwodigitsoftheirstudentnumber?\nJustifyyouranswer.\nExercise 5.2. Consider the multiplicative hashing scheme in Section 5.1.1, and suppose\nn=2d andd w/2.\n\u2264\n1. Show that, for any choice of the muliplier, z, there exists n values that all have the\nsamehashcode. (Hint: Thisiseasy,anddoesn\u2019trequireanynumbertheory.)\n2. Given the multiplier, z, describe n values that all have the same hash code. (Hint:\nThisisharder,andrequiressomebasicnumbertheory.)\nExercise 5.3. Prove that the bound 2/2d in Lemma 5.1 is the best possible by showing\nthat, if x =2w d 2 and y=3x, then Pr hash(x)=hash(y) =2/2d. (Hint look at the binary\n\u2212 \u2212\n{ }\nrepresentationsofzxandz3xandusethefactthatz3x=zx+2zx.)\nExercise5.4. ReproveLemma5.4usingthefullversionofStirling\u2019sApproximationgiven\ninSection1.2.2.\nExercise5.5. Considerthefollowingthesimplifiedversionofthecodeforaddinganele-\nmentxtoaLinearHashTable, whichsimplystoresxinthefirstnullarrayentryitfinds.\nExplain why this could be very slow by giving an example of a sequence of O(n) add(x),\nremove(x),andfind(x)operationsthatwouldtakeontheorderofn2 timetoexecute.\nLinearHashTable\nbool addSlow(T x) {\nif (2*(q+1) > t.length) resize(); // max 50% occupancy\nint i = hash(x);\nwhile (t[i] != null) {\nif (t[i] != del && x.equals(t[i])) return false;\ni = (i == t.length-1) ? 0 : i + 1; // increment i (mod t.length)\n}\nt[i] = x;\nn++; q++;\nreturn true;\n}\nExercise5.6. EarlyversionsoftheJavahashCode()methodfortheStringclassworkedby\nnot using all characters of long strings. For example, for a 16 character string, the hash\ncodewascomputedusingonlythe8even-indexedcharacters. Explainwhythiswasavery\nbadideabyGivinganexampleoflargesetofstringsthatallhavethesamehashcode.\n113\n5.HashTables 5.4.DiscussionandExercises\nExercise 5.7. Suppose you have an object made up of two w-bit integers, x and y. Show\nwhyx ydoes not make a good hash code for your object. Give an example of a large set\n\u2295\nofobjectsthatwouldallhavehashcode0.\nExercise 5.8. Suppose you have an object made up of two w-bit integers, x and y. Show\nwhy x+y does not make a good hash code for your object. Give an example of a large set\nofobjectsthatwouldallhavethesamehashcode.\nExercise5.9. Supposeyouhaveanobjectmadeupoftwow-bitintegers,xandy. Suppose\nthat the hash code for your object is defined by some deterministic function h(x,y) that\nproduces a single w-bit integer. Prove that there exists a large set of objects that have the\nsamehashcode.\nExercise5.10. Letp=2w 1forsomepositiveintegerw. Explainwhy,forapositiveinteger\n\u2212\nx\n(x mod 2w)+(xdiv2w) x mod (2w 1) .\n\u2261 \u2212\n(Thisgivesanalgorithmforcomputingx mod (2w 1)byrepeatedlysetting\n\u2212\nx=x&((1<<w) 1)+x>>w\n\u2212\nuntilx 2w 1.)\n\u2264 \u2212\nExercise5.11. Findsomecommonly-usedhashtableimplementation(TheC++STLunordered map\northeHashTableorLinearHashTableimplementationsinthisbook,forexample)andde-\nsignaprogramthatstoresintegersinthisdatastructuresothatthereareintegers,x,such\nthat find(x) takes linear time. That is, find a set of n integers for which there are cn ele-\nmentsthathashtothesametablelocation.\nDependingonhowgoodtheimplementationis,youmaybeabletodothisjustby\ninspectingthecodefortheimplementation,oryoumayhavetowritesomecodethatdoes\ntrial insertions and searches, timing how long it takes to add and find particular values.\n(Thiscanbe,andhasbeen,usedtolaunchdenialofserviceattacksonwebservers[14].)\n114\nChapter 6\nBinary Trees\nThis chapter introduces one of the most fundamental structures in computer science: bi-\nnarytrees. Theuseofthewordtreeherecomesfromthefactthat,whenwedrawthem,the\nresulting drawing often resembles the trees we find in a forest. There are lots of ways of\ndefiningbinarytrees. Mathematically,abinarytreeisaconnectedundirectedfinitegraph\nwithnocycles,andnovertexofdegreegreaterthanthree.\nFor most computer science applications, binary trees are rooted: A special node,\nr, of degree at most two is called the root of the tree. For every node, u (cid:44) r, the second\nnode on the path from u to r is called the parent of u. Each of the other nodes adjacent\nto u is called a child of u. Most of the binary trees we are interested in are ordered, so we\ndistinguishbetweentheleftchildandrightchildofu.\nIn illustrations, binary trees are usually drawn from the root downward, with the\nrootatthetopofthedrawingandtheleftandrightchildrenrespectivelygivenbyleftand\nright positions in the drawing (Figure 6.1). A binary tree with nine nodes is drawn this\nwayinFigure6.2.a.\nu.parent\nu\nu.left u.right\nFigure6.1: Theparent,leftchild,andrightchildofthenodeuinaBinaryTree.\n115\n6.BinaryTrees 6.1.BinaryTree: ABasicBinaryTree\nr r\n(a) (b)\nFigure6.2: Abinarytreewith(a)ninerealnodesand(b)tenexternalnodes.\nBinarytreesaresoimportantthataterminologyhasdevelopedaroundthem: The\ndepth of a node, u, in a binary tree is the length of the path from u to the root of the tree.\nIf a node, w, is on the path from u to r then w is called an ancestor of u and u a descendant\nof w. The subtree of a node, u, is the binary tree that is rooted at u and contains all of u\u2019s\ndescendants. The height of a node, u, is the length of the longest path from u to one of its\ndescendants. The height of a tree is the height of its root. A node, u, is a leaf if it has no\nchildren.\nWesometimesthinkofthetreeasbeingaugmentedwithexternalnodes. Anynode\nthatdoesnothavealeftchildhasanexternalnodeasitsleftchildandanynodethatdoes\nnothavearightchildhasanexternalnodeasitsrightchild(seeFigure6.2.b). Itiseasyto\nverify,byinduction,thatabinarytreehavingn 1realnodeshasn+1externalnodes.\n\u2265\n6.1 BinaryTree: A Basic Binary Tree\nThe simplest way to represent a node, u, in a binary tree is to store the (at most three)\nneighboursofuexplicitly:\nBinaryTree\nclass BTNode {\nN *left;\nN *right;\nN *parent;\nBTNode() {\nleft = right = parent = NULL;\n}\n};\n116\n6.BinaryTrees 6.1.BinaryTree: ABasicBinaryTree\nWhen one of these three neighbours is not present, we set it to nil. In this way, external\nnodesinthetreeaswellastheparentoftherootcorrespondtothevaluenil.\nThebinarytreeitselfcanthenberepresentedbyapointertoitsrootnode,r:\nBinaryTree\nNode *r; // root node\nWecancomputethedepthofanode,u,inabinarytreebycountingthenumberof\nstepsonthepathfromutotheroot:\nBinaryTree\nint depth(Node *u) {\nint d = 0;\nwhile (u != r) {\nu = u->parent;\nd++;\n}\nreturn d;\n}\n6.1.1 RecursiveAlgorithms\nItisveryeasytocomputefactsaboutbinarytreesusingrecursivealgorithms. Forexample,\ntocomputethesizeof(numberofnodesin)abinarytreerootedatnodeu,werecursively\ncomputethesizesofthetwosubtreesrootedatthechildrenofu,sumthesesizes,andadd\none:\nBinaryTree\nint size(Node *u) {\nif (u == nil) return 0;\nreturn 1 + size(u->left) + size(u->right);\n}\nTo compute the height of a node u we can compute the height of u\u2019s two subtrees,\ntakethemaximum,andaddone:\nBinaryTree\nint height(Node *u) {\nif (u == nil) return -1;\nreturn 1 + max(height(u->left), height(u->right));\n}\n6.1.2 TraversingBinaryTrees\nThe two algorithms from the previous section use recursion to visit all the nodes in a\nbinary tree. Each of them visits the nodes of the binary tree in the same order as the\n117\n6.BinaryTrees 6.1.BinaryTree: ABasicBinaryTree\nfollowingcode:\nBinaryTree\nvoid traverse(Node *u) {\nif (u == nil) return;\ntraverse(u->left);\ntraverse(u->right);\n}\nUsingrecursionthiswayproducesveryshort,simplecode,butcanbeproblematic.\nThe maximum depth of the recursion is given by the maximum depth of a node in the\nbinarytree,i.e.,thetree\u2019sheight. Iftheheightofthetreeisverylarge,thenthiscouldvery\nwellusemorestackspacethanisavailable,causingacrash.\nLuckily,traversingabinarytreecanbedonewithoutrecursion. Thisisdoneusing\nanalgorithmthatuseswhereitcamefromtodecidewhereitwillgonext. SeeFigure6.3.\nIf we arrive at a node u from u.parent, then the next thing to do is to visit u.left. If we\narriveatufromu.left,thenthenextthingtodoistovisitu.right. Ifwearriveatufrom\nu.right, then we are done visiting u\u2019s subtree, so we return to u.parent. The following\ncodeimplementsthisidea,withcodeincludedforhandlingthecaseswhereanyofu.left,\nu.right,oru.parentisnil:\nBinaryTree\nvoid traverse2() {\nNode *u = r, *prev = nil, *next;\nwhile (u != nil) {\nif (prev == u->parent) {\nif (u->left != nil) next = u->left;\nelse if (u->right != nil) next = u->right;\nelse next = u->parent;\n} else if (prev == u->left) {\nif (u->right != nil) next = u->right;\nelse next = u->parent;\n} else {\nnext = u->parent;\n}\nprev = u;\nu = next;\n}\n}\nThesamethingsthatcanbecomputedwithrecursivealgorithmscanalsobedone\nthisway. Forexample,tocomputethesizeofthetreewekeepacounter,n,andincrement\nnwhenevervisitinganodeforthefirsttime:\n118\n6.BinaryTrees 6.1.BinaryTree: ABasicBinaryTree\nr\nu.parent\nu\nu.left u.right\nFigure 6.3: The three cases that occur at node u when traversing a binary tree non-\nrecursively,andtheresultingtraversalofthetree.\nBinaryTree\nint size2() {\nNode *u = r, *prev = nil, *next;\nint n = 0;\nwhile (u != nil) {\nif (prev == u->parent) {\nn++;\nif (u->left != nil) next = u->left;\nelse if (u->right != nil) next = u->right;\nelse next = u->parent;\n} else if (prev == u->left) {\nif (u->right != nil) next = u->right;\nelse next = u->parent;\n} else {\nnext = u->parent;\n}\nprev = u;\nu = next;\n}\nreturn n;\n}\nInsomeimplementationsofbinarytrees,theparentfieldisnotused. Whenthisis\nthe case, a non-recursive implementation is still possible, but the implementation has to\nuseaList(orStack)tokeeptrackofthepathfromthecurrentnodetotheroot.\nA special kind of traversal that does not fit the pattern of the above functions is\nthe breadth-first traversal. In a breadth-first traversal, the nodes are visited level-by-level\nstarting at the root and working our way down, visiting the nodes at each level from left\n119\n6.BinaryTrees 6.2.BinarySearchTree: AnUnbalancedBinarySearchTree\nr\nFigure6.4: Duringabreadth-firsttraversal,thenodesofabinarytreearevisitedlevel-by-\nlevel,andleft-to-rightwithineachlevel.\ntoright. ThisissimilartothewaywewouldreadapageofEnglishtext. (SeeFigure6.4.)\nThisisimplementedusingaqueue,q,thatinitiallycontainsonlytheroot,r. Ateachstep,\nwe extract the next node, u, from q, process u and add u.left and u.right (if they are\nnon-nil)toq:\nBinaryTree\nvoid bfTraverse() {\nArrayDeque<Node*> q;\nif (r != nil) q.add(q.size(),r);\nwhile (q.size() > 0) {\nNode *u = q.remove(q.size()-1);\nif (u->left != nil) q.add(q.size(),u->left);\nif (u->right != nil) q.add(q.size(),u->right);\n}\n}\n6.2 BinarySearchTree: An Unbalanced Binary Search Tree\nA BinarySearchTree is a special kind of binary tree in which each node, u, also stores a\ndata value, u.x, from some total order. The data values in a binary search tree obey the\nbinary search tree property: For a node, u, every data value stored in the subtree rooted\nat u.left is less than u.x and every data value stored in the subtree rooted at u.right is\ngreaterthanu.x. AnexampleofaBinarySearchTreeisshowninFigure6.5.\n6.2.1 Searching\nThe binary search tree property is extremely useful because it allows us to quickly locate\na value, x, in a binary search tree. To do this we start searching for x at the root, r. When\nexamininganode,u,therearethreecases:\n120\n6.BinaryTrees 6.2.BinarySearchTree: AnUnbalancedBinarySearchTree\n7\n3 11\n1 5 9 13\n4 6 8 12 14\nFigure6.5: Abinarysearchtree.\n1. Ifx<u.xthenthesearchproceedstou.left;\n2. Ifx>u.xthenthesearchproceedstou.right;\n3. Ifx=u.xthenwehavefoundthenodeucontainingx.\nThe searchterminates when Case 3occurs or whenu=nil. In the former case, we found\nx. Inthelattercase,weconcludethatxisnotinthebinarysearchtree.\nBinarySearchTree\nT findEQ(T x) {\nNode *w = r;\nwhile (w != nil) {\nint comp = compare(x, w->x);\nif (comp < 0) {\nw = w->left;\n} else if (comp > 0) {\nw = w->right;\n} else {\nreturn w->x;\n}\n}\nreturn null;\n}\nTwo examples of searches in a binary search tree are shown in Figure 6.6. As the\nsecond example shows, even if we don\u2019t find x in the tree, we still gain some valuable\ninformation. Ifwelookatthelastnode,u,atwhichCase1occurred,weseethatu.xisthe\nsmallest value in the tree that is greater than x. Similarly, the last node at which Case 2\n121\n6.BinaryTrees 6.2.BinarySearchTree: AnUnbalancedBinarySearchTree\n7 7\n3 11 3 11\n1 5 9 13 1 5 9 13\n4 6 8 12 14 4 6 8 12 14\n(a) (b)\nFigure 6.6: An example of (a) a successful search (for 6) and (b) an unsuccessful search\n(for10)inabinarysearchtree.\noccurred contains the largest value in the tree that is less than x. Therefore, by keeping\ntrack of the last node, z, at which Case 1 occurs, a BinarySearchTree can implement the\nfind(x) operation that returns the smallest value stored in the tree that is greater than or\nequaltox:\nBinarySearchTree\nT find(T x) {\nNode *w = r, *z = nil;\nwhile (w != nil) {\nint comp = compare(x, w->x);\nif (comp < 0) {\nz = w;\nw = w->left;\n} else if (comp > 0) {\nw = w->right;\n} else {\nreturn w->x;\n}\n}\nreturn z == nil ? null : z->x;\n}\n6.2.2 Addition\nToaddanewvalue,x,toaBinarySearchTree,wefirstsearchforx. Ifwefindit,thenthere\nisnoneedtoinsertit. Otherwise,westorexataleafchildofthelastnode,p,encountered\nduring the search for x. Whether the new node is the left or right child of p depends on\n122\n6.BinaryTrees 6.2.BinarySearchTree: AnUnbalancedBinarySearchTree\ntheresultofcomparingxandp.x.\nBinarySearchTree\nbool add(T x) {\nNode *p = findLast(x);\nNode *u = new Node;\nu->x = x;\nreturn addChild(p, u);\n}\nBinarySearchTree\nNode* findLast(T x) {\nNode *w = r, *prev = nil;\nwhile (w != nil) {\nprev = w;\nint comp = compare(x, w->x);\nif (comp < 0) {\nw = w->left;\n} else if (comp > 0) {\nw = w->right;\n} else {\nreturn w;\n}\n}\nreturn prev;\n}\nBinarySearchTree\nbool addChild(Node *p, Node *u) {\nif (p == nil) {\nr = u; // inserting into empty tree\n} else {\nint comp = compare(u->x, p->x);\nif (comp < 0) {\np->left = u;\n} else if (comp > 0) {\np->right = u;\n} else {\nreturn false; // u.x is already in the tree\n}\nu->parent = p;\n}\nn++;\nreturn true;\n}\n123\n6.BinaryTrees 6.2.BinarySearchTree: AnUnbalancedBinarySearchTree\n7 7\n3 11 3 11\n1 5 9 13 1 5 9 13\n4 6 8 12 14 4 6 8 12 14\n8.5\nFigure6.7: Insertingthevalue8.5intoabinarysearchtree.\nAn example is shown in Figure 6.7. The most time-consuming part of this process is the\ninitial search for x, which takes time proportional to the height of the newly added node\nu. Intheworstcase,thisisequaltotheheightoftheBinarySearchTree.\n6.2.3 Removal\nDeletingavaluestoredinanode,u,ofaBinarySearchTreeisalittlemoredifficult. Ifuis\naleaf,thenwecanjustdetachufromitsparent. Evenbetter: Ifuhasonlyonechild,then\nwecanspliceufromthetreebyhavingu.parentadoptu\u2019schild(seeFigure6.8):\nBinarySearchTree\nvoid splice(Node *u) {\nNode *s, *p;\nif (u->left != nil) {\ns = u->left;\n} else {\ns = u->right;\n}\nif (u == r) {\nr = s;\np = nil;\n} else {\np = u->parent;\nif (p->left == u) {\np->left = s;\n} else {\np->right = s;\n}\n}\nif (s != nil) {\ns->parent = p;\n124\n6.BinaryTrees 6.2.BinarySearchTree: AnUnbalancedBinarySearchTree\n7\n3 11\n1 5 9 13\n4 6 8 12 14\nFigure6.8: Removingaleaf(6)oranodewithonlyonechild(9)iseasy.\n}\nn--;\n}\nThingsgettricky,though,whenuhastwochildren. Inthiscase,thesimplestthing\ntodoistofindanode,w,thathaslessthantwochildrensuchthatwecanreplaceu.xwith\nw.x. Tomaintainthebinarysearchtreeproperty,thevaluew.xshouldbeclosetothevalue\nof u.x. For example, picking w such that w.x is the smallest value greater than u.x will do.\nFinding the node w is easy; it is the smallest value in the subtree rooted at u.right. This\nnodecanbeeasilyremovedbecauseithasnoleftchild. (SeeFigure6.9)\nBinarySearchTree\nvoid remove(Node *u) {\nif (u->left == nil || u->right == nil) {\nsplice(u);\ndelete u;\n} else {\nNode *w = u->right;\nwhile (w->left != nil)\nw = w->left;\nu->x = w->x;\nsplice(w);\ndelete w;\n}\n}\n6.2.4 Summary\nThefind(x),add(x),andremove(x)operationsinaBinarySearchTreeeachinvolvefollow-\ningapathfromtherootofthetreetosomenodeinthetree. Withoutknowingmoreabout\nthe shape of the tree it is difficult to say much about the length of this path, except that\n125\n6.BinaryTrees 6.3.DiscussionandExercises\n7 7\n3 11 3 12\n1 5 9 13 1 5 9 13\n4 6 8 12 14 4 6 8 14\nFigure6.9: Deletingavalue(11)fromanode,u,withtwochildrenisdonebyreplacingu\u2019s\nvaluewiththesmallestvalueintherightsubtreeofu.\nit is less than n, the number of nodes in the tree. The following (unimpressive) theorem\nsummarizestheperformanceoftheBinarySearchTreedatastructure:\nTheorem 6.1. A BinarySearchTree implements the SSet interface. A BinarySearchTree\nsupportstheoperationsadd(x),remove(x),andfind(x)inO(n)timeperoperation.\nTheorem6.1comparespoorlywithTheorem4.1,whichshowsthattheSkiplistSSet\nstructurecanimplementtheSSetinterfacewithO(logn)expectedtimeperoperation. The\nproblem with the BinarySearchTree structure is that it can become unbalanced. Instead\noflookinglikethetreeinFigure6.5itcanlooklikealongchainofnnodes,allbutthelast\nhavingexactlyonechild.\nThere are a number of ways of avoiding unbalanced binary search trees, all of\nwhich lead to data structures that have O(logn) time operations. In Chapter 7 we show\nhow O(logn) expected time operations can be achieved with randomization. In Chapter 8\nwe show how O(logn) amortized time operations can be achieved with partial rebuilding\noperations. InChapter9weshowhowO(logn)worst-casetimeoperationscanbeachieved\nbysimulatingatreethatisnotbinary: atreeinwhichnodescanhaveuptofourchildren.\n6.3 Discussion and Exercises\nBinary trees have been used to model relationships for literally thousands of years. One\nreason for this is that binary trees naturally model (pedigree) family trees. These are the\nfamily trees in which the root is a person, the left and right children are the person\u2019s\nparents, and so on, recursively. In more recent centuries binary trees have also been used\ntomodelspecies-treesinbiology,wheretheleavesofthetreerepresentextantspeciesand\nthe internal nodes of the tree represent speciation events in which two populations of a\nsinglespeciesevolveintotwoseparatespecies.\n126\n6.BinaryTrees 6.3.DiscussionandExercises\nBinarysearchtreesappeartohavebeendiscoveredindependentlybyseveralgroups\ninthe1950s[43,Section6.2.2]. Furtherreferencestospecifickindsofbinarysearchtrees\nareprovidedinsubsequentchapters.\nWhen implementing a binary tree from scratch, there are several design decisions\nto be made. One of these is the question of whether or not each node stores a pointer to\nitsparent. Ifmostoftheoperationssimplyfollowaroot-to-leafpath,thenparentpointers\nareunnecessary,andareapotentialsourceofcodingerrors. Ontheotherhand,thelackof\nparent pointers means that tree traversals must be done recursively or with the use of an\nexplicitstack. Someothermethods(likeinsertingordeletingintosomekindsofbalanced\nbinarysearchtrees)arealsocomplicatedbythelackofparentpointers.\nAnother design decision is concerned with how to store the parent, left child, and\nright child pointers at a node. In the implementation given here, they are stored as sep-\narate variables. Another option is to store them in an array, p, of length 3, so that u.p[0]\nis the left child of u, u.p[1] is the right child of u, and u.p[2] is the parent of u. Using an\narraythiswaymeansthatsomesequencesofifstatementscanbesimplifiedintoalgebraic\nexpressions.\nAn example of such a simplification occurs during tree traversal. If a traversal\narrives at a node u from u.p[i], then the next node in the traversal is u.p[(i+1) mod 3].\nSimilar examples occur when there is left-right symmetry. For example, the sibling of\nu.p[i]isu.p[(i+1) mod 2]. Thisworkswhetheru.p[i]isaleftchild(i=0)orarightchild\n(i = 1) of u. In some cases this means that some complicated code that would otherwise\nneedtohavebothaleftversionandrightversioncanbewrittenonlyonce. Seethemethods\nrotateLeft(u)androtateRight(u)onpage139foranexample.\nExercise6.1. Provethatabinarytreehavingn 1nodeshasn 1edges.\n\u2265 \u2212\nExercise6.2. Provethatabinarytreehavingn 1realnodeshasn+1externalnodes.\n\u2265\nExercise 6.3. Prove that, if a binary tree, T, has at least one leaf, then either (a) T\u2019s root\nhasatmostonechildor(b)T hasmorethanoneleaf.\nExercise6.4. Writeanon-recursivevariantofthesize2()method,size(u),thatcomputes\nthesizeofthesubtreerootedatnodeu.\nExercise6.5. Writeanon-recursivemethod,height2(u),thatcomputestheheightofnode\nuinaBinaryTree.\nExercise6.6. A binary tree is balanced if, for every node u, the size of the subtrees rooted\natu.leftandu.rightdifferbyatmostone. Writearecursivemethod,isBalanced(),that\n127\n6.BinaryTrees 6.3.DiscussionandExercises\n0 11\n1 6 4 10\n2 3 7 9 0 3 6 9\n4 5 8 10 11 1 2 5 7 8\n5\n1 8\n0 3 7 10\n2 4 6 9 11\nFigure6.10: Pre-order,post-order,andin-ordernumberingsofabinarytree.\ntests if a binary tree is balanced. Your method should run in O(n) time. (Be sure to test\nyourcodeonsomelargetreeswithdifferentshapes;itiseasytowriteamethodthattakes\nmuchlongerthanO(n)time.)\nA pre-order traversal of a binary tree is a traversal that visits each node, u, before\nany of its children. An in-order traversal visits u after visiting all the nodes in u\u2019s left\nsubtree but before visiting any of the nodes in u\u2019s right subtree. A post-order traversal\nvisitsuonlyaftervisitingallothernodesinu\u2019ssubtree. Thepre/in/post-ordernumbering\nof a tree labels the nodes of a tree with the integers 0,...,n 1 in the order that they are\n\u2212\nencounteredbyapre/in/post-ordertraversal. SeeFigure6.10foranexample.\nExercise 6.7. Create a subclass of BinaryTree whose nodes have fields for storing pre-\norder, post-order, and in-order numbers. Write recursive methods preOrderNumber(),\ninOrderNumber(), and postOrderNumbers() that assign these numbers correctly. These\nmethodsshouldeachruninO(n)time.\nExercise6.8. Writenon-recursivefunctionsnextPreOrder(u),nextInOrder(u),andnextPostOrder(u)\nthatreturnthenodethatfollowsuinapre-order,in-order,orpost-ordertraversal,respec-\ntively. Thesefunctionsshouldtakeamortizedconstanttime;ifwestartatanynodeuand\nrepeatedlycalloneofthesefunctionsandassignthereturnvaluetouuntilu=null,then\nthecostofallthesecallsshouldbeO(n).\n128\n6.BinaryTrees 6.3.DiscussionandExercises\nExercise6.9. Supposewearegivenabinarytreewithpre-post-andin-ordernumbersas-\nsignedtothenodes. Showhowthesenumberscanbeusedtoanswereachofthefollowing\nquestionsinconstanttime:\n1. Givenanodeu,determinethesizeofthesubtreerootedatu.\n2. Givenanodeu,determinethedepthofu.\n3. Giventwonodesuandw,determineifuisanancestorofw\nExercise 6.10. Suppose you are given a list of nodes with pre-order and in-order num-\nbers assigned. Prove that there is at most one possible tree with this pre-order/in-order\nnumberingandshowhowtoconstructit.\nExercise6.11. Showthattheshapeofanybinarytreeonnnodescanberepresentedusing\nat most 2(n 1) bits. (Hint: think about recording what happens during a traversal and\n\u2212\nthenplayingbackthatrecordingtoreconstructthetree.)\nExercise 6.12. Illustrate what happens when we add the values 3.5 and then 4.5 to the\nbinarysearchtreeinFigure6.5.\nExercise6.13. Illustratewhathappenswhenweremovethevalues3andthen5fromthe\nbinarysearchtreeinFigure6.5.\nExercise6.14. DesignandimplementamethodBinarySearchTreemethodgetLE(x),that\nreturns a list of all items in the tree that are less than or equal to x. The running time of\nyour method should be O(n +h) where n is the number of items less than or equal to x\n(cid:48) (cid:48)\nandhistheheightofthetree.\nExercise6.15. Describehowtoaddtheelements 1,...,n toaninitiallyemptyBinarySearchTree\n{ }\ninsuchawaythattheresultingtreehasheightn 1. Howmanywaysaretheretodothis?\n\u2212\nExercise6.16. IfwehavesomeBinarySearchTreeandperformtheoperationsadd(x)fol-\nlowed by remove(x) (with the same value of x) do we necessarily return to the original\ntree?\nExercise6.17. Canaremove(x)operationincreasetheheightofanynodeinaBinarySearchTree?\nIfso,byhowmuch?\nExercise6.18. Cananadd(x)operationincreasetheheightofanynodeinaBinarySearchTree?\nCanitincreasetheheightofthetree? Ifso,byhowmuch?\n129\n6.BinaryTrees 6.3.DiscussionandExercises\nExercise6.19. DesignandimplementaversionofBinarySearchTreeinwhicheachnode,\nu, maintains values u.size (the size of the subtree rooted at u), u.depth (the depth of u),\nandu.height(theheightofthesubtreerootedatu).\nThese values should be maintained, even during the add(x) and remove(x) opera-\ntions, but this should not increase the cost of these operations by more than a constant\nfactor.\n130\nChapter 7\nRandom Binary Search Trees\nIn this chapter, we present a binary search tree structure that uses randomization to\nachieveO(logn)expectedtimeforalloperations.\n7.1 Random Binary Search Trees\nConsiderthetwobinarysearchtreesshowninFigure7.1. Theoneontheleftisalistand\ntheotherisaperfectlybalancedbinarysearchtree. Theoneonthelefthasheightn 1=14\n\u2212\nandtheoneontherighthasheightthree.\nImagine how these two trees could have been constructed. The one on the left\noccursifwestartwithanemptyBinarySearchTreeandaddthesequence\n0,1,2,3,4,5,6,7,8,9,10,11,12,13,14 .\n(cid:104) (cid:105)\nNo other sequence of additions will create this tree (as you can prove by induction on n).\nOntheotherhand,thetreeontherightcanbecreatedbythesequence\n7,3,11,1,5,9,13,0,2,4,6,8,10,12,14 .\n(cid:104) (cid:105)\nOthersequencesworkaswell,including\n7,3,1,5,0,2,4,6,11,9,13,8,10,12,14 ,\n(cid:104) (cid:105)\nand\n7,3,1,11,5,0,2,4,6,9,13,8,10,12,14 .\n(cid:104) (cid:105)\nIn fact, there are 21,964,800 addition sequences that generate the tree on the right and\nonlyonethatgeneratesthetreeontheleft.\nThe above example gives some anecdotal evidence that, if we choose a random\npermutationof0,...,14,andadditintoabinarysearchtreethenwearemorelikelytoget\n131\n7.RandomBinarySearchTrees 7.1.RandomBinarySearchTrees\n0\n1\n2\n7\n3\n3 11\n...\n1 5 9 13\n14 0 2 4 6 8 10 12 14\nFigure7.1: Twobinarysearchtreescontainingtheintegers0,...,14.\naverybalancedtree(therightsideofFigure7.1)thanwearetogetaveryunbalancedtree\n(theleftsideofFigure7.1).\nWe can formalize this notion by studying random binary search trees. A ran-\ndom binary search tree of size n is obtained in the following way: Take a random per-\nmutation, x ,...,x , of the integers 0,...,n 1 and add its elements, one by one, into\n0 n 1\n\u2212 \u2212\naBinarySearchTree. Byrandompermutationwemeanthateachofthepossiblen!permu-\ntations (orderings) of 0,...,n 1 is equally likely, so that the probability of obtaining any\n\u2212\nparticularpermutationis1/n!.\nNotethatthevalues0,...,n 1couldbereplacedbyanyorderedsetofnelements\n\u2212\nwithout changing any of the properties of the random binary search tree. The element\nx 0,...,n 1 issimplystandinginfortheelementofrankxinanorderedsetofsizen.\n\u2208{ \u2212 }\nBeforewecanpresentourmainresultaboutrandombinarysearchtrees, wemust\ntakesometimeforashortdigressiontodiscussatypeofnumberthatcomesupfrequently\nwhen studying randomized structures. For a non-negative integer, k, the k-th harmonic\nnumber,denotedH ,isdefinedas\nk\nH =1+1/2+1/3+ +1/k .\nk\n\u00b7\u00b7\u00b7\nThe harmonic number H has no simple closed form, but it is very closely related to the\nk\nnaturallogarithmofk. Inparticular,\nlnk<H lnk+1 .\nk\n\u2264\nk\nReaderswhohavestudiedcalculusmightnoticethatthisisbecausetheintegral (1/x)dx=\n1\nlnk. Keeping in mind that an integral canbe interpreted as the area between a curve and\n(cid:82)\n132\n7.RandomBinarySearchTrees 7.1.RandomBinarySearchTrees\n1 1\nf(x)=1/x\n1/2 1/2\n1/3 1/3\n. .\n. .\n. .\n1/k 1/k\n0 1 2 3 ... k 1 2 3 ... k\nFigure 7.2: The kth harmonic number H = k 1/i is upper-bounded by 1+ k (1/x)dx\nk i=1 1\nk k\nandlower-boundedby (1/x)dx. Thevalueof (1/x)dxisgivenbytheareaoftheshaded\n1 (cid:80)1 (cid:82)\nregion,whilethevalueofH isgivenbytheareaoftherectangles.\n(cid:82) k (cid:82)\nk\nthe x-axis, the value of H can be lower-bounded by the integral (1/x)dx and upper-\nk 1\nk\nboundedby1+ (1/x)dx. (SeeFigure7.2foragraphicalexplanation.)\n(cid:82)\n1\n(cid:82)\nLemma7.1. Inarandombinarysearchtreeofsizen,thefollowingstatementshold:\n1. Foranyx 0,...,n 1 ,theexpectedlengthofthesearchpathforxisH +H O(1).1\nx+1 n x\n\u2208{ \u2212 } \u2212 \u2212\n2. Foranyx ( 1,n) 0,...,n 1 ,theexpectedlengthofthesearchpathforxisH +H .\nx n x\n\u2208 \u2212 \\{ \u2212 } (cid:100) (cid:101) \u2212(cid:100) (cid:101)\nWewillproveLemma7.1inthenextsection. Fornow,considerwhatthetwoparts\nofLemma7.1tellus. Thefirstparttellsusthatifwesearchforanelementinatreeofsize\nn,thentheexpectedlengthofthesearchpathisatmost2lnn+O(1). Thesecondparttells\nusthesamethingaboutsearchingforavaluenotstoredinthetree. Whenwecomparethe\ntwopartsofthelemma,weseethatitisonlyslightlyfastertosearchforsomethingthatis\ninthetreecomparedtosomethingthatisnotinthetree.\n7.1.1 ProofofLemma7.1\nThe key observation needed to prove Lemma 7.1 is the following: The search path for a\nvalue x in the open interval ( 1,n) in a random binary search tree, T, contains the node\n\u2212\nwithkeyi <xifandonlyif,intherandompermutationusedtocreateT,i appearsbefore\nanyof i+1,i+2,..., x .\n{ (cid:98) (cid:99)}\n1Theexpressionsx+1andn xcanbeinterpretedrespectivelyasthenumberofelementsinthetreeless\n\u2212\nthanorequaltoxandthenumberofelementsinthetreegreaterthanorequaltox.\n133\n7.RandomBinarySearchTrees 7.1.RandomBinarySearchTrees\nj\n...,i,...,j 1 j+1,..., x ,...\n\u2212 b c\nFigure 7.3: The value i < x is on the search path for x if and only if i is the first element\namong i,i+1,..., x addedtothetree.\n{ (cid:98) (cid:99)}\nTo see this, refer to Figure 7.3 and notice that, until some value in i,i+1,..., x\n{ (cid:98) (cid:99)}\nis added, the search paths for each value in the open interval (i 1, x +1) are identical.\n\u2212 (cid:98) (cid:99)\n(Remember that for two search values to have different search paths, there must be some\nelement in the tree that compares differently with them.) Let j be the first element in\ni,i+1,..., x toappearintherandompermutation. Noticethatj isnowandwillalways\n{ (cid:98) (cid:99)}\nbeonthesearchpathforx. Ifj (cid:44) i thenthenodeu containingj iscreatedbeforethenode\nj\nu thatcontainsi. Later,wheni isadded,itwillbeaddedtothesubtreerootedatu .left,\ni j\nsincei <j. Ontheotherhand,thesearchpathforxwillnevervisitthissubtreebecauseit\nwillproceedtou .rightaftervisitingu .\nj j\nSimilarly,fori >x,i appearsinthesearchpathforxifandonlyifi appearsbefore\nanyof x , x +1,...,i 1 intherandompermutationusedtocreateT.\n{(cid:100) (cid:101) (cid:100) (cid:101) \u2212 }\nNotice that, if we start with a random permutation of 0,...,n , then the subse-\n{ }\nquences containing only i,i+1,..., x and x , x +1,...,i 1 are also random permu-\n{ (cid:98) (cid:99)} {(cid:100) (cid:101) (cid:100) (cid:101) \u2212 }\ntationsoftheirrespectiveelements. Eachelement,then,inthesubsets i,i+1,..., x and\n{ (cid:98) (cid:99)}\nx , x +1,...,i 1 isequallylikelytoappearbeforeanyotherinitssubsetintherandom\n{(cid:100) (cid:101) (cid:100) (cid:101) \u2212 }\npermutationusedtocreateT. Sowehave\n1/( x i+1) ifi <x\nPr i isonthesearchpathforx = (cid:98) (cid:99)\u2212 .\n{ } \uf8f1 1/(i x +1) ifi >x\n\uf8f4\uf8f4\uf8f2 \u2212(cid:100) (cid:101)\n\uf8f4\uf8f4\uf8f3\n134\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\nWith this observation, the proof of Lemma 7.1 involves some simple calculations\nwithharmonicnumbers:\nProofofLemma7.1. Let I be the indicator random variable that is equal to one when i\ni\nappearsonthesearchpathforxandzerootherwise. Thenthelengthofthesearchpathis\ngivenby\nI\ni\ni 0,...,n 1 x\n\u2208{ (cid:88)\u2212 }\\{ }\nso,ifx 0,...,n 1 ,theexpectedlengthofthesearchpathisgivenby(seeFigure7.4.a)\n\u2208{ \u2212 }\nx 1 n 1 x 1 n 1\n\u2212 \u2212 \u2212 \u2212\nE I + I = E[I ]+ E[I ]\ni i i i\n\uf8ee i=0 i=x+1 \uf8f9 i=0 i=x+1\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 (cid:88) (cid:88) \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb= (cid:88)\nx\n\u2212\n1\n1/( x\n(cid:88)\ni+1)+\nn\n\u2212\n1\n1/(i x +1)\n(cid:98) (cid:99)\u2212 \u2212(cid:100) (cid:101)\ni=0 i=x+1\n(cid:88) (cid:88)\nx 1 n 1\n\u2212 \u2212\n= 1/(x i+1)+ 1/(i x+1)\n\u2212 \u2212\ni=0 i=x+1\n(cid:88) (cid:88)\n1 1 1\n= + + +\n2 3 \u00b7\u00b7\u00b7 x+1\n1 1 1\n+ + + +\n2 3 \u00b7\u00b7\u00b7 n x\n\u2212\n=H +H 2 .\nx+1 n x\n\u2212 \u2212\nThecorrespondingcalculationsforasearchvaluex ( 1,n) 0,...,n 1 arealmostiden-\n\u2208 \u2212 \\{ \u2212 }\ntical(seeFigure7.4.b).\n7.1.2 Summary\nThefollowingtheoremsummarizestheperformanceofarandombinarysearchtree:\nTheorem7.1. ArandombinarysearchtreecanbeconstructedinO(nlogn)time. Inarandom\nbinarysearchtree,thefind(x)operationtakesO(logn)expectedtime.\nWeshouldemphasizeagainthattheexpectationinTheorem7.1iswithrespectto\nthe random permutation used to create the random binary search tree. In particular, it\ndoesnotdependonarandomchoiceofx;itistrueforeveryvalueofx.\n7.2 Treap: A Randomized Binary Search Tree\nThe problem with random binary search trees is, of course, that they are not dynamic.\nThey don\u2019t support the add(x) or remove(x) operations needed to implement the SSet in-\n135\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\nPr I =1 1 1 1 1 1 1 1\n{ i } x+1 x \u00b7\u00b7\u00b7 3 2 2 3 \u00b7\u00b7\u00b7 n x\n\u2212\ni 0 1 x 1 x x+1 n 1\n\u00b7\u00b7\u00b7 \u2212 \u00b7\u00b7\u00b7 \u2212\n(a)\nPr I =1 1 1 1 1 1 1 1 1 1\n{ i } x+1 x \u00b7\u00b7\u00b7 3 2 2 3 \u00b7\u00b7\u00b7 n x\nb c b c \u2212b c\ni 0 1 x x n 1\n\u00b7\u00b7\u00b7 b c d e \u00b7\u00b7\u00b7 \u2212\n(b)\nFigure7.4: Theprobabilitiesofanelementbeingonthesearchpathforxwhen(a)xisan\nintegerand(b)whenxisnotaninteger.\nterface. InthissectionwedescribeadatastructurecalledaTreapthatusesLemma7.1to\nimplementtheSSetinterface.2\nAnodeinaTreapislikeanodeinaBinarySearchTreeinthatithasadatavalue,\nx,butitalsocontainsauniquenumericalpriority,p,thatisassignedatrandom:\nTreap\nclass TreapNode : public BSTNode<Node, T> {\nfriend class Treap<Node,T>;\nint p;\n};\nInadditiontobeingabinarysearchtree,thenodesinaTreapalsoobeytheheapproperty:\n\u2022 (HeapProperty)Ateverynodeu,excepttheroot,u.parent.p<u.p.\nInotherwords,eachnodehasaprioritysmallerthanthatofitstwochildren. Anexample\nisshowninFigure7.5.\nThe heap and binary search tree conditions together ensure that, once the key (x)\nandpriority(p)foreachnodearedefined,theshapeoftheTreapiscompletelydetermined.\nThe heap property tells us that the node with minimum priority has to be the root, r, of\nthe Treap. The binary search tree property tells us that all nodes with keys smaller than\nr.x are stored in the subtree rooted at r.left and all nodes with keys larger than r.x are\nstoredinthesubtreerootedatr.right.\n2The names Treap comes from the fact that this data structure is simultaneously a binary search tree\n(Section6.2)andaheap(Chapter10).\n136\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\n3,1\n1,6 5,11\n0,9 2,99 4,14 9,17\n7,22\n6,42 8,49\nFigure 7.5: An example of a Treap containing the integers 0,...,9. Each node, u, is illus-\ntratedasaboxcontainingu.x,u.p.\nThe important point about the priority values in a Treap is that they are unique\nand assigned at random. Because of this, there are two equivalent ways we can think\naboutaTreap. Asdefinedabove,aTreapobeystheheapandbinarysearchtreeproperties.\nAlternatively, we can think of a Treap as a BinarySearchTree whose nodes were added\nin increasing order of priority. For example, the Treap in Figure 7.5 can be obtained by\naddingthesequenceof(x,p)values\n(3,1),(1,6),(0,9),(5,11),(4,14),(9,17),(7,22),(6,42),(8,49),(2,99)\n(cid:104) (cid:105)\nintoaBinarySearchTree.\nSince the priorities are chosen randomly, this is equivalent to taking a random\npermutationofthekeys\u2014inthiscasethepermutationis\n3,1,0,5,9,4,7,6,8,2\n(cid:104) (cid:105)\n\u2014 and adding these to a BinarySearchTree. But this means that the shape of a treap is\nidenticaltothatofarandombinarysearchtree. Inparticular,ifwereplaceeachkeyxby\nitsrank,3 thenLemma7.1applies. RestatingLemma7.1intermsofTreaps,wehave:\nLemma7.2. InaTreapthatstoresasetS ofnkeys,thefollowingstatementshold:\n1. Foranyx S,theexpectedlengthofthesearchpathforxisH +H O(1).\nr(x)+1 n r(x)\n\u2208 \u2212 \u2212\n2. Foranyx(cid:60) S,theexpectedlengthofthesearchpathforxisH +H .\nr(x) n r(x)\n\u2212\n3TherankofanelementxinasetSofelementsisthenumberofelementsinSthatarelessthanx.\n137\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\nu w\nw u\nrotateRight(u)\n\u21d2\nC rotateLeft(w) A\n\u21d0\nA B B C\nFigure7.6: Leftandrightrotationsinabinarysearchtree.\nHere,r(x)denotestherankofxinthesetS x .\n\u222a{ }\nAgain, we emphasize that the expectation in Lemma 7.2 is taken over the random\nchoices of the priorities for each node. It does not require any assumptions about the\nrandomnessinthekeys.\nLemma 7.2 tells us that Treaps can implement the find(x) operation efficiently.\nHowever, the real benefit of a Treap is that it can support the add(x) and delete(x) oper-\nations. To do this, it needs to perform rotations in order to maintain the heap property.\nRefer to Figure 7.6. A rotation in a binary search tree is a local modification that takes a\nparent u of a node w and makes w the parent of u, while preserving the binary search tree\nproperty. Rotationscomeintwoflavours: leftorrightdependingonwhetherwisarightor\nleftchildofu,respectively.\nThecodethatimplementsthishastohandlethesetwopossibilitiesandbecareful\nofaboundarycase(whenuistheroot)sotheactualcodeisalittlelongerthanFigure7.6\nwouldleadareadertobelieve:\nBinarySearchTree\nvoid rotateLeft(Node *u) {\nNode *w = u->right;\nw->parent = u->parent;\nif (w->parent != nil) {\nif (w->parent->left == u) {\nw->parent->left = w;\n} else {\nw->parent->right = w;\n}\n}\nu->right = w->left;\n138\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\nif (u->right != nil) {\nu->right->parent = u;\n}\nu->parent = w;\nw->left = u;\nif (u == r) { r = w; r->parent = nil; }\n}\nvoid rotateRight(Node *u) {\nNode *w = u->left;\nw->parent = u->parent;\nif (w->parent != nil) {\nif (w->parent->left == u) {\nw->parent->left = w;\n} else {\nw->parent->right = w;\n}\n}\nu->left = w->right;\nif (u->left != nil) {\nu->left->parent = u;\n}\nu->parent = w;\nw->right = u;\nif (u == r) { r = w; r->parent = nil; }\n}\nIntermsoftheTreapdatastructure,themostimportantpropertyofarotationisthatthe\ndepthofwdecreasesbyonewhilethedepthofuincreasesbyone.\nUsing rotations, we can implement the add(x) operation as follows: We create a\nnewnode,u,andassignu.x=xandpickarandomvalueforu.p. Nextweadduusingthe\nusual add(x) algorithm for a BinarySearchTree, so that u is now a leaf of the Treap. At\nthispoint,ourTreapsatisfiesthebinarysearchtreeproperty,butnotnecessarilytheheap\nproperty. Inparticular,itmaybethecasethatu.parent.p>u.p. Ifthisisthecase,thenwe\nperform a rotation at node w=u.parent so that u becomes the parent of w. If u continues\ntoviolatetheheapproperty,wewillhavetorepeatthis,decreasingu\u2019sdepthbyoneevery\ntime,untilueitherbecomestherootoru.parent.p<u.p.\nTreap\nbool add(T x) {\nNode *u = new Node;\nu->x = x;\nu->p = rand();\nif (BinarySearchTree<Node,T>::add(u)) {\n139\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\nbubbleUp(u);\nreturn true;\n}\nreturn false;\n}\nvoid bubbleUp(Node *u) {\nwhile (u->parent != nil && u->parent->p > u->p) {\nif (u->parent->right == u) {\nrotateLeft(u->parent);\n} else {\nrotateRight(u->parent);\n}\n}\nif (u->parent == nil) {\nr = u;\n}\n}\nAnexampleofanadd(x)operationisshowninFigure7.7.\nThe running time of the add(x) operation is given by the time it takes to follow\nthe search path for x plus the number of rotations performed to move the newly-added\nnode, u, up to its correct location in the Treap. By Lemma 7.2, the expected length of the\nsearch path is at most 2lnn+O(1). Furthermore, each rotation decreases the depth of u.\nThis stops if u becomes the root, so the expected number of rotations cannot exceed the\nexpected length of the search path. Therefore, the expected running time of the add(x)\noperationinaTreapisO(logn). (Exercise7.5asksyoutoshowthattheexpectednumber\nofrotationsperformedduringanadditionisactuallyonlyO(1).)\nThe remove(x) operation in a Treap is the opposite of the add(x) operation. We\nsearchforthenode,u,containingxandthenperformrotationstomoveudownwardsuntil\nitbecomesaleafandthenwespliceufromtheTreap. Noticethat,tomoveudownwards,\nwe can perform either a left or right rotation at u, which will replace u with u.right or\nu.left,respectively. Thechoiceismadebythefirstofthefollowingthatapply:\n1. Ifu.leftandu.rightarebothnull,thenuisaleafandnorotationisperformed.\n2. If u.left (or u.right) is null, then perform a right (or left, respectively) rotation at\nu.\n3. If u.left.p < u.right.p (or u.left.p > u.right.p), then perform a right rotation (or\nleftrotation,respectively)atu.\n140\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\n3,1\n1,6 5,11\n0,9 2,99 4,14 9,14\n1.5,4 7,22\n6,42 8,49\n3,1\n1,6 5,11\n0,9 1.5,4 4,14 9,14\n2,99 7,22\n6,42 8,49\n3,1\n1.5,4 5,11\n1,6 2,99 4,14 9,14\n0,9 7,22\n6,42 8,49\nFigure7.7: Addingthevalue1.5intotheTreapfromFigure7.5.\n141\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\nThese three rules ensure that the Treap doesn\u2019t become disconnected and that the heap\npropertyisrestoredonceuisremoved.\nTreap\nbool remove(T x) {\nNode *u = findLast(x);\nif (u != nil && compare(u->x, x) == 0) {\ntrickleDown(u);\nsplice(u);\ndelete u;\nreturn true;\n}\nreturn false;\n}\nvoid trickleDown(Node *u) {\nwhile (u->left != nil || u->right != nil) {\nif (u->left == nil) {\nrotateLeft(u);\n} else if (u->right == nil) {\nrotateRight(u);\n} else if (u->left->p < u->right->p) {\nrotateRight(u);\n} else {\nrotateLeft(u);\n}\nif (r == u) {\nr = u->parent;\n}\n}\n}\nAnexampleoftheremove(x)operationisshowninFigure7.8.\nThe trick to analyze the running time of the remove(x) operation is to notice that\nthisoperationisthereverseoftheadd(x)operation. Inparticular,ifweweretoreinsertx,\nusing the same priority u.p, then the add(x) operation would do exactly the same number\nof rotations and would restore the Treap to exactly the same state it was in before the\nremove(x) operation took place. (Reading from bottom-to-top, Figure 7.8 illustrates the\naddition of the value 9 into a Treap.) This means that the expected running time of the\nremove(x) on a Treap of size n is proportional to the expected running time of the add(x)\noperationonaTreapofsizen 1. Weconcludethattheexpectedrunningtimeofremove(x)\n\u2212\nisO(logn).\n142\n7.RandomBinarySearchTrees 7.2.Treap: ARandomizedBinarySearchTree\n3,1\n1,6 5,11\n0,9 2,99 4,14 9,17\n7,22\n6,42 8,49\n3,1\n1,6 5,11\n0,9 2,99 4,14 7,22\n6,42 9,17\n8,49\n3,1\n1,6 5,11\n0,9 2,99 4,14 7,22\n6,42 8,49\n9,17\n3,1\n1,6 5,11\n0,9 2,99 4,14 7,22\n6,42 8,49\nFigure7.8: Removingthevalue9fromtheTreapinFigure7.5.\n143\n7.RandomBinarySearchTrees 7.3.DiscussionandExercises\n7.2.1 Summary\nThefollowingtheoremsummarizestheperformanceoftheTreapdatastructure:\nTheorem7.2. ATreapimplementstheSSetinterface. ATreapsupportstheoperationsadd(x),\nremove(x),andfind(x)inO(logn)expectedtimeperoperation.\nItisworthcomparingtheTreapdatastructuretotheSkiplistSSetdatastructure.\nBothimplementtheSSetoperationsinO(logn)expectedtimeperoperation. Inbothdata\nstructures, add(x) and remove(x) involve a search and then a constant number of pointer\nchanges (see Exercise 7.5 below). Thus, for both these structures, the expected length of\nthesearchpathisthecriticalvalueinassessingtheirperformance. InaSkiplistSSet,the\nexpectedlengthofasearchpathis\n2logn+O(1) ,\nInaTreap,theexpectedlengthofasearchpathis\n2lnn+O(1) 1.386logn+O(1) .\n\u2248\nThus, thesearchpathsinaTreapareconsiderablyshorterandthistranslatesintonotice-\nablyfasteroperationsonTreapsthanSkiplists. Exercise4.7inChapter4showshowthe\nexpectedlengthofthesearchpathinaSkiplistcanbereducedto\nelnn+O(1) 1.884logn+O(1)\n\u2248\nby using biased coin tosses. Even with this optimization, the expected length of search\npathsinaSkiplistSSetisnoticeablylongerthaninaTreap.\n7.3 Discussion and Exercises\nRandom binary search trees have been studied extensively. Devroye [16] gives a proof of\nLemma 7.1 and related results. There are much stronger results in the literature as well.\nThe most impressive of which is due to Reed [57], who shows that the expected height of\narandombinarysearchtreeis\n\u03b1lnn \u03b2lnlnn+O(1)\n\u2212\nwhere \u03b1 4.31107 is the unique solution on [2, ) of the equation \u03b1ln((2e/\u03b1)) = 1 and\n\u2248 \u221e\n\u03b2= 3 . Furthermore,thevarianceoftheheightisconstant.\n2ln(\u03b1/2)\n144\n7.RandomBinarySearchTrees 7.3.DiscussionandExercises\nThe name Treap was coined by Aragon and Seidel [60] who discussed Treaps\nand some of their variants. However, their basic structure was studied much earlier by\nVuillemin[67]whocalledthemCartesiantrees.\nOne space-optimization of the Treap data structure that is sometimes performed\nis the elimination of the explicit storage of the priority p in each node. Instead, the pri-\nority of a node, u, is computed by hashing u\u2019s address in memory. Although a number of\nhash functions will probably work well for this in practice, for the important parts of the\nproof of Lemma 7.1 to remain valid, the hash function should be randomized and have\nthemin-wiseindependentproperty: Foranydistinctvaluesx ,...,x ,eachofthehashvalues\n1 k\nh(x ),...,h(x )shouldbedistinctwithhighprobabilityand,foreachi 1,...,k ,\n1 k\n\u2208{ }\nPr h(x )=min h(x ),...,h(x ) c/k\ni 1 k\n{ { }}\u2264\nforsomeconstantc. Onesuchclassofhashfunctionsthatiseasytoimplementandfairly\nfastistabulationhashing(Section5.2.3).\nAnotherTreapvariantthatdoesn\u2019tstoreprioritiesateachnodeistherandomized\nbinarysearch tree of Mart\u00b4\u0131nez and Roura [46]. In this variant, every node, u, stores the\nsize, u.size, of the subtree rooted at u. Both the add(x) and remove(x) algorithms are\nrandomized. Thealgorithmforaddingxtothesubtreerootedatudoesthefollowing:\n1. Withprobability1/(size(u)+1),xisaddedtheusualway,asaleaf,androtationsare\nthendonetobringxuptotherootofthissubtree.\n2. Otherwise, x is recursively added into one of the two subtrees rooted at u.left or\nu.right,asappropriate.\nThe first case corresponds to an add(x) operation in a Treap where x\u2019s node receives a\nrandom priority that is smaller than any of the size(u) priorities in u\u2019s subtree, and this\ncaseoccurswithexactlythesameprobability.\nRemovingavaluexfromarandomizedbinarysearchtreeissimilartotheprocess\nofremovingfromaTreap. Wefindthenode,u,thatcontainsxandthenperformrotations\nthatrepeatedlyincreasethedepthofuuntilitbecomesaleaf,atwhichpointwecansplice\nit from the tree. The choice of whether to perform a left or right rotation at each step is\nrandomized.\n1. With probability u.left.size/(u.size 1), we perform a right rotation at u, making\n\u2212\nu.lefttherootofthesubtreethatwasformerlyrootedatu.\n145\n7.RandomBinarySearchTrees 7.3.DiscussionandExercises\n2. With probability u.right.size/(u.size 1), we perform a left rotation at u, making\n\u2212\nu.righttherootofthesubtreethatwasformerlyrootedatu.\nAgain, we can easily verify that these are exactly the same probabilities that the removal\nalgorithminaTreapwillperformaleftorrightrotationofu.\nRandomized binary search trees have the disadvantage, compared to treaps, that\nwhen adding and removing elements they make many random choices and they must\nmaintain the sizes of subtrees. One advantage of randomized binary search trees over\ntreapsisthatsubtreesizescanserveanotherusefulpurpose,namelytoprovideaccessby\nrank in O(logn) expected time (see Exercise 7.10). In comparison, the random priorities\nstoredintreapnodeshavenouseotherthankeepingthetreapbalanced.\nExercise7.1. Illustratetheadditionof4.5(withpriority7)andthen7.5(withpriority20)\nontheTreapinFigure7.5.\nExercise7.2. Illustratetheremovalof5andthen7ontheTreapinFigure7.5.\nExercise 7.3. Prove the assertion that there are 21,964,800 sequences that generate the\ntree on the right hand side of Figure 7.1. (Hint: Give a recursive formula for the number\nofsequencesthatgenerateacompletebinarytreeofheighthandevaluatethisformulafor\nh=3.)\nExercise7.4. Design and implement the permute(a) method that takes as input an array,\na,containingndistinctvaluesandrandomlypermutesa. ThemethodshouldruninO(n)\ntimeandyoushouldprovethateachofthen!possiblepermutationsofaisequallyproba-\nble.\nExercise7.5. UsebothpartsofLemma7.2toprovethattheexpectednumberofrotations\nperformedbyanadd(x)operation(andhencealsoaremove(x)operation)isO(1).\nExercise 7.6. Modify the Treap implementation given here so that it does not explicitly\nstorepriorities. Instead,itshouldsimulatethembyhashingthehashCode()ofeachnode.\nExercise7.7. Supposethatabinarysearchtreestores,ateachnode,u,theheight,u.height,\nofthesubtreerootedatu,andthesize,u.sizeofthesubtreerootedatu.\n1. Show how, if we perform a left or right rotation at u, then these two quantities can\nbeupdated,inconstanttime,forallnodesaffectedbytherotation.\n2. Explainwhythesameresultisnotpossibleifwetrytoalsostorethedepth,u.depth,\nofeachnodeu.\n146\n7.RandomBinarySearchTrees 7.3.DiscussionandExercises\nExercise7.8. DesignanimplementanalgorithmthatconstructsaTreapfromasortedar-\nray,a,ofnelements. ThismethodshouldruninO(n)worst-casetimeandshouldconstruct\naTreapthatisindistinguishablefromoneinwhichtheelementsofawereaddedoneata\ntimeusingtheadd(x)method.\nExercise7.9. ThisexerciseworksoutthedetailsofhowonecanefficientlysearchaTreap\ngivenapointerthatisclosetothenodewearesearchingfor.\n1. Design and implement a Treap implementation in which each node keeps track of\ntheminimumandmaximumvaluesinitssubtree.\n2. Usingthisextrainformation,addafingerFind(x,u)methodthatexecutesthefind(x)\noperation with the help of a pointer to the node u (which is hopefully not far from\nthe node that contains x). This operation should start at u and walk upwards until\nit reaches a node w such that w.min x w.max. From that point onwards, it should\n\u2264 \u2264\nperformastandardsearchforxstartingfromw. (OnecanshowthatfingerFind(x,u)\ntakesO(1+logr)time,wherer isthenumberofelementsinthetreapwhosevalueis\nbetweenxandu.x.)\n3. Extendyourimplementationintoaversionofatreapthatstartsallitsfind(x)oper-\nationsfromthenodemostrecentlyfoundbyfind(x).\nExercise7.10. DesignandimplementaversionofaTreapthatincludesaget(i)operation\nthatreturnsthekeywithrankiintheTreap. (Hint: Haveeachnode,u,keeptrackofthe\nsizeofthesubtreerootedatu.)\nExercise7.11. ImplementaTreapList,animplementationoftheListinterfaceasatreap.\nEachnodeinthetreapshouldstorealistitem,andanin-ordertraversalofthetreapfinds\nthe items in the same order that they occur in the list. All the List operations get(i),\nset(i,x),add(i,x)andremove(i)shouldruninO(logn)expectedtime.\nExercise 7.12. Design and implement a version of a Treap that supports the split(x)\noperation. This operation removes all values from the Treap that are greater than x and\nreturnsasecondTreapthatcontainsalltheremovedvalues.\nExample: thecodet2=t.split(x)removesfromtallvaluesgreaterthanxandreturnsa\nnew Treap t2 containing all these values. The split(x) operation should run in O(logn)\nexpectedtime.\nWarning: For thismodification towork properlyand stillallow thesize() methodto run\ninconstanttime,itisnecessarytoimplementthemodificationsinExercise7.10.\n147\n7.RandomBinarySearchTrees 7.3.DiscussionandExercises\nExercise 7.13. Design and implement a version of a Treap that supports the absorb(t2)\noperation, which can be thought of as the inverse of the split(x) operation. This opera-\ntion removes all values from the Treap t2 and adds them to the receiver. This operation\npresupposes that the smallest value in t2 is greater than the largest value in the receiver.\nTheabsorb(t2)operationshouldruninO(logn)expectedtime.\nExercise7.14. ImplementMartinez\u2019srandomizedbinarysearchtrees,asdiscussedinthis\nsection. Compare theperformance of yourimplementation withthat of theTreapimple-\nmentation.\n148\nChapter 8\nScapegoat Trees\nIn this chapter, we study a binary search tree data structure, the ScapegoatTree. This\nstructure is based on the common wisdom that, when something goes wrong, the first\nthing we should do is find someone to blame it on (the scapegoat). Once blame is firmly\nestablished,wecanleavethescapegoattofixtheproblem.\nA ScapegoatTree keeps itself balanced by partial rebuilding operations. During a\npartialrebuildingoperation,anentiresubtreeisdeconstructedandrebuiltintoaperfectly\nbalanced subtree. There are many ways of rebuilding a subtree rooted at node u into\na perfectly balanced tree. One of the simplest is to traverse u\u2019s subtree, gathering all its\nnodesintoanarrayaandthentorecursivelybuildabalancedsubtreeusinga. Ifweletm=\na.length/2,thentheelementa[m]becomestherootofthenewsubtree,a[0],...,a[m 1]get\n\u2212\nstoredrecursivelyintheleftsubtreeanda[m+1],...,a[a.length 1]getstoredrecursively\n\u2212\nintherightsubtree.\nScapegoatTree\nvoid rebuild(Node *u) {\nint ns = BinaryTree<Node>::size(u);\nNode *p = u->parent;\nNode **a = new Node*[ns];\npackIntoArray(u, a, 0);\nif (p == nil) {\nr = buildBalanced(a, 0, ns);\nr->parent = nil;\n} else if (p->right == u) {\np->right = buildBalanced(a, 0, ns);\np->right->parent = p;\n} else {\np->left = buildBalanced(a, 0, ns);\np->left->parent = p;\n}\ndelete[] a;\n149\n8.ScapegoatTrees 8.1.ScapegoatTree: ABinarySearchTreewithPartialRebuilding\n}\nint packIntoArray(Node *u, Node **a, int i) {\nif (u == nil) {\nreturn i;\n}\ni = packIntoArray(u->left, a, i);\na[i++] = u;\nreturn packIntoArray(u->right, a, i);\n}\nAcalltorebuild(u)takesO(size(u))time. Thesubtreebuiltbyrebuild(u)hasminimum\nheight;thereisnotreeofsmallerheightthathassize(u)nodes.\n8.1 ScapegoatTree: A Binary Search Tree with Partial Rebuilding\nAScapegoatTreeisaBinarySearchTreethat,inadditiontokeepingtrackofthenumber,\nn, of nodes in the tree also keeps a counter, q, that maintains an upper-bound on the\nnumberofnodes.\nScapegoatTree\nint q;\nAtalltimes,nandqobeythefollowinginequalities:\nq/2 n q .\n\u2264 \u2264\nInaddition,aScapegoatTreehaslogarithmicheight;atalltimes,theheightofthescape-\ngoattreedoesnotexceed:\nlog q log 2n<log n+2 . (8.1)\n3/2 \u2264 3/2 3/2\nEvenwiththisconstraint,aScapegoatTreecanlooksurprisinglyunbalanced. Thetreein\nFigure8.1hasq=n=10andheight5<log 10 5.679.\n3/2 \u2248\nImplementing the find(x) operation in a ScapegoatTree is done using the stan-\ndard algorithm for searching in a BinarySearchTree (see Section 6.2). This takes time\nproportionaltotheheightofthetreewhich,by(8.1)isO(logn).\nTo implement the add(x) operation, we first increment n and q and then use the\nusual algorithm for adding x to a binary search tree; we search for x and then add a new\nleaf u with u.x = x. At this point, we may get lucky and the depth of u might not exceed\nlog q. Ifso,thenweleavewellenoughaloneanddon\u2019tdoanythingelse.\n3/2\nUnfortunately, it will sometimes happen that depth(u) > log q. In this case we\n3/2\nneed to do something to reduce the height. This isn\u2019t a big job; there is only one node,\n150\n8.ScapegoatTrees 8.1.ScapegoatTree: ABinarySearchTreewithPartialRebuilding\n7\n6 8\n5 9\n2\n1 4\n0 3\nFigure8.1: AScapegoatTreewith10nodesandheight5.\nnamely u, whose depth exceeds log q. To fix u, we walk from u back up to the root\n3/2\nlookingforascapegoat,w. Thescapegoat,w,isaveryunbalancednode. Ithastheproperty\nthat\nsize(w.child) 2\n> , (8.2)\nsize(w) 3\nwherew.childisthechildofwonthepathfromtheroottou. We\u2019llveryshortlyprovethat\na scapegoat exists. For now, we can take it for granted. Once we\u2019ve found the scapegoat\nw, we completely destroy the subtree rooted at w and rebuild it into a perfectly balanced\nbinarysearchtree. Weknow,from(8.2),that,evenbeforetheadditionofu,w\u2019ssubtreewas\nnotacompletebinarytree. Therefore,whenwerebuildw,theheightdecreasesbyatleast\n1sothatheightoftheScapegoatTreeisonceagainatmostlog q.\n3/2\nScapegoatTree\nbool add(T x) {\n// first do basic insertion keeping track of depth\nNode *u = new Node;\nu->x = x;\nu->left = u->right = u->parent = nil;\nint d = addWithDepth(u);\nif (d > log32(q)) {\n// depth exceeded, find scapegoat\nNode *w = u->parent;\nint a = BinaryTree<Node>::size(w);\nint b = BinaryTree<Node>::size(w->parent);\nwhile (3*a <= 2*b) {\nw = w->parent;\n151\n8.ScapegoatTrees 8.1.ScapegoatTree: ABinarySearchTreewithPartialRebuilding\n7 7\n6 8 6 8\n5 6 > 2 9 3 9\n7 3\n2 3 1 4\n6\n1 4 2 0 2 3.5 5\n3\n0 3 1\n2\n3.5\nFigure 8.2: Inserting 3.5 into a ScapegoatTree increases its height to 6, which violates\n(8.1)since6>log 11 5.914. Ascapegoatisfoundatthenodecontaining5.\n3/2 \u2248\na = BinaryTree<Node>::size(w);\nb = BinaryTree<Node>::size(w->parent);\n}\nrebuild(w->parent);\n}\nreturn d >= 0;\n}\nIfweignorethecostoffindingthescapegoatwandrebuildingthesubtreerootedat\nw,thentherunningtimeofadd(x)isdominatedbytheinitialsearch,whichtakesO(logq)=\nO(logn) time. We will account for the cost of finding the scapegoat and rebuilding using\namortizedanalysisinthenextsection.\nTheimplementationofremove(x)inaScapegoatTreeisverysimple. Wesearchfor\nxandremoveitusingtheusualalgorithmforremovinganodefromaBinarySearchTree.\n(Notethatthiscanneverincreasetheheightofthetree.) Next,wedecrementnbutleaveq\nunchanged. Finally, we check if q>2n and, if so, we rebuild the entire tree into a perfectly\nbalancedbinarysearchtreeandsetq=n.\nScapegoatTree\nbool remove(T x) {\nif (BinarySearchTree<Node,T>::remove(x)) {\nif (2*n < q) {\nrebuild(r);\nq = n;\n152\n8.ScapegoatTrees 8.1.ScapegoatTree: ABinarySearchTreewithPartialRebuilding\n}\nreturn true;\n}\nreturn false;\n}\nAgain, if we ignore the cost of rebuilding, the running time of the remove(x) operation is\nproportionaltotheheightofthetree,andisthereforeO(logn).\n8.1.1 AnalysisofCorrectnessandRunning-Time\nIn this section we analyze the correctness and amortized running time of operations on a\nScapegoatTree. Wefirstprovethecorrectnessbyshowingthat,whentheadd(x)operation\nresultsinanodethatviolatesCondition(8.1),thenwecanalwaysfindascapegoat:\nLemma8.1. Letubeanodeofdepthh>log qinaScapegoatTree. Thenthereexistsanode\n3/2\nwonthepathfromutotherootsuchthat\nsize(w)\n>2/3 .\nsize(parent(w))\nProof. Suppose,forthesakeofcontradiction,thatthisisnotthecase,and\nsize(w)\n2/3 .\nsize(parent(w)) \u2264\nfor all nodes w on the path from u to the root. Denote the path from the root to u as\nr = u ,...,u = u. Then, we have size(u ) = n, size(u ) 2n, size(u ) 4n and, more\n0 h 0 1 \u2264 3 2 \u2264 9\ngenerally,\n2 i\nsize(u ) n .\ni \u2264 3\n(cid:18) (cid:19)\nButthisgivesacontradiction,sincesize(u) 1,hence\n\u2265\n2 h 2 log 3/2 q 2 log 3/2 n 1\n1 size(u) n< n n= n=1 .\n\u2264 \u2264 3 3 \u2264 3 n\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nNext,weanalyzethepartsoftherunningtimethatwehavenotyetaccountedfor.\nTherearetwoparts: Thecostofcallstosize(u)whensearchforscapegoatnodes,andthe\ncostofcallstorebuild(w)whenwefindascapegoatw. Thecostofcallstosize(u)canbe\nrelatedtothecostofcallstorebuild(w),asfollows:\nLemma 8.2. During a call to add(x) in a ScapegoatTree, the cost of finding the scapegoat w\nandrebuildingthesubtreerootedatwisO(size(w)).\n153\n8.ScapegoatTrees 8.1.ScapegoatTree: ABinarySearchTreewithPartialRebuilding\nProof. The cost of rebuilding the scapegoat node w, once we find it, is O(size(w)). When\nsearching for the scapegoat node, we call size(u) on a sequence of nodes u ,...,u until\n0 k\nwe find the scapegoat u =w. However, since u is the first node in this sequence that is a\nk k\nscapegoat,weknowthat\n2\nsize(u )< size(u )\ni 3 i+1\nforalli 0,...,k 2 . Therefore,thecostofallcallstosize(u)is\n\u2208{ \u2212 }\nk k 1\n\u2212\nO size(u ) = O size(u )+ size(u )\nk i k k i 1\n\uf8eb \u2212 \uf8f6 \uf8eb \u2212 \u2212 \uf8f6\ni=0 i=0\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed (cid:88) \uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n= O\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edsize(u\n)+\n(cid:88)\nk \u2212 1 2 i\nsize(u\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n)\nk 3 k\n\uf8eb \uf8f6\n= O\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edsize(u\n) 1\n(cid:88) i\n+\n=0\nk\n(cid:18)\n\u2212 1\n(cid:19)\n2 i\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nk 3\n\uf8eb \uf8eb \uf8f6\uf8f6\n= O(\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nsize(u\nk\n))\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n=O(\n(cid:88) i\ns\n=\ni\n0\nz\n(cid:18)\ne(\n(cid:19)\nw)\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n)\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\nwherethelastlinefollowsfromthefactthatthesumisageometricallydecreasingseries.\nAll that remains is to prove an upper-bound on the cost of all calls to rebuild(u)\nduringasequenceofmoperations:\nLemma8.3. StartingwithanemptyScapegoatTreeanysequenceofmadd(x)andremove(x)\noperationscausesatmostO(mlogm)timetobeusedbyrebuild(u)operations.\nProof. To prove this, we will use a credit scheme. We imagine that each node stores a\nnumberofcredits. Eachcreditcanpayforsomeconstant,c,unitsoftimespentrebuilding.\nThe scheme gives out a total of O(mlogm) credits and every call to rebuild(u) is paid for\nwithcreditsstoredatu.\nDuring an insertion or deletion, we give one credit to each node on the path to\nthe inserted node, or deleted node, u. In this way we hand out at most log q log m\n3/2 \u2264 3/2\ncreditsperoperation. Duringadeletionwealsostoreanadditional1credit\u201contheside.\u201d\nThus,intotalwegiveoutatmostO(mlogm)credits. Allthatremainsistoshowthatthese\ncreditsaresufficienttopayforallcallstorebuild(u).\nIf we call rebuild(u) during an insertion, it is because u is a scapegoat. Suppose,\nwithoutlossofgenerality,that\nsize(u.left) 2\n> .\nsize(u) 3\n154\n8.ScapegoatTrees 8.2.DiscussionandExercises\nUsingthefactthat\nsize(u)=1+size(u.left)+size(u.right)\nwededucethat\n1\nsize(u.left)>size(u.right)\n2\nandtherefore\n1 1\nsize(u.left) size(u.right)> size(u.left)> size(u) .\n\u2212 2 3\nNow,thelasttimeasubtreecontaininguwasrebuilt(orwhenuwasinserted,ifasubtree\ncontaininguwasneverrebuilt),wehad\nsize(u.left) size(u.right) 1 .\n\u2212 \u2264\nTherefore, the number of add(x) or remove(x) operations that have affected u.left or\nu.rightsincethenisatleast\n1\nsize(u) 1 .\n3 \u2212\nand there are therefore at least this many credits stored at u that are available to pay for\ntheO(size(u))timeittakestocallrebuild(u).\nIf we call rebuild(u) during a deletion, it is because q>2n. In this case, we have\nq n>n credits stored \u201con the side\u201d and we use these to pay for the O(n) time it takes to\n\u2212\nrebuildtheroot. Thiscompletestheproof.\n8.1.2 Summary\nThefollowingtheoremsummarizestheperformanceoftheScapegoatTreedatastructure:\nTheorem8.1. AScapegoatTreeimplementstheSSetinterface. Ignoringthecostofrebuild(u)\noperations,aScapegoatTreesupportstheoperationsadd(x),remove(x),andfind(x)inO(logn)\ntimeperoperation.\nFurthermore,beginningwithanemptyScapegoatTree,anysequenceofmadd(x)and\nremove(x)operationsresultsinatotalofO(mlogm)timespentduringallcallstorebuild(u).\n8.2 Discussion and Exercises\nThe term scapegoat tree is due to Galperin and Rivest [30], who define and analyze these\ntrees. However,thesamestructurewasdiscoveredearlierbyAndersson[4,6],whocalled\nthemgeneralbalancedtreessincetheycanhaveanyshapeaslongastheirheightissmall.\n155\n8.ScapegoatTrees 8.2.DiscussionandExercises\nExperimenting with the ScapegoatTree implementation will reveal that it is of-\nten considerably slower than the other SSet implementations in this book. This may be\nsomewhatsurprising,sinceheightboundof\nlog q 1.709logn+O(1)\n3/2 \u2248\nisbetterthantheexpectedlengthofasearchpathinaSkiplistandnottoofarfromthat\nof a Treap. The implementation could be optimized by storing the sizes of subtrees ex-\nplicitlyateachnodeorbyreusingalreadycomputedsubtreesizes(Exercises8.5and8.6).\nEven with these optimizations, there will always be sequences of add(x) and delete(x)\noperationforwhichaScapegoatTreetakeslongerthanotherSSetimplementations.\nThis gap in performance is due to the fact that, unlike the other SSet implemen-\ntations discussed in this book, a ScapegoatTree can spend a lot of time restructuring\nitself. Exercise 8.3 asks you to prove that there are sequences of n operations in which a\nScapegoatTreewillspendontheorderofnlogntimeincallstorebuild(u). Thisisincon-\ntrasttootherSSetimplementationsdiscussedinthisbookthatonlymakeO(n)structural\nchangesduringasequenceofnoperations. Thisis,unfortunately,anecessaryconsequence\nofthefactthataScapegoatTreedoesallitsrestructuringbycallstorebuild(u)[17].\nDespitetheirlackofperformance,thereareapplicationsinwhichaScapegoatTree\ncould be the right choice. This would occur any time there is additional data associated\nwithnodesthatcannotbeupdatedinconstanttimewhenarotationisperformed,butthat\ncan be updated during a rebuild(u) operation. In such cases, the ScapegoatTree and re-\nlatedstructuresbasedonpartialrebuildingmaywork. Anexampleofsuchanapplication\nisoutlinedinExercise8.12.\nExercise8.1. Illustrate the addition of the values 1.5 and then 1.6 on the ScapegoatTree\ninFigure8.1.\nExercise 8.2. Illustrate what happens when we start with an empty ScapegoatTree and\naddthesequence1,5,2,4,3andshowwherethecreditsdescribedintheproofofLemma8.3\ngo,andhowtheyareusedduringthissequenceofadditions.\nExercise 8.3. Show that, if we start with an empty ScapegoatTree and call add(x) for\nx = 1,2,3,...,n, then the total time spent during calls to rebuild(u) is at least cnlogn for\nsomeconstantc>0.\nExercise8.4. TheScapegoatTree,asdescribedinthischapter,guaranteesthatthelength\nofthesearchpathdoesnotexceedlog q.\n3/2\n156\n8.ScapegoatTrees 8.2.DiscussionandExercises\n1. Design, analyze, and implement a modified version of ScapegoatTree where the\nlengthofthesearchpathdoesnotexceedlog q,wherebisaparameterwith1<b<\nb\n2.\n2. Whatdoesyouranalysisand/orexperimentssayabouttheamortizedcostoffind(x),\nadd(x)andremove(x)asafunctionofnandb?\nExercise8.5. Modifytheadd(x)methodoftheScapegoatTreesothatitdoesnotwasteany\ntimerecomputingthesizesofsubtreesthatithasalreadycomputedthesizeof. Thisispos-\nsiblebecause,bythetimethemethodwantstocomputesize(w),ithasalreadycomputed\nsize(w.left) or size(w.right). Compare the performance of your modified implementa-\ntionwiththeimplementationgivenhere.\nExercise 8.6. Implement a second version of the ScapegoatTree data structure that ex-\nplicitly stores and maintains the sizes of the subtree rooted at each node. Compare the\nperformanceoftheresultingimplementationwiththatoftheoriginalScapegoatTreeim-\nplementationaswellastheimplementationfromExercise8.5.\nExercise 8.7. Reimplement the rebuild(u) method discussed at the beginning of this\nchapter so that it does not require the use of an array to store the nodes of the subtree\nbeing rebuilt. Instead, it should use recursion to first connect the nodes into a linked list\nand then convert this linked list into a perfectly balanced binary tree. (There are very\nelegantrecursiveimplementationsofbothsteps.)\nExercise8.8. AnalyzeandimplementaWeightBalancedTree. Thisisatreeinwhicheach\nnodeu,excepttheroot,maintainsthebalanceinvariantthatsize(u) (2/3)size(u.parent).\n\u2264\nTheadd(x)andremove(x)operationsareidenticaltothestandardBinarySearchTreeoper-\nations,exceptthatanytimethebalanceinvariantisviolatedatanodeu,thesubtreerooted\natu.parentisrebuilt. YouranalysisshouldshowthatoperationsonaWeightBalancedTree\nruninO(logn)amortizedtime.\nExercise8.9. AnalyzeandimplementaWeightBalancedTree. Thisisatreeinwhicheach\nnodeu,excepttheroot,maintainsthebalanceinvariantthatsize(u) (2/3)size(u.parent).\n\u2264\nTheadd(x)andremove(x)operationsareidenticaltothestandardBinarySearchTreeop-\nerations, except that any time the balance invariant is violated at a node u, rotations are\nperformedinordertorestoretheseinvariants. Youranalysisshouldshowthatoperations\nonaWeightBalancedTreeruninO(logn)worst-casetime.\n157\n8.ScapegoatTrees 8.2.DiscussionandExercises\nExercise8.10. Analyze and implement a CountdownTree. In a CountdownTree each node\nukeepsatimeru.t. Theadd(x)andremove(x)operationsareexactlythesameasinastan-\ndardBinarySearchTreeexceptthat,wheneveroneoftheseoperationsaffectsu\u2019ssubtree,\nu.t is decremented. When u.t=0 the entire subtree rooted at u is rebuilt into a perfectly\nbalanced binary search tree. When a node u is involved in a rebuilding operation (either\nbecauseuisrebuiltoroneofu\u2019sancestorsisrebuilt)u.tisresettosize(u)/3.\nYour analysis should show that operations on a CountdownTree run in O(logn)\namortized time. (Hint: First show that each node u satisfies some version of a balance\ninvariant.)\nExercise 8.11. Analyze and implement a DynamiteTree. In a DynamiteTree each node u\nkeeps tracks of the size of the subtree rooted at u in a variable u.size. The add(x) and\nremove(x)operationsareexactlythesameasinastandardBinarySearchTreeexceptthat,\nwhenever one of these operations affects a node u\u2019s subtree, u explodes with probability\n1/u.size. When u explodes, its entire subtree is rebuilt into a perfectly balanced binary\nsearchtree.\nYour analysis should show that operations on a DynamiteTree run in O(logn) ex-\npectedtime.\nExercise8.12. DesignandimplementaSequencedatastructurethatmaintainsasequence\n(list)ofelements. Itsupportstheseoperations:\n\u2022 addAfter(e): Add a new element after the element e in the sequence. Return the\nnewlyaddedelement. (Ifeisnull,thenewelementisaddedatthebeginningofthe\nsequence.)\n\u2022 remove(e): Removeefromthesequence.\n\u2022 testBefore(e1,e2): returntrueifandonlyife1comesbeforee2inthesequence.\nThefirsttwooperationsshouldruninO(logn)amortizedtime. Thethirdoperationshould\nruninconstant-time.\nTheSequencedatastructurecanbeimplementedbystoringtheelementsinsome-\nthing like aScapegoatTree, in the same orderthat they occur in the sequence. Toimple-\nment testBefore(e1,e2) in constant time, each element e is labelled with an integer that\nencodes the path from the root to e. In this way, testBefore(e1,e2) can be implemented\njustbycomparingthelabelsofe1ande2.\n158\nChapter 9\nRed-Black Trees\nInthischapter,wepresentred-blacktrees,aversionofbinarysearchtreesthathaveloga-\nrithmicdepth. Red-blacktreesareoneofthemostwidely-useddatastructuresinpractice.\nThey appear as the primary search structure in many library implementations, including\nthe Java Collections Framework and several implementations of the C++ Standard Tem-\nplate Library. They are also used within the Linux operating system kernel. There are\nseveralreasonsforthepopularityofred-blacktrees:\n1. Ared-blacktreestoringnvalueshasheightatmost2logn.\n2. The add(x) and remove(x) operations on a red-black tree run in O(logn) worst-case\ntime.\n3. Theamortizednumberofrotationsdoneduringanadd(x)orremove(x)operationis\nconstant.\nThefirsttwoofthesepropertiesalreadyputred-blacktreesaheadofskiplists,treaps,and\nscapegoat trees. Skiplists and treaps rely on randomization and their O(logn) running\ntimes are only expected. Scapegoat trees have a guaranteed bound on their height, but\nadd(x)andremove(x)onlyruninO(logn)amortizedtime. Thethirdpropertyisjusticing\nonthecake. Ittellsusthatthatthetimeneededtoaddorremoveanelementxisdwarfed\nbythetimeittakestofindx.1\nHowever,thenicepropertiesofred-blacktreescomewithaprice: implementation\ncomplexity. Maintaining a bound of 2logn on the height is not easy. It requires a careful\nanalysisofanumberofcasesanditrequiresthattheimplementationdoesexactlytheright\n1Notethatskiplistsandtreapsalsohavethispropertyintheexpectedsense. SeeExercise4.6andExer-\ncise7.5.\n159\n9.Red-BlackTrees 9.1.2-4Trees\nFigure9.1: A2-4treeofheight3.\nthing in each case. One misplaced rotation or change of color produces a bug that can be\nverydifficulttounderstandandtrackdown.\nRather than jumping directly into the implementation of red-black trees, we will\nfirst provide some background on a related data structure: 2-4 trees. This will give some\ninsight into how red-black trees were discovered and why efficiently maintaining red-\nblacktreesisevenpossible.\n9.1 2-4 Trees\nA2-4treeisarootedtreewiththefollowingproperties:\nProperty9.1(height). Allleaveshavethesamedepth.\nProperty9.2(degree). Everyinternalnodehas2,3,or4children.\nAnexampleofa2-4treeisshowninFigure9.1. Thepropertiesof2-4treesimply\nthattheirheightislogarithmicinthenumberofleaves:\nLemma9.1. A2-4treewithnleaveshasheightatmostlogn.\nProof. Thelower-boundof2onthenumberofchildrenofaninternalnodeimpliesthat,if\ntheheightofa2-4treeish,thenithasatleast2h leaves. Inotherwords,\nn 2h .\n\u2265\nTakinglogarithmsonbothsidesofthisinequalitygivesh logn.\n\u2264\n9.1.1 AddingaLeaf\nAdding a leaf to a 2-4 tree is easy (see Figure 9.2). If we want to add a leaf u as the child\nof some node w on the second-last level, we simply make u a child of w. This certainly\n160\n9.Red-BlackTrees 9.1.2-4Trees\nw\nw\nu\nw w\n0\nu\nFigure9.2: Addingaleaftoa2-4Tree. Thisprocessstopsafteronesplitbecausew.parent\nhasdegreelessthan4beforetheaddition.\nmaintains the height property, but could violate the degree property; if w had 4 children\nprior to adding u, then w now has 5 children. In this case, we split w into two nodes, w\nand w\u2019, having 2 and 3 children, respectively. But now w\u2019 has no parent, so we recursively\nmake w\u2019 a child of w\u2019s parent. Again, this may cause w\u2019s parent to have too many children\ninwhichcasewesplitit. Thisprocessgoesonuntilwereachanodethathasfewerthan4\nchildren, oruntilwesplittheroot, r, intotwonodesrandr . Inthelattercase, wemake\n(cid:48)\na new root that has r and r as children. This simultaneously increases the depth of all\n(cid:48)\nleavesandsomaintainstheheightproperty.\nSince the height of the 2-4 tree is never more than logn, the process of adding a\nleaffinishesafteratmostlognsteps.\n161\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\n9.1.2 RemovingaLeaf\nRemoving a leaf from a 2-4 tree is a little more tricky (see Figure 9.3). To remove a leaf u\nfrom its parent w, we just remove it. If w had only two children prior to the removal of u,\nthenwisleftwithonlyonechildandviolatesthedegreeproperty.\nTocorrectthis,welookatw\u2019ssibling,w . Thenodew issuretoexistsincew\u2019sparent\n(cid:48) (cid:48)\nhasatleast2children. Ifw has3or4children,thenwetakeoneofthesechildrenfromw\n(cid:48) (cid:48)\nandgiveittow. Nowwhas2childrenandw has2or3childrenandwearedone.\n(cid:48)\nOntheotherhand,ifw hasonlytwochildren,thenwemergewandw intoasingle\n(cid:48) (cid:48)\nnode, w, that has 3 children. Next we recursively remove w from the parent of w . This\n(cid:48) (cid:48)\nprocessendswhenwereachanode, u, whereuoritssiblinghasmorethan2children; or\nwereachtheroot. Inthelattercase,iftherootisleftwithonly1child,thenwedeletethe\nroot and make its child the new root. Again, this simultaneously decreases the height of\neveryleafandthereforemaintainstheheightproperty.\nAgain,sincetheheightofthetreeisnevermorethanlogn,theprocessofremoving\naleaffinishesafteratmostlognsteps.\n9.2 RedBlackTree: A Simulated 2-4 Tree\nA red-black tree is a binary search tree in which each node, u, has a color which is either\nredorblack. Redisrepresentedbythevalue0andblackbythevalue1.\nRedBlackTree\nclass RedBlackNode : public BSTNode<Node, T> {\nfriend class RedBlackTree<Node, T>;\nchar color;\n};\nint red = 0;\nint black = 1;\nBeforeandafteranyoperationonared-blacktree,thefollowingtwopropertiesare\nsatisfied. Eachpropertyisdefinedbothintermsofthecolorsredandblack,andinterms\nofthenumericvalues0and1.\nProperty 9.3 (black-height). There are the same number of black nodes on every root to\nleafpath. (Thesumofthecolorsonanyroottoleafpathisthesame.)\nProperty 9.4 (no-red-edge). No two red nodes are adjacent. (For any node u, except the\nroot,u.color+u.parent.color 1.)\n\u2265\n162\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nu\nFigure 9.3: Removing a leaf from a 2-4 Tree. This process goes all the way to the root\nbecauseallofu\u2019sancestorsandtheirsiblingshavedegree2.\n163\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nFigure 9.4: An example of a red-black tree with black-height 3. External (nil) nodes are\ndrawnassquares.\nNotice that we can always color the root, r, of a red-black tree black without vi-\nolating either of these two properties, so we will assume that the root is black, and the\nalgorithms for updating a red-black tree will maintain this. Another trick that simplifies\nred-blacktreesistotreattheexternalnodes(representedbynil)asblacknodes. Thisway,\nevery real node, u, of a red-black tree has exactly two children, each with a well-defined\ncolor. Anexampleofared-blacktreeisshowninFigure9.4.\n9.2.1 Red-BlackTreesand2-4Trees\nAtfirstitmightseemsurprisingthatared-blacktreecanbeefficientlyupdatedtomaintain\ntheblack-heightandno-red-edgeproperties, anditseemsunusualtoevenconsiderthese\nasusefulproperties. However,red-blacktreesweredesignedtobeanefficientsimulation\nof2-4treesasbinarytrees.\nRefertoFigure9.5. Consideranyred-blacktree,T,havingnnodesandperformthe\nfollowing transformation: Remove each red node u and connect u\u2019s two children directly\ntothe(black)parentofu. AfterthistransformationweareleftwithatreeT havingonly\n(cid:48)\nblacknodes.\nEveryinternalnodeinT has2,3,or4children: Ablacknodethatstartedoutwith\n(cid:48)\ntwo black children will still have two black children after this transformation. A black\nnode that started out with one red and one black child will have three children after this\ntransformation. A black node that started out with two red children will have 4 children\nafter this transformation. Furthermore, the black-height property now guarantees that\neveryroot-to-leafpathinT hasthesamelength. Inotherwords,T isa2-4tree!\n(cid:48) (cid:48)\nThe 2-4 tree T has n+1 leaves that correspond to the n+1 external nodes of the\n(cid:48)\nred-blacktree. Therefore,thistreehasheightlog(n+1). Now,everyroottoleafpathinthe\n164\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nFigure9.5: Everyred-blacktreehasacorresponding2-4tree.\n2-4 tree corresponds to a path from the root of the red-black tree T to an external node.\nThe first and last node in this path are black and at most one out of every two internal\nnodes is red, so this path has at most log(n+1) black nodes and at most log(n+1) 1 red\n\u2212\nnodes. Therefore,thelongestpathfromtheroottoanyinternalnodeinT isatmost\n2log(n+1) 2 2logn ,\n\u2212 \u2264\nforanyn 1. Thisprovesthemostimportantpropertyofred-blacktrees:\n\u2265\nLemma9.2. Theheightofred-blacktreewithnnodesisatmost2logn.\nNow that we have seen the relationship between 2-4 trees and red-black trees, it\nis not hard to believe that we can efficiently maintain a red-black tree while adding and\nremovingelements.\nWehavealreadyseenthataddinganelementinaBinarySearchTreecanbedone\nbyaddinganewleaf. Therefore,toimplementadd(x)inared-blacktreeweneedamethod\nof simulating splitting a degree 5 node in a 2-4 tree. A degree 5 node is represented by a\nblacknodethathastworedchildrenoneofwhichalsohasaredchild. Wecan\u201csplit\u201dthis\nnode by coloring it red and coloring its two children black. An example of this is shown\ninFigure9.6.\nSimilarly, implementing remove(x) requires a method of merging two nodes and\nborrowing a child from a sibling. Merging two nodes is the inverse of a split (shown in\n165\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nw\nw\nu\nw w\n0\nu\nFigure 9.6: Simulating a 2-4 tree split operation during an addition in a red-black tree.\n(Thissimulatesthe2-4treeadditionshowninFigure9.2.)\n166\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nFigure 9.6), and involves coloring two (black) siblings red and coloring their (red) parent\nblack. Borrowing from a sibling is the most complicated of the procedures and involves\nbothrotationsandrecoloringofnodes.\nOf course, during all of this we must still maintain the no-red-edge property and\ntheblack-heightproperty. Whileitisnolongersurprisingthatthiscanbedone,thereare\na large number of cases that have to be considered if we try to do a direct simulation of\na 2-4 tree by a red-black tree. At some point, it just becomes simpler to forget about the\nunderlying2-4treeandworkdirectlytowardsmaintainingthered-blacktreeproperties.\n9.2.2 Left-LeaningRed-BlackTrees\nThere is no single definition of a red-black tree. Rather, there are a family of structures\nthat manage to maintain the black-height and no-red-edge properties during add(x) and\nremove(x) operations. Different structures go about it in different ways. Here, we imple-\nmentadatastructurethatwecallaRedBlackTree. Thisstructureimplementsaparticular\nvariantofred-blacktreesthatsatisfiesanadditionalproperty:\nProperty9.5(left-leaning). Atanynodeu,ifu.leftisblack,thenu.rightisblack.\nNote that the red-black tree shown in Figure 9.4 does not satisfy the left-leaning\nproperty;itisviolatedbytheparentoftherednodeintherightmostpath.\nThereasonformaintainingtheleft-leaningpropertyisthatitreducesthenumber\nof cases encountered when updating the tree during add(x) and remove(x) operations. In\nterms of 2-4 trees, it implies that every 2-4 tree has a unique representation: A node of\ndegree2becomesablacknodewith2blackchildren. Anodeofdegree3becomesablack\nnodewhoseleftchildisredandwhoserightchildisblack. Anodeofdegree4becomesa\nblacknodewithtworedchildren.\nBefore we describe the implementation of add(x) and remove(x) in detail, we first\npresentsomesimplesubroutinesusedbythesemethodsthatareillustratedinFigure9.7.\nThe first two subroutines are for manipulating colors while preserving the black-height\nproperty. The pushBlack(u) method takes as input a black node u that has two red chil-\ndren and colors u red and its two children black. The pullBlack(u) method reverses this\noperation:\nRedBlackTree\nvoid pushBlack(Node *u) {\nu->color--;\nu->left->color++;\nu->right->color++;\n167\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nu u u u\npushBlack(u) pullBlack(u) flipLeft(u) flipRight(u)\n\u21d3 \u21d3 \u21d3 \u21d3\nu u\nu u\nFigure9.7: Flips,pullsandpushes\n}\nvoid pullBlack(Node *u) {\nu->color++;\nu->left->color--;\nu->right->color--;\n}\nThe flipLeft(u) method swaps the colors of u and u.right and then performs a\nleft rotation at u. This reverses the colors of these two nodes as well as their parent-child\nrelationship:\nRedBlackTree\nvoid flipLeft(Node *u) {\nswapColors(u, u->right);\nrotateLeft(u);\n}\nThe flipLeft(u) operation is especially useful in restoring the left-leaning property at a\nnodeu thatviolatesit(becauseu.leftisblackandu.rightisred). Inthisspecialcase,we\ncanbeassuredthisoperationpreservesboththeblack-heightandno-red-edgeproperties.\nThe flipRight(u) operation is symmetric to flipLeft(u) with the roles of left and right\nreversed.\nRedBlackTree\nvoid flipRight(Node *u) {\nswapColors(u, u->left);\nrotateRight(u);\n}\n168\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\n9.2.3 Addition\nToimplementadd(x)inaRedBlackTree,weperformastandardBinarySearchTreeinser-\ntion,whichaddsanewleaf,u,withu.x=xandsetu.color=red. Notethatthisdoesnot\nchange the black height of any node, so it does not violate the black-height property. It\nmay, however, violate the left-leaning property (if u is the right child of its parent) and it\nmayviolatetheno-red-edgeproperty(ifu\u2019sparentisred). Torestoretheseproperties,we\ncallthemethodaddFixup(u).\nRedBlackTree\nbool add(T x) {\nNode *u = new Node();\nu->left = u->right = u->parent = nil;\nu->x = x;\nu->color = red;\nbool added = BinarySearchTree<Node,T>::add(u);\nif (added)\naddFixup(u);\nreturn added;\n}\nThe addFixup(u) method, illustrated in Figure 9.8, takes as input a node u whose\ncolor is red and which may be violating the no-red-edge property and/or the left-leaning\nproperty. The following discussion is probably impossible to follow without referring to\nFigure 9.8 or recreating it on a piece of paper. Indeed, the reader may wish to study this\nfigurebeforecontinuing.\nIfuistherootofthetree, thenwecancolorublackandthisrestoresbothproper-\nties. If u\u2019s sibling is also red, then u\u2019s parent must be black, so both the left-leaning and\nno-red-edgepropertiesalreadyhold.\nOtherwise, we first determine if u\u2019s parent, w, violates the left-leaning property\nand,ifso,performaflipLeft(w)operationandsetu=w. Thisleavesusinawell-defined\nstate: uistheleftchildofitsparent,w,sownowsatisfiestheleft-leaningproperty. Allthat\nremains is to ensure the no-red-edge property at u. We only have to worry about the case\nwherewisred,sinceotherwiseualreadysatisfiestheno-red-edgeproperty.\nSince we are not done yet, u is red and w is red. The no-red-edge property (which\nisonlyviolatedbyuandnotbyw)impliesthatu\u2019sgrandparentgexistsandisblack. Ifg\u2019s\nrightchildisred,thentheleft-leaningpropertyensuresthatbothg\u2019schildrenarered,and\nacalltopushBlack(g)makesgredandwblack. Thisrestorestheno-red-edgepropertyat\nu,butmaycauseittobeviolatedatg,sothewholeprocessstartsoverwithu=g.\n169\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nu\nu.parent.left.color\nw w w\nu u u\nreturn flipLeft(w);u=w\nw\nu\nw.color\nw w\nu u\ng.right.color return\ng g g\nw w w\nu u u\nflipRight(g) pushBlack(g) pushBlack(g)\nw newu newu\nu w w\nu u\nreturn\nFigure9.8: AsingleroundintheprocessoffixingProperty2afteraninsertion.\n170\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nIf g\u2019s right child is black, then a call to flipRight(g) makes w the (black) parent\nof g and gives w two red children, u and g. This ensures that u satisfies the no-red-edge\npropertyandgsatisfiestheleft-leaningproperty. Inthiscasewecanstop.\nRedBlackTree\nvoid addFixup(Node *u) {\nwhile (u->color == red) {\nif (u == r) { // u is the root - done\nu->color = black;\nreturn;\n}\nNode *w = u->parent;\nif (w->left->color == black) { // ensure left-leaning\nflipLeft(w);\nu = w;\nw = u->parent;\n}\nif (w->color == black)\nreturn; // no red-red edge = done\nNode *g = w->parent; // grandparent of u\nif (g->right->color == black) {\nflipRight(g);\nreturn;\n} else {\npushBlack(g);\nu = g;\n}\n}\n}\nThe insertFixup(u) method takes constant time per iteration and each iteration\neitherfinishesormovesuclosertotheroot. ThisimpliesthattheinsertFixup(u)method\nfinishesafterO(logn)iterationsinO(logn)time.\n9.2.4 Removal\nThe remove(x) operation in a RedBlackTree is the most complicated operation to imple-\nment, and this is true of all known implementations. Like remove(x) in a BinarySearch-\nTree, this operation boils down to finding a node w with only one child, u, and splicing w\noutofthetreebyhavingw.parentadoptu.\nTheproblemwiththisisthat,ifwisblack,thentheblack-heightpropertywillnow\nbe violated at w.parent. We get around this problem, temporarily, by adding w.color to\nu.color. Ofcourse,thisintroducestwootherproblems: (1)uandwbothstartedoutblack,\n171\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nthenu.color+w.color=2(doubleblack),whichisaninvalidcolor. Ifwwasred,thenitis\nreplacedbyablacknodeu,whichmayviolatetheleft-leaningpropertyatu.parent. Both\noftheseproblemsareresolvedwithacalltotheremoveFixup(u)method.\nRedBlackTree\nbool remove(T x) {\nNode *u = findLast(x);\nif (u == nil || compare(u->x, x) != 0)\nreturn false;\nNode *w = u->right;\nif (w == nil) {\nw = u;\nu = w->left;\n} else {\nwhile (w->left != nil)\nw = w->left;\nu->x = w->x;\nu = w->right;\n}\nsplice(w);\nu->color += w->color;\nu->parent = w->parent;\ndelete w;\nremoveFixup(u);\nreturn true;\n}\nThe removeFixup(u) method takes as input a node u whose color is black (1) or\ndouble-black(2). Ifuisdouble-black, thenremoveFixup(u)performsaseriesofrotations\nand recoloring operations that move the double-black node up the tree until it can be\ngotten rid of. During this process, the node u changes until, at the end of this process, u\nrefers to the root of the subtree that has been changed. The root of this subtree may have\nchanged color. In particular, it may have gone from red to black, so the removeFixup(u)\nmethodfinishesbycheckingifu\u2019sparentviolatestheleft-leaningpropertyand,ifso,fixes\nit.\nRedBlackTree\nvoid removeFixup(Node *u) {\nwhile (u->color > black) {\nif (u == r) {\nu->color = black;\n} else if (u->parent->left->color == red) {\nu = removeFixupCase1(u);\n} else if (u == u->parent->left) {\n172\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nu = removeFixupCase2(u);\n} else {\nu = removeFixupCase3(u);\n}\n}\nif (u != r) { // restore left-leaning property, if necessary\nNode *w = u->parent;\nif (w->right->color == red && w->left->color == black) {\nflipLeft(w);\n}\n}\n}\nThe removeFixup(u) method is illustrated in Figure 9.9. Again, the following text\nwillbeverydifficult,ifnotimpossible,tofollowwithoutreferringconstantlytoFigure9.9.\nEach iteration of the loop in removeFixup(u) processes the double-black node u based on\noneoffourcases.\nCase0: uistheroot. Thisistheeasiestcasetotreat. Werecolorutobeblackandthisdoes\nnotviolateanyofthered-blacktreeproperties.\nCase1: u\u2019ssibling,v,isred. Inthiscase,u\u2019ssiblingistheleftchildofitsparent,w(bythe\nleft-leaningproperty). Weperformaright-flipatwandthenproceedtothenextiteration.\nNotethatthiscausesw\u2019sparenttoviolatetheleft-leaningpropertyanditcausesthedepth\nof u to increase. However, it also implies that the next iteration will be in Case 3 with w\ncolored red. When examining Case 3, below, we will see that this means the process will\nstopduringthenextiteration.\nRedBlackTree\nNode* removeFixupCase1(Node *u) {\nflipRight(u->parent);\nreturn u;\n}\nCase 2: u\u2019s sibling, v, is black and u is the left child of its parent, w. In this case, we call\npullBlack(w), making u black, v red, and darkening the color of w to black or double-\nblack. Atthispoint,wdoesnotsatisfytheleft-leaningproperty,sowecallflipLeft(w)to\nfixthis.\nAt this point, w is red and v is the root of the subtree we started with. We need to\ncheck if w causes no-red-edge property to be violated. We do this by inspecting w\u2019s right\nchild,q. Ifqisblack,thenwsatisfiestheno-red-edgepropertyandwecancontinuetothe\nnextiterationwithu=v.\n173\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nremoveFixupCase2(u) removeFixupCase3(u) removeFixupCase1(u)\nw w w\nu v v u u\npullBlack(w) pullBlack(w)\nflipRight(w)\nw w\nu v v u\nflipLeft(w) flipRight(w) w\nnewu\nv v\nw w\nu q\nq.color q.color\nv v(newu) v v\nw w w\nu q q q u q u\nv.left.color\nrotateLeft(w) rotateRight(w)\nv\nv v w w\nq q q u q u\nw w flipLeft(v) pushBlack(v)\nu u\nflipRight(v) flipLeft(v) w(newu)\nv u w\nq q q u\nw v v w\nu u\npushBlack(q) pushBlack(q)\nq q\nw v v w\nu u\nv.right.color\nq q\nw v w v\nu u\nflipLeft(v)\nq\nw\nv\nu\nFigure 9.9: A single round in the process of eliminating a double-black node after a re-\nmoval.\n174\n9.Red-BlackTrees 9.2.RedBlackTree: ASimulated2-4Tree\nOtherwise (q is red), both the no-red-edge property and the left-leaning property\nare violated at q and w, respectively. A call to rotateLeft(w) restores the left-leaning\nproperty, but the no-red-edge property is still violated. At this point, q is the left child\nof v and w is the left child of q, q and w are both red and v is black or double-black. A\nflipRight(v) makes q the parent of both v and w. Following this up by a pushBlack(q)\nmakesbothvandwblackandsetsthecolorofqbacktotheoriginalcolorofw.\nAt this point, there is no more double-black node and the no-red-edge and black-\nheight properties are reestablished. The only possible problem that remains is that the\nright child of v may be red, in which case the left-leaning property is violated. We check\nthisandperformaflipLeft(v)tocorrectitifnecessary.\nRedBlackTree\nNode* removeFixupCase2(Node *u) {\nNode *w = u->parent;\nNode *v = w->right;\npullBlack(w); // w->left\nflipLeft(w); // w is now red\nNode *q = w->right;\nif (q->color == red) { // q-w is red-red\nrotateLeft(w);\nflipRight(v);\npushBlack(q);\nif (v->right->color == red)\nflipLeft(v);\nreturn q;\n} else {\nreturn v;\n}\n}\nCase3: u\u2019ssiblingisblackanduistherightchildofitsparent,w. Thiscaseissymmetricto\nCase 2 and is handled mostly the same way. The only differences come from the fact that\ntheleft-leaningpropertyisasymmetric,soitrequiresdifferenthandling.\nAsbefore,webeginwithacalltopullBlack(w),whichmakesvredandublack. A\ncall to flipRight(w) promotes v to the root of the subtree. At this point w is red, and the\ncodebranchestwowaysdependingonthecolorofw\u2019sleftchild,q.\nIf q is red, then the code finishes up exactly the same way that Case 2 finishes up,\nbutisevensimplersincethereisnodangerofvnotsatisfyingtheleft-leaningproperty.\nThe more complicated case occurs when q is black. In this case, we examine the\ncolor of v\u2019s left child. If it is red, then v has two red children and its extra black can be\n175\n9.Red-BlackTrees 9.3.Summary\npushed down with a call to pushBlack(v). At this point, v now has w\u2019s original color and\nwearedone.\nIfv\u2019sleftchildisblackthenvviolatestheleft-leaningpropertyandwerestorethis\nwithacalltoflipLeft(v). ThenextiterationofremoveFixup(u)thencontinueswithu=v.\nRedBlackTree\nNode* removeFixupCase3(Node *u) {\nNode *w = u->parent;\nNode *v = w->left;\npullBlack(w);\nflipRight(w); // w is now red\nNode *q = w->left;\nif (q->color == red) { // q-w is red-red\nrotateRight(w);\nflipLeft(v);\npushBlack(q);\nreturn q;\n} else {\nif (v->left->color == red) {\npushBlack(v); // both v\u2019s children are red\nreturn v;\n} else { // ensure left-leaning\nflipLeft(v);\nreturn w;\n}\n}\n}\nEach iteration of removeFixup(u) takes constant time. Cases 2 and 3 either finish\normoveuclosertotherootofthetree. Case0(whereuistheroot)alwaysterminatesand\nCase 1 leads immediately to Case 3, which also terminates. Since the height of the tree is\natmost2logn,weconcludethatthereareatmostO(logn)iterationsofremoveFixup(u)so\nremoveFixup(u)runsinO(logn)time.\n9.3 Summary\nThefollowingtheoremsummarizestheperformanceoftheRedBlackTreedatastructure:\nTheorem9.1. ARedBlackTreeimplementstheSSetinterface. ARedBlackTreesupportsthe\noperationsadd(x),remove(x),andfind(x)inO(logn)worst-casetimeperoperation.\nNotincludedintheabovetheoremistheextrabonus\n176\n9.Red-BlackTrees 9.3.Summary\nTheorem9.2. BeginningwithanemptyRedBlackTree,anysequenceofmadd(x)andremove(x)\noperationsresultsinatotalofO(m)timespentduringallcallsaddFixup(u)andremoveFixup(u).\nWewillonlysketchaproofofTheorem9.2. BycomparingaddFixup(u)andremoveFixup(u)\nwiththealgorithmsforaddingorremovingaleafina2-4tree,wecanconvinceourselves\nthat this property is something that is inherited from a 2-4 tree. In particular, if we can\nshowthatthetotaltimespentsplitting,merging,andborrowingina2-4treeisO(m),then\nthisimpliesTheorem9.2.\nThe proof of this for 2-4 trees uses the potential method of amortized analysis.2\nDefinethepotentialofaninternalnodeuina2-4treeas\n1 ifuhas2children\n\u03a6(u)=\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2 0\n3\ni\ni\nf\nf\nu\nu\nh\nh\na\na\ns\ns\n3\n4\nc\nc\nh\nh\ni\ni\nl\nl\nd\nd\nr\nr\ne\ne\nn\nn\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nandthepotentialofa2-4treeasthesumofthepotentialsofitsnodes. Whenasplitoccurs,\nit is because a node of degree 4 becomes two nodes, one of degree 2 and one of degree 3.\nThis means that the overall potential drops by 3 1 0 = 2. When a merge occurs, two\n\u2212 \u2212\nnodesthatusedtohavedegree2arereplacedbyonenodeofdegree3. Theresultisadrop\ninpotentialof2 0=2. Therefore,foreverysplitormerge,thepotentialdecreasesby2.\n\u2212\nNextnoticethat,ifweignoresplittingandmergingofnodes,thereareonlyacon-\nstantnumberofnodeswhosenumberofchildrenischangedbytheadditionorremovalof\naleaf. Whenaddinganode,onenodehasitsnumberofchildrenincreaseby1,increasing\nthe potential by at most 3. During the removal of a leaf, one node has its number of chil-\ndrendecreaseby1,increasingthepotentialbyatmost1, andtwonodesmaybeinvolved\ninaborrowingoperation,increasingtheirtotalpotentialbyatmost1.\nTo summarize, each merge and split causes the potential to drop by at least 2.\nIgnoring merging and splitting, each addition or removal causes the potential to rise by\nat most 3, and the potential is always non-negative. Therefore, the number of splits and\nmerges caused by m additions or removals on an initially empty tree is at most 3m/2.\nTheorem 9.2 is a consequence of this analysis and the correspondence between 2-4 trees\nandred-blacktrees.\n2SeetheproofsofLemma2.2andLemma3.1forotherapplicationsofthepotentialmethod.\n177\n9.Red-BlackTrees 9.4.DiscussionandExercises\n9.4 Discussion and Exercises\nRed-black trees were first introduced by Guibas and Sedgewick [34]. Despite their high\nimplementation complexity they are found in some of the most commonly used libraries\nand applications. Most algorithms and data structures textbooks discuss some variant of\nred-blacktrees.\nAndersson [5] describes a left-leaning version of balanced trees that are similar to\nred-blacktreesbuthavetheadditionalconstraintthatanynodehasatmostoneredchild.\nThisimpliesthatthesetreessimulate2-3treesratherthan2-4trees. Theyaresignificantly\nsimpler,though,thantheRedBlackTreestructurepresentedinthischapter.\nSedgewick[59]describesatleasttwoversionsofleft-leaningred-blacktrees. These\nuserecursionalongwithasimulationoftop-downsplittingandmergingin2-4trees. The\ncombinationofthesetwotechniquesmakesforparticularlyshortandelegantcode.\nA related, and older, data structure is the AVL tree [3]. AVL trees are height-\nbalanced: Ateachnodeu,theheightofthesubtreerootedatu.leftandthesubtreerooted\natu.rightdifferbyatmostone. Itfollowsimmediatelythat,ifF(h)istheminimumnum-\nberofleavesinatreeofheighth,thenF(h)obeystheFibonaccirecurrence\nF(h)=F(h 1)+F(h 2)\n\u2212 \u2212\nwith base cases F(0) = 1 and F(1) = 1. This means F(h) is approximately \u03d5h/\u221a5, where\n\u03d5 = (1+\u221a5)/2 1.61803399 is the golden ratio. (More precisely, \u03d5h/\u221a5 F(h) 1/2.)\n\u2248 | \u2212 | \u2264\nArguingasintheproofofLemma9.1,thisimplies\nh log n 1.440420088logn ,\n\u2264 \u03d5 \u2248\nso AVL trees have smaller height than red-black trees. The height-balanced property can\nbemaintainedduringadd(x)andremove(x)operationsbywalkingbackupthepathtothe\nroot and performing a rebalancing operation at each node u where the height of u\u2019s left\nandrightsubtreesdifferby2. SeeFigure9.10.\nAndersson\u2019svariantofred-blacktrees, Sedgewick\u2019svariantofred-blacktrees, and\nAVL trees are all simpler to implement than the RedBlackTree structure defined here.\nUnfortunately, none of them can guarantee that the amortized time spent rebalancing is\nO(1)perupdate. Inparticular,thereisnoanalogueofTheorem9.2forthosestructures.\nExercise9.1. Illustratethe2-4treethatcorrespondstotheRedBlackTreeinFigure9.11.\nExercise9.2. Illustratetheadditionof13, then3.5, then3.3ontheRedBlackTreeinFig-\nure9.11.\n178\n9.Red-BlackTrees 9.4.DiscussionandExercises\nh\nh+2\nh\nh+2\nh+1\nFigure 9.10: Rebalancing in an AVL tree. At most 2 rotations are required to convert a\nnodewhosesubtreeshaveheighthandh+2intoanodewhosesubtreeseachhaveheight\natmosth+1.\n6\n4 10\n2 5 8 12\n1 3 7 9 11\nFigure9.11: Ared-blacktreetopracticeon.\n179\n9.Red-BlackTrees 9.4.DiscussionandExercises\nExercise 9.3. Illustrate the removal of 11, then 9, then 5 on the RedBlackTree in Fig-\nure9.11.\nExercise 9.4. Show that, for arbitrarily large values of n, there are red-black trees with n\nnodesthathaveheight2logn O(1).\n\u2212\nExercise 9.5. Show that, for arbitrarily large values of n, there exist sequences of add(x)\nandremove(x)operationsthatleadtored-blacktreeswithnnodesthathaveheight2logn\n\u2212\nO(1).\nExercise9.6. Why does the method remove(x) in the RedBlackTree implementation per-\nform the assignment u.parent=w.parent? Shouldn\u2019t this already be done by the call to\nsplice(w)?\nExercise9.7. Supposea2-4tree,T,hasn leavesandn internalnodes.\n(cid:96) i\n1. Whatistheminimumvalueofn ,asafunctionofn ?\ni (cid:96)\n2. Whatisthemaximumvalueofn ,asafunctionofn ?\ni (cid:96)\n3. IfT isared-blacktreethatrepresentsT,thenhowmanyrednodesdoesT have?\n(cid:48) (cid:48)\nExercise9.8. Supposeyouaregivenabinarysearchtreewithnnodesandheightatmost\n2logn 2. Is it always possible to color the nodes red and black so that the tree satisfies\n\u2212\nthe black-height and no-red-edge properties? If so, can it also be made to satisfy the left-\nleaningproperty?\nExercise 9.9. Suppose you have two red-black trees T and T that have the same black\n1 2\nheight, h, and that the largest key in T is smaller than the smallest key in T . Show how\n1 2\ntomergeT andT inintoasinglered-blacktreeinO(h)time.\n1 2\nExercise9.10. ExtendyoursolutiontoExercise9.9tothecasewherethetwotreesT and\n1\nT havedifferentblackheights,h (cid:44) h . Therunning-timeshouldbeO(max h ,h ).\n2 1 2 1 2\n{ }\nExercise9.11. Prove that, during an add(x) operation, an AVL tree must perform at most\none rebalancing operation (that involves at most 2 rotations; see Figure 9.10). Give an\nexample of an AVL tree and a remove(x) operation on that tree that requires on the order\noflognrebalancingoperations.\nExercise9.12. ImplementanAVLTreeclassthatimplementsAVLtreesasdescribedabove.\nCompareitsperformancetothatoftheRedBlackTreeimplementation. Whichimplemen-\ntationhasafasterfind(x)operation?\n180\n9.Red-BlackTrees 9.4.DiscussionandExercises\nExercise 9.13. Design and implement a series of experiments that compare the relative\nperformanceoffind(x),add(x),andremove(x)forSkiplistSSet,ScapegoatTree,Treap,\nand RedBlackTree. Be sure to include multiple test scenarios, including cases where the\ndata is random, already sorted, is removed in random order, is removed in sorted order,\nandsoon.\n181\n9.Red-BlackTrees 9.4.DiscussionandExercises\n182\nChapter 10\nHeaps\nIn this chapter, we discuss two implementations of the extremely useful priority Queue\ndata structure. Both of these structures are a special kind of binary tree called a heap,\nwhich means \u201ca disorganized pile.\u201d This is in contrast to binary search trees that can\nthoughtofasahighlyorganizedpile.\nThefirstheapimplementationusesanarraytosimulateacompletebinarytree. It\nisveryfastandisthebasisofoneofthefastestknownsortingalgorithms,namelyheapsort\n(seeSection11.1.3). Thesecondimplementationisbasedonmoreflexiblebinarytrees. It\nsupports a meld(h) operation that allows the priority queue to absorb the elements of a\nsecondpriorityqueueh.\n10.1 BinaryHeap: An Implicit Binary Tree\nOur first implementation of a (priority) Queue is based on a technique that is over 400\nyearsold. Eytzinger\u2019smethodallowsustorepresentacompletebinarytreeasanarray. This\nis done by laying out the nodes of the tree in breadth-first order (see Section 6.1.2) in the\narray. In this way, the root is stored at position 0, the root\u2019s left child is stored at position\n1, the root\u2019s right child at position 2, the left child of the left child of the root is stored at\nposition3,andsoon. SeeFigure10.1.\nIf we do this for a large enough tree, some patterns emerge. The left child of the\nnodeatindexiisatindexleft(i)=2i+1andtherightchildofthenodeatindexiisat\nindexright(i)=2i+2. Theparentofthenodeatindexiisatindexparent(i)=(i 1)/2.\nBinaryHeap \u2212\nint left(int i) {\nreturn 2*i + 1;\n}\nint right(int i) {\nreturn 2*i + 2;\n}\n183\n10.Heaps 10.1.BinaryHeap: AnImplicitBinaryTree\n0\n1 2\n3 4 5 6\n7 8 9 10 11 12 13 14\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nFigure10.1: Eytzinger\u2019smethodrepresentsacompletebinarytreeasanarray.\nint parent(int i) {\nreturn (i-1)/2;\n}\nABinaryHeapusesthistechniquetoimplicitlyrepresentacompletebinarytreein\nwhich the elements are heap-ordered: The value stored at any index i is not smaller than\nthevalue storedat indexparent(i), with theexception ofthe rootvalue, i=0. Itfollows\nthatthesmallestvalueinthepriorityQueueisthereforestoredatposition0(theroot).\nInaBinaryHeap,thenelementsarestoredinanarraya:\nBinaryHeap\narray<T> a;\nint n;\nImplementing the add(x) operation is fairly straightforward. As with all array-\nbased structures, we first check if a is full (because a.length = n) and, if so, we grow a.\nNext, we place x at location a[n] and increment n. At this point, all that remains is to\nensurethatwemaintaintheheapproperty. Wedothisbyrepeatedlyswappingxwithits\nparentuntilxisnolongersmallerthanitsparent. SeeFigure10.2.\nBinaryHeap\nbool add(T x) {\nif (n + 1 > a.length) resize();\na[n++] = x;\nbubbleUp(n-1);\nreturn true;\n}\nvoid bubbleUp(int i) {\nint p = parent(i);\n184\n10.Heaps 10.1.BinaryHeap: AnImplicitBinaryTree\nwhile (i > 0 && compare(a[i], a[p]) < 0) {\na.swap(i,p);\ni = p;\np = parent(i);\n}\n}\nImplementing the remove() operation, which removes the smallest value from the\nheap,isalittletrickier. Weknowwherethesmallestvalueis(attheroot),butweneedto\nreplaceitafterweremoveitandensurethatwemaintaintheheapproperty.\nThe easiest way to do this is to replace the root with the value a[n 1], delete that\n\u2212\nvalue, and decrement n. Unfortunately, the new root element is now probably not the\nsmallest element, so it needs to be moved downwards. We do this by repeatedly compar-\ning this element to its two children. If it is the smallest of the three then we are done.\nOtherwise,weswapthiselementwiththesmallestofitstwochildrenandcontinue.\nBinaryHeap\nT remove() {\nT x = a[0];\na[0] = a[--n];\ntrickleDown(0);\nif (3*n < a.length) resize();\nreturn x;\n}\nvoid trickleDown(int i) {\ndo {\nint j = -1;\nint r = right(i);\nif (r < n && compare(a[r], a[i]) < 0) {\nint l = left(i);\nif (compare(a[l], a[r]) < 0) {\nj = l;\n} else {\nj = r;\n}\n} else {\nint l = left(i);\nif (l < n && compare(a[l], a[i]) < 0) {\nj = l;\n}\n}\nif (j >= 0) a.swap(i, j);\ni = j;\n185\n10.Heaps 10.1.BinaryHeap: AnImplicitBinaryTree\n4\n9 8\n17 26 50 16\n19 69 32 93 55\n4 9 8 17 26 50 16 19 69 32 93 55\n4\n9 8\n17 26 50 16\n19 69 32 93 55 6\n4 9 8 17 26 50 16 19 69 32 93 55 6\n4\n9 8\n17 26 6 16\n19 69 32 93 55 50\n4 9 8 17 26 6 16 19 69 32 93 55 50\n4\n9 6\n17 26 8 16\n19 69 32 93 55 50\n4 9 6 17 26 8 16 19 69 32 93 55 50\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nFigure10.2: Addingthevalue6toaBinaryHeap.\n186\n10.Heaps 10.2.MeldableHeap: ARandomizedMeldableHeap\n} while (i >= 0);\n}\nAs with other array-based structures, we will ignore the time spent in calls to\nresize(),sincethesecanbeaccountedforwiththeamortizationargumentfromLemma2.1.\nTherunningtimesofbothadd(x)andremove()thendependontheheightofthe(implicit)\nbinarytree. Luckily,thisisacompletebinarytree;everylevelexceptthelasthasthemaxi-\nmumpossiblenumberofnodes. Therefore,iftheheightofthistreeish,thenithasatleast\n2h nodes. Statedanotherway\nn 2h .\n\u2265\nTakinglogarithmsonbothsidesofthisequationgives\nh logn .\n\u2264\nTherefore,boththeadd(x)andremove()operationruninO(logn)time.\n10.1.1 Summary\nThefollowingtheoremsummarizestheperformanceofaBinaryHeap:\nTheorem10.1. ABinaryHeapimplementsthe(priority)Queueinterface. Ignoringthecostof\ncallstoresize(),aBinaryHeapsupportstheoperationsadd(x)andremove()inO(logn)time\nperoperation.\nFurthermore, beginning with an empty BinaryHeap, any sequence of m add(x) and\nremove()operationsresultsinatotalofO(m)timespentduringallcallstoresize().\n10.2 MeldableHeap: A Randomized Meldable Heap\nIn this section, we describe the MeldableHeap, a priority Queue implementation in which\ntheunderlyingstructureisalsoaheap-orderedbinarytree. However,unlikeaBinaryHeap\ninwhichtheunderlyingbinarytreeiscompletelydefinedbythenumberofelements,there\narenorestrictionsontheshapeofthebinarytreethatunderliesaMeldableHeap;anything\ngoes.\nThe add(x) and remove() operations in a MeldableHeap are implemented in terms\nofthemerge(h1,h2)operation. Thisoperationtakestwoheapnodesh1andh2andmerges\nthem, returning a heap node that is the root of a heap that contains all elements in the\nsubtreerootedath1andallelementsinthesubtreerootedath2.\nThenicethingaboutamerge(h1,h2)operationisthatitcanbedefinedrecursively.\nSeeFigure10.4. Ifeitherofh1orh2isnil,thenwearemergingwithanemptyset,sowe\n187\n10.Heaps 10.2.MeldableHeap: ARandomizedMeldableHeap\n4\n9 6\n17 26 8 16\n19 69 32 93 55 50\n4 9 6 17 26 8 16 19 69 32 93 55 50\n50\n9 6\n17 26 8 16\n19 69 32 93 55\n50 9 6 17 26 8 16 19 69 32 93 55\n6\n9 50\n17 26 8 16\n19 69 32 93 55\n6 9 50 17 26 8 16 19 69 32 93 55\n6\n9 8\n17 26 50 16\n19 69 32 93 55\n6 9 8 17 26 50 16 19 69 32 93 55\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nFigure10.3: Removingtheminimumvalue,4,fromaBinaryHeap.\n188\n10.Heaps 10.2.MeldableHeap: ARandomizedMeldableHeap\nreturn h2 or h1, respectively. Otherwise, assume h1.x h2.x since, if h1.x>h2.x, then we\n\u2264\ncan reverse the roles of h1 and h2. Then we know that the root of the merged heap will\ncontain h1.x and we can recursively merge h2 with h1.left or h1.right, as we wish. This\nis where randomization comes in, and we toss a coin to decide whether to merge h2 with\nh1.leftorh1.right:\nMeldableHeap\nNode* merge(Node *h1, Node *h2) {\nif (h1 == nil) return h2;\nif (h2 == nil) return h1;\nif (compare(h1->x, h2->x) > 0) return merge(h2, h1);\n// now we know h1->x <= h2->x\nif (rand() % 2) {\nh1->left = merge(h1->left, h2);\nif (h1->left != nil) h1->left->parent = h1;\n} else {\nh1->right = merge(h1->right, h2);\nif (h1->right != nil) h1->right->parent = h1;\n}\nreturn h1;\n}\nIn the next section, we show that merge(h1,h2) runs in O(logn) expected time,\nwherenisthetotalnumberofelementsinh1andh2.\nWith access to a merge(h1,h2) operation, the add(x) operation is easy. We create a\nnewnodeucontainingxandthenmergeuwiththerootofourheap:\nMeldableHeap\nbool add(T x) {\nNode *u = new Node();\nu->left = u->right = u->parent = nil;\nu->x = x;\nr = merge(u, r);\nr->parent = nil;\nn++;\nreturn true;\n}\nThistakesO(log(n+1))=O(logn)expectedtime.\nThe remove() operation is similarly easy. The node we want to remove is the root,\nsowejustmergeitstwochildrenandmaketheresulttheroot:\nMeldableHeap\nT remove() {\nT x = r->x;\n189\n10.Heaps 10.2.MeldableHeap: ARandomizedMeldableHeap\nh1 merge(h1,h2) h2\n4 19\n9 8 25 20\n17 26 50 16 28 89\n19 55 32 93 99\n\u21d3\n4\n9\nmerge(h1.right,h2)\n17 26\n8\n19\n19\n50 16\n25 20\n55\n28 89\n32 93 99\nFigure10.4: Mergingh1andh2isdonebymergingh2withoneofh1.leftorh1.right.\n190\n10.Heaps 10.2.MeldableHeap: ARandomizedMeldableHeap\nNode *tmp = r;\nr = merge(r->left, r->right);\ndelete tmp;\nif (r != nil) r->parent = nil;\nn--;\nreturn x;\n}\nAgain,thistakesO(logn)expectedtime.\nAdditionally, a MeldableHeap can implement many other operations in O(logn)\nexpectedtime,including:\n\u2022 remove(u): removethenodeu(anditskeyu.x)fromtheheap.\n\u2022 absorb(h): add all the elements of the MeldableHeap h to this heap, emptying h in\ntheprocess.\nEach of these operations can be implemented using a constant number of merge(h1,h2)\noperationsthateachtakeO(logn)time.\n10.2.1 Analysisofmerge(h1,h2)\nThe analysis of merge(h1,h2) is based on the analysis of a random walk in a binary tree.\nA random walk in a binary tree is a walk that starts at the root of the tree. At each step\ninthewalk, acoinistossedandthewalkproceedstotheleftorrightchildofthecurrent\nnodedependingontheresultofthiscointoss. Thewalkendswhenitfallsoffthetree(the\ncurrentnodebecomesnil).\nThefollowinglemmaissomewhatremarkablebecauseitdoesnotdependatallon\ntheshapeofthebinarytree:\nLemma 10.1. The expected length of a random walk in a binary tree with n nodes is at most\nlog(n+1).\nProof. The proof is by induction on n. In the base case, n = 0 and the walk has length\n0=log(n+1). Supposenowthattheresultistrueforallnon-negativeintegersn <n.\n(cid:48)\nLet n denote the size of the root\u2019s left subtree, so that n =n n 1 is the size of\n1 2 1\n\u2212 \u2212\ntheroot\u2019srightsubtree. Startingattheroot,thewalktakesonestepandthencontinuesin\na subtree of size n or continues in a subtree of size n . By our inductive hypothesis, the\n1 2\nexpectedlengthofthewalkisthen\n1 1\nE[W]=1+ log(n +1)+ log(n +1) ,\n1 2\n2 2\n191\n10.Heaps 10.2.MeldableHeap: ARandomizedMeldableHeap\nsinceeachofn andn arelessthann. Sincelogisaconcavefunction,E[W]ismaximized\n1 2\nwhen n = n = (n 1)/2. Therefore, the expected number of steps taken by the random\n1 2\n\u2212\nwalkis\n1 1\nE[W]=1+ log(n +1)+ log(n +1)\n1 2\n2 2\n1+log((n 1)/2+1)\n\u2264 \u2212\n=1+log((n+1)/2)\n=log(n+1) .\nWemakeaquickdigressiontonotethat,forreaderswhoknowalittleaboutinfor-\nmationtheory,theproofofLemma10.1canbestatedintermsofentropy.\nInformationTheoreticProofofLemma10.1. Letd denotethedepthoftheithexternalnode\ni\nand recall that a binary tree with n nodes has n+1 external nodes. The probability of the\nrandomwalkreachingtheithexternalnodeisexactlyp\ni\n=1/2d i,sotheexpectedlengthof\ntherandomwalkisgivenby\nn n n\nH = p d = p log 2d i = p log(1/p )\ni i i i i\ni=0 i=0 i=0\n(cid:88) (cid:88) (cid:16) (cid:17) (cid:88)\nThe right hand side of this equation is easily recognizable as the entropy of a probability\ndistributionovern+1elements. Abasicfactabouttheentropyofadistributionovern+1\nelementsisthatitdoesnotexceedlog(n+1),whichprovesthelemma.\nWith this result on random walks, we can now easily prove that the running time\nofthemerge(h1,h2)operationisO(logn).\nLemma10.2. Ifh1andh2aretherootsoftwoheapscontainingn andn nodes,respectively,\n1 2\nthentheexpectedrunningtimeofmerge(h1,h2)isatmostO(logn),wheren=n +n .\n1 2\nProof. Eachstepofthemergealgorithmtakesonestepofarandomwalk,eitherintheheap\nrooted at h1 or the heap rooted at h2 The algorithm terminates when either of these two\nrandomwalksfalloutofitscorrespondingtree(whenh1=nullorh2=null). Therefore,\ntheexpectednumberofstepsperformedbythemergealgorithmisatmost\nlog(n +1)+log(n +1) 2logn .\n1 2\n\u2264\n192\n10.Heaps 10.3.DiscussionandExercises\n10.2.2 Summary\nThefollowingtheoremsummarizestheperformanceofaMeldableHeap:\nTheorem10.2. AMeldableHeapimplementsthe(priority)Queueinterface. AMeldableHeap\nsupportstheoperationsadd(x)andremove()inO(logn)expectedtimeperoperation.\n10.3 Discussion and Exercises\nThe implicit representation of a complete binary tree as an array, or list, seems to have\nbeen first proposed by Eytzinger [24], as a representation for pedigree family trees. The\nBinaryHeapdatastructuredescribedherewasfirstintroducedbyWilliams[69].\nTherandomizedMeldableHeapdatastructuredescribedhereappearstohavefirst\nbeen proposed by Gambin and Malinowski [31]. Other meldable heap implementations\nexist, including leftist heaps [13, 43, Section 5.3.2], binomial heaps [66], Fibonacci heaps\n[29], pairing heaps [28], and skew heaps [63], although none of these are as simple as the\nMeldableHeapstructure.\nSome of the above structures also support a decreaseKey(u,y) operation in which\nthe value stored at node u is decreased to y. (It is a pre-condition that y u.x.) This\n\u2264\noperationcanbeimplementedinO(logn)timeinmostoftheabovestructuresbyremoving\nnodeuandaddingy. However,someofthesestructurescanimplementdecreaseKey(u,y)\nmore efficiently. In particular, decreaseKey(u,y) takes O(1) amortized time in Fibonacci\nheaps and O(loglogn) amortized time in a special version of pairing heaps [22]. This\nmore efficient decreaseKey(u,y) operation has applications in speeding up several graph\nalgorithmsincludingDijkstra\u2019sshortestpathalgorithm[29].\nExercise10.1. Illustratetheadditionofthevalues7andthen3totheBinaryHeapshown\nattheendofFigure10.2.\nExercise 10.2. Illustrate the removal of the next two values (6 and 8) on the BinaryHeap\nshownattheendofFigure10.3.\nExercise10.3. Implement the remove(i) method, that removes the value stored in a[i] in\naBinaryHeap. ThismethodshouldruninO(logn)time. Next,explainwhythismethodis\nnotlikelytobeuseful.\nExercise10.4. Ad-arytreeisageneralizationofabinarytreeinwhicheachinternalnode\nhas d children. Using Eytzinger\u2019s method it is also possible to represent complete d-ary\ntrees using arrays. Work out the equations that, given an index i, determine the index of\ni\u2019sparentandeachofi\u2019sd childreninthisrepresentation.\n193\n10.Heaps 10.3.DiscussionandExercises\nExercise10.5. UsingwhatyoulearnedinExercise10.4,designandimplementaDaryHeap,\nthe d-ary generalization of a BinaryHeap. Analyze the running times of operations on a\nDaryHeap and test the performance of your DaryHeap implementation against that of the\nBinaryHeapimplementationgivenhere.\nExercise10.6. Illustratetheadditionofthevalues17andthen82intheMeldableHeaph1\nshowninFigure10.4. Useacointosimulatearandombitwhenneeded.\nExercise10.7. Illustratetheremovalofthenexttwovalues(4and8)intheMeldableHeap\nh1showninFigure10.4. Useacointosimulatearandombitwhenneeded.\nExercise10.8. Implementtheremove(u)method,thatremovesthenodeufromaMeldableHeap.\nThismethodshouldruninO(logn)expectedtime.\nExercise 10.9. Show how, in a BinaryHeap or MeldableHeap, to find the second smallest\nvalueinconstanttime.\nExercise 10.10. Show how, in a BinaryHeap or MeldableHeap, to find the kth smallest\nvalueinO(klogk)time. (Hint: Usinganotherheapmighthelp.)\nExercise 10.11. Suppose you are given k sorted lists, of total length n. Show how, using\na heap, to merge these into a single sorted list in O(nlogk) time. (Hint: Starting with the\ncasek=2canbeinstructive.)\n194\nChapter 11\nSorting Algorithms\nThis chapter discusses algorithms for sorting a set of n items. This might seem like a\nstrangetopicforabookondatastructures,butthereareseveralgoodreasonsforincluding\nit here. The most obvious reason is that two of these sorting algorithms (quicksort and\nheap-sort) are intimately related to two of the data structures we have already studied\n(randombinarysearchtreesandheaps,respectively).\nThefirstpartofthischapterdiscussesalgorithmsthatsortusingonlycomparisons\nand presents three algorithms that run in O(nlogn) time. As it turns out, all three al-\ngorithms are asymptotically optimal; no algorithm that uses only comparisons can avoid\ndoingroughlynlogncomparisonsintheworst-caseandeventheaverage-case.\nBefore continuing, we should note that any of the SSet or priority Queue imple-\nmentations presented in previous chapters can also be used to obtain an O(nlogn) time\nsorting algorithm. For example, we can sort n items by performing n add(x) operations\nfollowed by n remove() operations on a BinaryHeap or MeldableHeap. Alternatively, we\ncanusenadd(x)operationsonanyofthebinarysearchtreedatastructuresandthenper-\nformanin-ordertraversal(Exercise6.8)toextracttheelementsinsortedorder. However,\ninbothcaseswegothroughalotofoverheadtobuildastructurethatisneverfullyused.\nSortingissuchanimportantproblemthatitisworthwhiledevelopingdirectmethodsthat\nareasfast,simple,andspace-efficientaspossible.\nThe second part of this chapter shows that, if we allow other operations besides\ncomparisons, then all bets are off. Indeed, by using array-indexing, it is possible to sort a\nsetofnintegersintherange 0,...,nc 1 inO(cn)time.\n{ \u2212 }\n195\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\n11.1 Comparison-Based Sorting\nInthissection,wepresentthreesortingalgorithms: merge-sort,quicksort,andheap-sort.\nAll these algorithms take an input array a and sort the elements of a into non-decreasing\norder in O(nlogn) (expected) time. These algorithms are all comparison-based. These\nalgorithms don\u2019t care what type of data is being sorted, the only operation they do on\nthe data is comparisons using the compare(a,b) method. Recall, from Section 1.1.4, that\ncompare(a,b)returnsanegativevalueifa<b,apositivevalueifa>b,andzeroifa=b.\n11.1.1 Merge-Sort\nThemerge-sortalgorithmisaclassicexampleofrecursivedivideandconquer: Ifthelength\nofaisatmost1,thenaisalreadysorted,sowedonothing. Otherwise,wesplitaintotwo\nhalves, a0=a[0],...,a[n/2 1] and a1=a[n/2],...,a[n 1]. We recursively sort a0 and a1,\n\u2212 \u2212\nandthenwemerge(thenowsorted)a0anda1togetourfullysortedarraya:\nAlgorithms\nvoid mergeSort(array<T> &a) {\nif (a.length <= 1) return;\narray<T> a0(0);\narray<T>::copyOfRange(a0, a, 0, a.length/2);\narray<T> a1(0);\narray<T>::copyOfRange(a1, a, a.length/2, a.length);\nmergeSort(a0);\nmergeSort(a1);\nmerge(a0, a1, a);\n}\nAnexampleisshowninFigure11.1.\nCompared to sorting, merging the two sorted arrays a0 and a1 is fairly easy. We\nadd elements to a one at a time. If a0 or a1 is empty we add the next elements from the\nother (non-empty) array. Otherwise, we take the minimum of the next element in a0 and\nthenextelementina1andaddittoa:\nAlgorithms\nvoid merge(array<T> &a0, array<T> &a1, array<T> &a) {\nint i0 = 0, i1 = 0;\nfor (int i = 0; i < a.length; i++) {\nif (i0 == a0.length)\na[i] = a1[i1++];\nelse if (i1 == a1.length)\na[i] = a0[i0++];\nelse if (compare(a0[i0], a1[i1]) < 0)\na[i] = a0[i0++];\n196\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\na 13 8 5 2 4 0 6 9 7 3 12 1 10 11\na0 13 8 5 2 4 0 6 9 7 3 12 1 10 11 a1\nmergeSort(a0,c) mergeSort(a1,c)\na0 0 2 4 5 6 8 13 1 3 7 9 10 11 12 a1\nmerge(a0,a1,a)\na 0 1 2 3 4 5 6 7 8 9 10 11 12 13\nFigure11.1: TheexecutionofmergeSort(a,c)\nelse\na[i] = a1[i1++];\n}\n}\nNotice that the merge(a0,a1,a,c) algorithm performs at most n 1 comparisons before\n\u2212\nrunningoutofelementsinoneofa0ora1.\nTo understand the running-time of merge-sort, it is easiest to think of it in terms\nof its recursion tree. Suppose for now that n is a power of 2, so that n=2logn, and logn is\nan integer. Refer to Figure 11.2. Merge-sort turns the problem of sorting n elements into\n2 problems, each of sorting n/2 elements. These two subproblem are then turned into 2\nproblemseach,foratotalof4subproblems,eachofsizen/4. These4subproblemsbecome\n8subproblems,eachofsizen/8,andsoon. Atthebottomofthisprocess,n/2subproblems,\neach of size 2, are converted into n problems, each of size 1. For each subproblem of size\nn/2i, the time spent merging and copying data is O(n/2i). Since there are 2i subproblems\nof size n/2i, the total time spent working on problems of size 2i, not counting recursive\ncalls,is\n2i O(n/2i)=O(n) .\n\u00d7\nTherefore,thetotalamountoftimetakenbymerge-sortis\nlogn\nO(n)=O(nlogn) .\ni=0\n(cid:88)\n197\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\nn =n\nn n =n\n2 2\nn + n + n + n =n\n4 4 4 4\nn + n + n + n + n + n + n + n =n\n8 8 8 8 8 8 8 8\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n2 + 2 + 2 + + 2 + 2 + 2 =n\n\u00b7\u00b7\u00b7\n1 + 1 + 1 + 1 + 1 + 1 + + 1 + 1 + 1 + 1 + 1 + 1 =n\n\u00b7\u00b7\u00b7\nFigure11.2: Themerge-sortrecursiontree.\nTheproofofthefollowingtheoremisbasedonthesameanalysisasabove,buthas\ntobealittlemorecarefultodealwiththecaseswherenisnotapowerof2.\nTheorem 11.1. The mergeSort(a,c) algorithm runs in O(nlogn) time and performs at most\nnlogncomparisons.\nProof. The proof is by induction on n. The base case, in which n = 1, is trivial; when\npresentedwithanarrayoflength0or1thealgorithmsimplyreturnswithoutperforming\nanycomparisons.\nMerging two sorted lists of total length n requires at most n 1 comparisons. Let\n\u2212\nC(n) denote the maximum number of comparisons performed by mergeSort(a,c) on an\narray a of length n. If n is even, then we apply the inductive hypothesis to the two sub-\nproblemsandobtain\nC(n) n 1+2C(n/2)\n\u2264 \u2212\nn 1+2((n/2)log(n/2))\n\u2264 \u2212\n=n 1+nlog(n/2)\n\u2212\n=n 1+nlogn n\n\u2212 \u2212\n<nlogn .\n198\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\nThecasewherenisoddisslightlymorecomplicated. Forthiscase,weusetwoinequalities,\nthatareeasytoverify:\nlog(x+1) log(x)+1 , (11.1)\n\u2264\nforallx 1and\n\u2265\nlog(x+1/2)+log(x 1/2) 2log(x) , (11.2)\n\u2212 \u2264\nfor all x 1/2. Inequality (11.1) comes from the fact that log(x)+1=log(2x) while (11.2)\n\u2265\nfollowsfromthefactthatlogisaconcavefunction. Withthesetoolsinhandwehave,for\noddn,\nC(n) n 1+C( n/2 )+C( n/2 )\n\u2264 \u2212 (cid:100) (cid:101) (cid:98) (cid:99)\nn 1+ n/2 log n/2 + n/2 log n/2\n\u2264 \u2212 (cid:100) (cid:101) (cid:100) (cid:101) (cid:98) (cid:99) (cid:98) (cid:99)\n=n 1+(n/2+1/2)log(n/2+1/2)+(n/2 1/2)log(n/2 1/2)\n\u2212 \u2212 \u2212\nn 1+nlog(n/2)+(1/2)(log(n/2+1/2) log(n/2 1/2))\n\u2264 \u2212 \u2212 \u2212\nn 1+nlog(n/2)+1/2\n\u2264 \u2212\n<n+nlog(n/2)\n=n+n(logn 1)\n\u2212\n=nlogn .\n11.1.2 Quicksort\nThe quicksort algorithm is another classic divide and conquer algorithm. Unlike merge-\nsort, which does merging after solving the two subproblems, quicksort does all its work\nupfront.\nQuicksortissimpletodescribe: Pickarandompivotelement,x,froma;partitiona\nintothesetofelementslessthanx,thesetofelementsequaltox,andthesetofelements\ngreater than x; and, finally, recursively sort the first and third sets in this partition. An\nexampleisshowninFigure11.3.\nAlgorithms\nvoid quickSort(array<T> &a) {\nquickSort(a, 0, a.length);\n}\nvoid quickSort(array<T> &a, int i, int n) {\nif (n <= 1) return;\nT x = a[i + rand()%n];\nint p = i-1, j = i, q = i+n;\n// a[i..p]<x, a[p+1..q-1]??x, a[q..i+n-1]>x\n199\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\nx\n13 8 5 2 4 0 6 9 7 3 12 1 10 11\n1 8 5 2 4 0 6 7 3 9 12 10 11 13\nquickSort(a,0,9) quickSort(a,10,4)\n0 1 2 3 4 5 6 7 8 9 10 11 12 13\n0 1 2 3 4 5 6 7 8 9 10 11 12 13\nFigure11.3: Anexampleexecutionof quickSort(a,0,14)\nwhile (j < q) {\nint comp = compare(a[j], x);\nif (comp < 0) { // move to beginning of array\na.swap(j++, ++p);\n} else if (comp > 0) {\na.swap(j, --q); // move to end of array\n} else {\nj++; // keep in the middle\n}\n}\n// a[i..p]<x, a[p+1..q-1]=x, a[q..i+n-1]>x\nquickSort(a, i, p-i+1);\nquickSort(a, q, n-(q-i));\n}\nAll of this is done in-place, so that instead of making copies of subarrays being sorted,\nthe quickSort(a,i,n,c) method only sorts the subarray a[i],...,a[i+n 1]. Initially, this\n\u2212\nmethodiscalledasquickSort(a,0,a.length,c).\nAttheheartofthequicksortalgorithmisthein-placepartitioningalgorithm. This\nalgorithm,withoutusinganyextraspace,swapselementsinaandcomputesindicespand\nqsothat\n<x if0 i p\n\u2264 \u2264\na[i]\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2 =\n>x\nx i\ni\nf\nf\np\nq\n<i\ni\n<q\nn 1\nThis partitioning, which is done by\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nthe while\n\u2264\nloop\n\u2264\nin\n\u2212\nthe code, works by iteratively in-\ncreasing p and decreasing q while maintaining the first and last of these conditions. At\n200\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\neachstep,theelementatpositionjiseithermovedtothefront,leftwhereitis,ormoved\nto the back. In the first two cases, j is incremented, while in the last case, j is not incre-\nmentedsincethenewelementatpositionjhasnotbeenprocessedyet.\nQuicksortisverycloselyrelatedtotherandombinarysearchtreesstudiedinSec-\ntion7.1. Infact,iftheinputtoquicksortconsistsofndistinctelements,thenthequicksort\nrecursion tree is a random binary search tree. To see this, recall that when constructing a\nrandombinarysearchtreethefirstthingwedoispickarandomelementxandmakeitthe\nroot of the tree. After this, every element will eventually be compared to x, with smaller\nelementsgoingintotheleftsubtreeandlargerelementsgoingintotherightsubtree.\nIn quicksort, we select a random element x and immediately compare everything\ntox,puttingthesmallerelementsatthebeginningofthearrayandlargerelementsatthe\nend of the array. Quicksort then recursively sorts the beginning of the array and the end\nof the array, while the random binary search tree recursively inserts smaller elements in\ntheleftsubtreeoftherootandlargerelementsintherightsubtreeoftheroot.\nTheabovecorrespondencebetweenrandombinarysearchtreesandquicksortmeans\nthatwecantranslateLemma7.1toastatementaboutquicksort:\nLemma11.1. When quicksort is called to sort an array containing the integers 0,...,n 1, the\n\u2212\nexpectednumberoftimeselementiiscomparedtoapivotelementisatmostH +H .\ni+1 n i\n\u2212\nA little summing of harmonic numbers gives us the following theorem about the\nrunningtimeofquicksort:\nTheorem 11.2. When quicksort is called to sort an array containing n distinct elements, the\nexpectednumberofcomparisonsperformedisatmost2nlnn+O(n).\nProof. LetT bethenumberofcomparisonsperformedbyquicksortwhensortingndistinct\nelements. UsingLemma11.1,wehave:\nn 1\n\u2212\nE[T]= (H +H )\ni+1 n i\n\u2212\ni=0\n(cid:88)\nn\n=2 H\ni\ni=1\n(cid:88)\nn\n2 H\nn\n\u2264\ni=1\n(cid:88)\n2nlnn+2n=2nlnn+O(n)\n\u2264\n201\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\nTheorem 11.3 describes the case where the elements being sorted are all distinct.\nWhentheinputarray,a,containsduplicateelements,theexpectedrunningtimeofquick-\nsort is no worse, and can be even better; any time a duplicate element x is chosen as a\npivot, all occurrences of x get grouped together and don\u2019t take part in either of the two\nsubproblems.\nTheorem11.3. ThequickSort(a,c)methodrunsinO(nlogn)expectedtimeandtheexpected\nnumberofcomparisonsitperformsisatmost2nlnn+O(n).\n11.1.3 Heap-sort\nThe heap-sort algorithm is another in-place sorting algorithm. Heap-sort uses the binary\nheaps discussed in Section 10.1. Recall that the BinaryHeap data structure represents a\nheap using a single array. The heap-sort algorithm converts the input array a into a heap\nandthenrepeatedlyextractstheminimumvalue.\nMore specifically, a heap stores n elements at array locations a[0],...,a[n 1] with\n\u2212\nthe smallest value stored at the root, a[0]. After transforming a into a BinaryHeap, the\nheap-sortalgorithmrepeatedlyswapsa[0]anda[n 1],decrementsn,andcallstrickleDown(0)\n\u2212\nsothata[0],...,a[n 2]onceagainareavalidheaprepresentation. Whenthisprocessends\n\u2212\n(because n=0) the elements of a are stored in decreasing order, so a is reversed to obtain\nthefinalsortedorder.1 Figure11.1.3showsanexampleoftheexecutionofheapSort(a,c).\nBinaryHeap\nvoid sort(array<T> &b) {\nBinaryHeap<T> h(b);\nwhile (h.n > 1) {\nh.a.swap(--h.n, 0);\nh.trickleDown(0);\n}\nb = h.a;\nb.reverse();\n}\nA key subroutine in heap sort is the constructor for turning an unsorted array\na into a heap. It would be easy to do this in O(nlogn) time by repeatedly calling the\nBinaryHeap add(x) method, but we can do better by using a bottom-up algorithm. Recall\nthat, in a binary heap, the children of a[i] are stored at positions a[2i+1] and a[2i+2].\n1Thealgorithmcouldalternativelyredefinethecompare(x,y)functionsothattheheapsortalgorithmstores\ntheelementsdirectlyinascendingorder.\n202\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\n5\n9 6\n10 13 8 7\n1112\n5 9 6 10 13 8 7 11 12 4 3 2 1 0\n0 1 2 3 4 5 6 7 8 11 12 13 14 15\nFigure11.4: AsnapshotoftheexecutionofheapSort(a,c). Theshadedpartofthearrayis\nalready sorted. The unshaded part is a BinaryHeap. During the next iteration, element 5\nwillbeplacedintoarraylocation8.\nThis implies that the elements a[ n/2 ],...,a[n 1] have no children. In other words, each\n(cid:98) (cid:99) \u2212\nof a[ n/2 ],...,a[n 1] is a sub-heap of size 1. Now, working backwards, we can call\n(cid:98) (cid:99) \u2212\ntrickleDown(i) for each i n/2 1,...,0 . This works, because by the time we call\n\u2208 {(cid:98) (cid:99)\u2212 }\ntrickleDown(i), each of the two children of a[i] are the root of a sub-heap so calling\ntrickleDown(i)makesa[i]intotherootofitsownsubheap.\nBinaryHeap\nBinaryHeap(array<T> &b) : a(0) {\na = b;\nn = a.length;\nfor (int i = n/2-1; i >= 0; i--) {\ntrickleDown(i);\n}\n}\nTheinterestingthingaboutthisbottom-upstrategyisthatitismoreefficientthan\ncalling add(x) n times. To see this, notice that, for n/2 elements, we do no work at all, for\nn/4 elements, we call trickleDown(i) on a subheap rooted at a[i] and whose height is 1,\nforn/8elements,wecalltrickleDown(i)onasubheapwhoseheightis2,andsoon. Since\nthe work done by trickleDown(i) is proportional to the height of the sub-heap rooted at\na[i],thismeansthatthetotalworkdoneisatmost\nlogn\n\u221e \u221e\nO((i 1)n/2i) O(in/2i)=O(n) i/2i =O(2n)=O(n) .\n\u2212 \u2264\ni=1 i=1 i=1\n(cid:88) (cid:88) (cid:88)\nThesecond-lastequalityfollowsbyrecognizingthatthesum i/2i isequal,bydefini-\n\u221ei=1\n(cid:80)\n203\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\ntion, to the expected number times we toss a coin up to and including the first time the\ncoincomesupasheadsandapplyingLemma4.2.\nThefollowingtheoremdescribestheperformanceofheapSort(a,c).\nTheorem11.4. TheheapSort(a,c)methodrunsinO(nlogn)timeandperformsatmost2nlogn+\nO(n)comparisons.\nProof. The algorithm runs in 3 steps: (1) Transforming a into a heap, (2) repeatedly ex-\ntracting the minimum element from a, and (3) reversing the elements in a. We have just\narguedthatstep1takesO(n)timeandperformsO(n)comparisons. Step3takesO(n)time\nand performs no comparisons. Step 2 performs n calls to trickleDown(0). The ith such\ncalloperatesonaheapofsizen i andperformsatmost2log(n i)comparisons. Summing\n\u2212 \u2212\nthisoveri gives\nn i n i\n\u2212 \u2212\n2log(n i) 2logn=2nlogn\n\u2212 \u2264\ni=0 i=0\n(cid:88) (cid:88)\nAdding the number of comparisons performed in each of the three steps completes the\nproof.\n11.1.4 ALower-BoundforComparison-BasedSorting\nWe have now seen three comparison-based sorting algorithms that each run in O(nlogn)\ntime. Bynow,weshouldbewonderingiffasteralgorithmsexist. Theshortanswertothis\nquestion is no. If the only operations allowed on the elements of a are comparisons then\nno algorithm can avoid doing roughly nlogn comparisons. This is not difficult to prove,\nbutrequiresalittleimagination. Ultimately,itfollowsfromthefactthat\nlog(n!)=logn+log(n 1)+ +log(1)=nlogn O(n) .\n\u2212 \u00b7\u00b7\u00b7 \u2212\n(ProvingthisfactisleftasExercise11.10.)\nWe will first focus our attention on deterministic algorithms like merge-sort and\nheap-sort and on a particular fixed value of n. Imagine such an algorithm is being used\nto sort n distinct elements. The key to proving the lower-bound is to observe that, for a\ndeterministic algorithm with a fixed value of n, the first pair of elements that are com-\npared is always the same. For example, in heapSort(a,c), when n is even, the first call to\ntrickleDown(i)iswithi=n/2 1andthefirstcomparisonisbetweenelementsa[n/2 1]\n\u2212 \u2212\nanda[n 1].\n\u2212\nSince all input elements are distinct, this first comparison has only two possible\noutcomes. The second comparison done by the algorithm may depend on the outcome of\n204\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\na[0]\u2276a[1]\n< >\na[1]\u2276a[2] a[0]\u2276a[2]\n< > < >\na[0]<a[1]<a[2] a[0]\u2276a[2] a[1]<a[0]<a[2] a[1]\u2276a[2]\n< > < >\na[0]<a[2]<a[1] a[2]<a[0]<a[1] a[1]<a[2]<a[0] a[2]<a[1]<a[0]\nFigure11.5: Acomparisontreeforsortinganarraya[0],a[1],a[2]oflengthn=3.\nthefirstcomparison. Thethirdcomparisonmaydependontheresultsofthefirsttwo,and\nso on. In this way, any deterministic comparison-based sorting algorithm can be viewed\nas a rooted binary comparison-tree. Each internal node, u, of this tree is labelled with a\npair of indices u.i and u.j. If a[u.i] < a[u.j] the algorithm proceeds to the left subtree,\notherwiseitproceedstotherightsubtree. Eachleafwofthistreeislabelledwithapermu-\ntationw.p[0],...,w.p[n 1]of0,...,n 1. Thispermutationrepresentsthepermutationthat\n\u2212 \u2212\nisrequiredtosortaifthecomparisontreereachesthisleaf. Thatis,\na[w.p[0]]<a[w.p[1]]< <a[w.p[n 1]] .\n\u00b7\u00b7\u00b7 \u2212\nAnexampleofacomparisontreeforanarrayofsizen=3isshowninFigure11.5.\nThe comparison tree for a sorting algorithm tells us everything about the algo-\nrithm. Ittellsusexactlythesequenceofcomparisonsthatwillbeperformedforanyinput\narray,a,havingndistinctelementsandittellsushowthealgorithmwillreorderatosort\nit. An immediate consequence of this is that the comparison tree must have at least n!\nleaves; if not, then there are two distinct permutations that lead to the same leaf, so the\nalgorithmdoesnotcorrectlysortatleastoneofthesepermutations.\nForexample,thecomparisontreeinFigure11.6hasonly4<3!=6leaves. Inspect-\ning this tree, we see that the two input arrays 3,1,2 and 3,2,1 both lead to the rightmost\nleaf. Ontheinput3,1,2thisleafcorrectlyoutputsa[1]=1,a[2]=2,a[0]=3. However,on\nthe input 3,2,1, this node incorrectly outputs a[1] = 2,a[2] = 1,a[0] = 3. This discussion\nleadstotheprimarylower-boundforcomparison-basedalgorithms.\nTheorem 11.5. For any deterministic comparison-based sorting algorithm and any integer\nA\nn 1, there exists an input array a of length n such that performs at least log(n!)=nlogn\n\u2265 A \u2212\nO(n)comparisonswhensortinga.\n205\n11.SortingAlgorithms 11.1.Comparison-BasedSorting\na[0]\u2276a[1]\n< >\na[1]\u2276a[2] a[0]\u2276a[2]\n< > < >\na[0]<a[1]<a[2] a[0]<a[2]<a[1] a[1]<a[0]<a[2] a[1]<a[2]<a[0]\nFigure11.6: Acomparisontreethatdoesnotcorrectlysorteveryinputpermutation.\nProof. By the above discussion, the comparison tree defined by must have at least n!\nA\nleaves. Aneasyinductiveproofshowsthatanybinarytreewithkleaveshasheightatleast\nlogk. Therefore,thecomparisontreefor hasaleaf,w,ofdepthatleastlog(n!)andthere\nA\nisaninputarrayathatleadstothisleaf. Theinputarrayaisaninputforwhich doesat\nA\nleastlog(n!)comparisons.\nTheorem 11.5 deals with deterministic algorithms like merge-sort and heap-sort,\nbutdoesn\u2019ttellusanythingaboutrandomizedalgorithmslikequicksort. Couldarandom-\nized algorithm beat the log(n!) lower bound on the number of comparisons? The answer,\nagain, is no. Again, the way to prove it is to think differently about what a randomized\nalgorithmis.\nInthefollowingdiscussion,wewillimplicitlyassumethatourdecisiontreeshave\nbeen\u201ccleanedup\u201dinthefollowingway: Anynodethatcannotbereachedbysomeinput\narray a is removed. This cleaning up implies that the tree has exactly n! leaves. It has at\nleastn!leavesbecause, otherwise, itcouldnotsortcorrectly. Ithasatmostn!leavessince\neach of the possible n! permutation of n distinct elements follows exactly one root to leaf\npathinthedecisiontree.\nWe can think of a randomized sorting algorithm as a deterministic algorithm\nR\nthat takes two inputs: The input array a that should be sorted and a long sequence b =\nb ,b ,b ,...,b of random real numbers in the range [0,1]. The random numbers provide\n1 2 3 m\nthe randomization. When the algorithm wants to toss a coin or make a random choice, it\ndoessobyusingsomeelementfromb. Forexample,tocomputetheindexofthefirstpivot\ninquicksort,thealgorithmcouldusetheformula nb .\n1\n(cid:98) (cid:99)\n\u02c6\nNow, notice that if we fix b to some particular sequence b then becomes a de-\nR\n\u02c6 \u02c6\nterministic sorting algorithm, (b), that has an associated comparison tree, (b). Next,\nR T\nnoticethatifweselectatobearandompermutationof 1,...,n ,thenthisisequivalentto\n{ }\n206\n11.SortingAlgorithms 11.2.CountingSortandRadixSort\nselectingarandomleaf,w,fromthen!leavesof (b \u02c6 ).\nT\nExercise 11.12 asks you to prove that, if we select a random leaf from any binary\ntree with k leaves, then the expected depth of that leaf is at least logk. Therefore, the\n\u02c6\nexpected number of comparisons performed by the (deterministic) algorithm (b) when\nR\ngiven an input array containing a random permutation of 1,...,n is at least log(n!). Fi-\n{ }\n\u02c6\nnally, notice that this is true for every choice of b, therefore it holds even for . This\nR\ncompletestheproofofthelower-boundforrandomizedalgorithms.\nTheorem 11.6. For any (deterministic or randomized) comparison-based sorting algorithm\nA\nand any integer n 1, the expected number of comparisons done by when sorting a random\n\u2265 A\npermutationof 1,...,n isatleastlog(n!)=nlogn O(n).\n{ } \u2212\n11.2 Counting Sort and Radix Sort\nIn this section we consider two sorting algorithms that are not comparison-based. These\nalgorithms are specialized for sorting small integers. These algorithms get around the\nlower-bounds of Theorem 11.5 by using (parts of) the elements of a as indices into an\narray. Considerastatementoftheform\nc[a[i]]=1 .\nThis statement executes in constant time, but has c.length possible different outcomes,\ndependingonthevalueofa[i]. Thismeansthattheexecutionofanalgorithmthatmakes\nsuch a statement can not be modelled as a binary tree. Ultimately, this is the reason that\nthealgorithmsinthissectionareabletosortfasterthancomparison-basedalgorithms.\n11.2.1 CountingSort\nSupposewehaveaninputarrayaconsistingofnintegers,eachintherange0,...,k 1. The\n\u2212\ncounting-sort algorithm sorts a using an auxiliary array c of counters. It outputs a sorted\nversionofaasanauxiliaryarrayb.\nTheideabehindcounting-sortissimple: Foreachi 0,...,k 1 ,countthenumber\n\u2208{ \u2212 }\nofoccurrencesofiinaandstorethisinc[i]. Now,aftersorting,theoutputwilllooklike\nc[0] occurrences of 0, followed by c[1] occurrences of 1, followed by c[2] occurrences of\n2,...,followedbyc[k 1]occurrencesofk 1. Thecodethatdoesthisisveryslick,andits\n\u2212 \u2212\nexecutionisillustratedinFigure11.7:\nAlgorithms\nvoid countingSort(array<int> &a, int k) {\narray<int> c(k, 0);\n207\n11.SortingAlgorithms 11.2.CountingSortandRadixSort\na 7 2 9 0 1 2 0 9 7 4 4 6 9 1 0 9 3 2 5 9\nc 3 2 3 1 2 1 1 2 0 5\n0 1 2 3 4 5 6 7 8 9\nc 3 5 8 9 11 12 13 15 15 20\n0\nb 0 0 0 1 1 2 2 2 3 4 4 5 6 7 7 9 9 9 9 9\n0 1 2 3 4 5 6 78 9\nc 3 5 8 9 11 12 13 15 20\n0\na 7 2 9 0 1 2 0 9 7 4 4 6 9 1 0 9 3 2 5 9\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\nFigure11.7: Theoperationofcountingsortonanarrayoflengthn=20thatstoresintegers\n0,...,k 1=9.\n\u2212\nfor (int i = 0; i < a.length; i++)\nc[a[i]]++;\nfor (int i = 1; i < k; i++)\nc[i] += c[i-1];\narray<int> b(a.length);\nfor (int i = a.length-1; i >= 0; i--)\nb[--c[a[i]]] = a[i];\na = b;\n}\nThe first for loop in this code sets each counter c[i] so that it counts the number\nof occurrences of i in a. By using the values of a as indices, these counters can all be\ncomputed in O(n) time with a single for loop. At this point, we could use c to fill in the\noutputarraybdirectly. However,thiswouldnotworkiftheelementsofahaveassociated\ndata. Thereforewespendalittleextraefforttocopytheelementsofaintob.\nThenextforloop,whichtakesO(k)time,computesarunning-sumofthecounters\nso that c[i] becomes the number of elements in a that are less than or equal to i. In\n208\n11.SortingAlgorithms 11.2.CountingSortandRadixSort\nparticular,foreveryi 0,...,k 1 ,theoutputarray,b,willhave\n\u2208{ \u2212 }\nb[c[i 1]]=b[c[i 1]+1]= =b[c[i] 1]=i .\n\u2212 \u2212 \u00b7\u00b7\u00b7 \u2212\nFinally,thealgorithmscansabackwardstoputitselementsinorderintoanoutputarray\nb. When scanning, the element a[i]=j is placed at location b[c[j] 1] and the value c[j]\n\u2212\nisdecremented.\nTheorem 11.7. The countingSort(a,k) method can sort an array a containing n integers in\ntheset 0,...,k 1 inO(n+k)time.\n{ \u2212 }\nThe counting-sort algorithm has the nice property of being stable; it preserves the\nrelativeorderofelementsthatareequal. Iftwoelementsa[i]anda[j]havethesamevalue,\nandi<jthena[i]willappearbeforea[j]inb. Thiswillbeusefulinthenextsection.\n11.2.2 Radix-Sort\nCounting-sort is very efficient for sorting an array of integers when the length, n, of the\narray is not much smaller than the maximum value, k 1, that appears in the array. The\n\u2212\nradix-sortalgorithm,whichwenowdescribe,usesseveralpassesofcounting-sorttoallow\nforamuchgreaterrangeofmaximumvalues.\nRadix-sort sorts w-bit integers by using w/d passes of counting-sort to sort these\nintegers d bits at a time.2 More precisely, radix sort first sorts the integers by their least\nsignificant d bits, then their next significant d bits, and so on until, in the last pass, the\nintegersaresortedbytheirmostsignificantdbits.\nAlgorithms\nvoid radixSort(array<int> &a) {\nint d = 8, w = 32;\nfor (int p = 0; p < w/d; p++) {\narray<int> c(1<<d, 0);\n// the next three for loops implement counting-sort\narray<int> b(a.length);\nfor (int i = 0; i < a.length; i++)\nc[(a[i] >> d*p)&((1<<d)-1)]++;\nfor (int i = 1; i < 1<<d; i++)\nc[i] += c[i-1];\nfor (int i = a.length-1; i >= 0; i--)\nb[--c[(a[i] >> d*p)&((1<<d)-1)]] = a[i];\na = b;\n}\n}\n2Weassumethatddividesw,otherwisewecanalwaysincreasewtod w/d .\n(cid:100) (cid:101)\n209\n11.SortingAlgorithms 11.3.DiscussionandExercises\n01010001 11001000 11110000 00000001 00000001\n00000001 00101000 01010001 11001000 00001111\n11001000 11110000 00000001 00001111 00101000\n00101000 01010001 01010101 01010001 01010001\n00001111 00000001 11001000 01010101 01010101\n11110000 01010101 00101000 00101000 10101010\n10101010 10101010 10101010 10101010 11001000\n01010101 00001111 00001111 11110000 11110000\nFigure11.8: Usingradixsorttosortw=8-bitintegersbyusing4passesofcountingsorton\nd=2-bitintegers.\n(Inthiscode,theexpression(a[i]>>d p)&((1<<d) 1)extractstheintegerwhosebinary\n\u2217 \u2212\nrepresentation is given by bits (p+1)d 1,...,pd of a[i].) An example of the steps of this\n\u2212\nalgorithmisshowninFigure11.8.\nThisremarkablealgorithmsortscorrectlybecausecounting-sortisastablesorting\nalgorithm. If x < y are two elements of a and the most significant bit at which x differs\nfromyhasindexr,thenxwillbeplacedbeforeyduringpass r/d andsubsequentpasses\n(cid:98) (cid:99)\nwillnotchangetherelativeorderofxandy.\nRadix-sortperformsw/dpassesofcounting-sort. EachpassrequiresO(n+2d)time.\nTherefore,theperformanceofradix-sortisgivenbythefollowingtheorem.\nTheorem11.8. Foranyintegerd>0,theradixSort(a,k)methodcansortanarrayacontain-\ningnw-bitintegersinO((w/d)(n+2d))time.\nIf we think, instead, of the elements of the array being in the range 0,...,nc 1 ,\n{ \u2212 }\nandtaked= logn weobtainthefollowingversionofTheorem11.8.\n(cid:100) (cid:101)\nCorollary 11.1. The radixSort(a,k) method can sort an array a containing n integer values\nintherange 0,...,nc 1 inO(cn)time.\n{ \u2212 }\n11.3 Discussion and Exercises\nSorting is probably the fundamental algorithmic problem in computer science, and has\na long history. Knuth [43] attributes the merge-sort algorithm to von Neumann (1945).\nQuicksort is due to Hoare [35]. The original heap-sort algorithm is due to Williams [69],\nbuttheversionpresentedhere(inwhichtheheapisconstructedbottom-upinO(n)time)\n210\n11.SortingAlgorithms 11.3.DiscussionandExercises\nis due to Floyd [25]. Lower-bounds for comparison-based sorting appear to be folklore.\nThefollowingtablesummarizestheperformanceofthesecomparison-basedalgorithms:\ncomparisons in-place\nMerge-sort nlogn worst-case No\nQuicksort 1.38nlogn+O(n)expected Yes\nHeap-sort 2nlogn+O(n)worst-case Yes\nEach of these comparison-based algorithms has advantages and disadvantages.\nMerge-sort does the fewest comparisons and does not rely on randomization. Unfortu-\nnately, it uses an auxilliary array during its merge phase. Allocating this array can be\nexpensiveandisapotentialpointoffailureifmemoryislimited. Quicksortisanin-place\nalgorithmandisaclosesecondintermsofthenumberofcomparisons,butisrandomized\nsothisrunningtimeisnotalwaysguaranteed. Heap-sortdoesthemostcomparisons,but\nitisin-placeanddeterministic.\nThereisonesettinginwhichmerge-sortisaclear-winner;thisoccurswhensorting\nalinked-list. Inthiscase,theauxiliaryarrayisnotneeded;twosortedlinkedlistsarevery\neasilymergedintoasinglesortedlinked-listbypointermanipulations(seeExercise11.2).\nThecounting-sortandradix-sortalgorithmsdescribedhereareduetoSeward[61,\nSection 2.4.6]. However, variants of radix-sort have been used since the 1920\u2019s to sort\npunch cards using punched card sorting machines. These machines can sort a stack of\ncards into two piles based on the existence (or not) of a hole in a specific location on the\ncard. Repeatingthisprocessfordifferentholelocationsgivesanimplementationofradix-\nsort.\nFinally,wenotethatcountingsortandradix-sortcanbeusedtosortothertypesof\nnumbers than non-negative integers. Straightforward modifications of counting sort can\nsort integers, from any interval a,...,b in O(n+b a) time. Similarly, radix sort can sort\n{ } \u2212\nthese integers in O(n(log (b a)) time. Finally, both these algorithms can also be used to\nn\n\u2212\nsortfloatingpointnumbersintheIEEE754floatingpointformat. ThisisbecausetheIEEE\nformat was designed to allow comparison of two floating point numbers by comparing\ntheirvaluesasiftheywereintegersinasigned-magnitudebinaryrepresentation[2].\nExercise11.1. Illustratetheexecutionofmerge-sortandheap-sortonaninputarraycon-\ntaining 1,7,4,6,2,8,3,5. Give a sample illustration of one possible execution of quicksort\nonthesamearray.\n211\n11.SortingAlgorithms 11.3.DiscussionandExercises\nExercise11.2. Implementaversionofthemerge-sortalgorithmthatsortsaDLListwith-\noutusinganauxiliaryarray. (SeeExercise3.13.)\nExercise 11.3. Some implementations of quickSort(a,i,n,c) always use a[i] as a pivot.\nGiven an example of an input array of length n in which such an implementation would\nn\nperform comparisons.\n2\nExercise(cid:0)1(cid:1)1.4. Some implementations of quickSort(a,i,n,c) always use a[i+n/2] as a\npivot. Given an example of an input array of length n in which such an implementation\nn\nwouldperform comparisons.\n2\nExercise 11.5. S (cid:0) h (cid:1) ow that, for any implementation of quickSort(a,i,n,c) that chooses a\npivot deterministically, without looking at a[i],...,a[i+n 1], there exists an input array\n\u2212\noflengthnthatcausesthisimplementationtoperform n comparisons.\n2\nExercise11.6. DesignaComparator,c,thatyoucouldpa (cid:0) s (cid:1) sasanargumenttoquickSort(a,i,n,c)\nn\nandthatwouldcausequicksorttoperform comparisons. (Hint: Yourcomparatordoes\n2\nnotactuallyneedtolookatthevaluesbeingcompared.)\n(cid:0) (cid:1)\nExercise 11.7. Analyze the expected number of comparisons done by Quicksort a little\nmore carefully than the proof of Theorem 11.3. In particular, show that the expected\nnumberofcomparisonsis2nH n+H .\nn n\n\u2212\nExercise11.8. Describe an input array that causes heap sort to perform at least 2nlogn\n\u2212\nO(n)comparisons. Justifyyouranswer.\nExercise11.9. Findanotherpairofpermutationsof1,2,3thatarenotcorrectlysortedby\nthecomparison-treeinFigure11.6.\nExercise11.10. Provethatlogn!=nlogn O(n).\n\u2212\nExercise11.11. Provethatabinarytreewithk leaveshasheightatleastlogk.\nExercise11.12. Provethat,ifwepickarandomleaffromabinarytreewithk leaves,then\ntheexpectedheightofthisleafisatleastlogk.\nExercise11.13. The implementation of radixSort(a,k) given here works when the input\narray, a contains only unsigned integers. Write a version that works correctly for signed\nintegers.\n212\nChapter 12\nGraphs\nIn this chapter, we study two representations of graphs and basic algorithms on these\nrepresentations.\nMathematically,a(directed)graphisapairG=(V,E)whereV isasetofverticesand\nE isasetoforderedpairsofverticescallededges. Anedge(i,j)isdirectedfromitoj;iis\ncalledthesourceoftheedgeandjiscalledthetarget. ApathinGisasequenceofvertices\nv ,...,v such that, for every i 1,...,k , the edge (v ,v ) is in E. A path v ,...,v is a\n0 k i 1 i 0 k\n\u2208 { } \u2212\ncycleif,additionally,theedge(v ,v )isinE. Apath(orcycle)issimpleifallofitsvertices\nk 0\nare unique. If there is a path from some vertex v to some vertex v then we say that v is\ni j j\nreachablefromv . AnexampleofagraphisshowninFigure12.1.\ni\nGraphshaveanenormousnumberofapplications,duetotheirabilitytomodelso\nmany phenomena. There are many obvious examples. Computer networks can be mod-\nelledasgraphs,withverticescorrespondingtocomputersandedgescorrespondingto(di-\nrected) communication links between those computers. Street networks can be modelled\nas graphs, with vertices representing intersections and edges representing streets joining\nconsecutiveintersections.\nLessobviousexamplesoccurassoonaswerealizethatgraphscanmodelanypair-\nwise relationships within a set. For example, in a university setting we might have a\ntimetable conflict graph whose vertices represent courses offered in the university and\nin which the edge (i,j) is present if and only if there is at least one student that is tak-\ning both class i and class j. Thus, an edge indicates that the exam for class i can not be\nscheduledatthesametimeastheexamforclassj.\nThroughout this section, we will use n to denote the number of vertices of G and\nm to denote the number of edges of G. That is, n = V and m = E. Furthermore, we will\n| | | |\nassume that V = 0,...,n 1 . Any other data that we would like to associate with the\n{ \u2212 }\n213\n12.Graphs\n0 1 2 3\n4 5 6 7\n8 9 10 11\nFigure 12.1: A graph with 12 vertices. Vertices are drawn as numbered circles and edges\naredrawnaspointedcurvespointingfromsourcetotarget.\nelementsofV canbestoredinanarrayoflengthn.\nSometypicaloperationsperformedongraphsare:\n\u2022 addEdge(i,j): Addtheedge(i,j)toE.\n\u2022 removeEdge(i,j): Removetheedge(i,j)fromE.\n\u2022 hasEdge(i,j): Checkiftheedge(i,j) E\n\u2208\n\u2022 outEdges(i): ReturnaListofallintegersjsuchthat(i,j) E\n\u2208\n\u2022 inEdges(i): ReturnaListofallintegersjsuchthat(j,i) E\n\u2208\nNotethattheseoperationsarenotterriblydifficulttoimplementefficiently. Forex-\nample,thefirstthreeoperationscanbeimplementeddirectlybyusingaUSet,sotheycan\nbe implemented in constant expected time using the hash tables discussed in Chapter 5.\nThelasttwooperationscanbeimplementedinconstanttimebystoring,foreachvertex,a\nlistofitsadjacentvertices.\nHowever,differentapplicationsofgraphshavedifferentperformancerequirements\nfortheseoperationsand,ideally,wecanusethesimplestimplementationthatsatisfiesall\nthe application\u2019s requirements. For this reason, we discuss two broad categories of graph\nrepresentations.\n214\n12.Graphs 12.1.AdjacencyMatrix: RepresentingaGraphbyaMatrix\n12.1 AdjacencyMatrix: Representing a Graph by a Matrix\nAnadjacencymatrixisawayofrepresentingannvertexgraphG=(V,E)byann nmatrix,\n\u00d7\na,whoseentriesarebooleanvalues.\nAdjacencyMatrix\nint n;\nbool **a;\nThematrixentrya[i][j]isdefinedas\ntrue if(i,j) E\na[i][j]= \u2208\n\uf8f1false\notherwise\n\uf8f4\uf8f4\uf8f4\uf8f2\nTheadjacencymatrixforthegraphinF\n\uf8f4\uf8f4\uf8f4\uf8f3igure12.1isshowninFigure12.2.\nWiththisrepresentation,theaddEdge(i,j),removeEdge(i,j),andhasEdge(i,j)op-\nerationsjustinvolvesettingorreadingthematrixentrya[i][j]:\nAdjacencyMatrix\nvoid addEdge(int i, int j) {\na[i][j] = true;\n}\nvoid removeEdge(int i, int j) {\na[i][j] = false;\n}\nbool hasEdge(int i, int j) {\nreturn a[i][j];\n}\nTheseoperationsclearlytakeconstanttimeperoperation.\nWheretheadjacencymatrixperformspoorlyiswiththeoutEdges(i)andinEdges(i)\noperations. To implement these, we must scan all n entries in the corresponding row or\ncolumnofaandgatherupalltheindices,j,wherea[i][j],respectivelya[j][i],istrue.\nAdjacencyMatrix\nvoid outEdges(int i, List &edges) {\nfor (int j = 0; j < n; j++)\nif (a[i][j]) edges.add(j);\n}\nvoid inEdges(int i, List &edges) {\nfor (int j = 0; j < n; j++)\nif (a[j][i]) edges.add(j);\n}\n215\n12.Graphs 12.1.AdjacencyMatrix: RepresentingaGraphbyaMatrix\n0 1 2 3\n4 5 6 7\n8 9 10 11\n0 1 2 3 4 5 6 7 8 9 10 11\n0 0 1 0 0 1 0 0 0 0 0 0 0\n1 1 0 1 0 0 1 1 0 0 0 0 0\n2 1 0 0 1 0 0 1 0 0 0 0 0\n3 0 0 1 0 0 0 0 1 0 0 0 0\n4 1 0 0 0 0 1 0 0 1 0 0 0\n5 0 1 1 0 1 0 1 0 0 1 0 0\n6 0 0 1 0 0 1 0 1 0 0 1 0\n7 0 0 0 1 0 0 1 0 0 0 0 1\n8 0 0 0 0 1 0 0 0 0 1 0 0\n9 0 0 0 0 0 1 0 0 1 0 1 0\n10 0 0 0 0 0 0 1 0 0 1 0 1\n11 0 0 0 0 0 0 0 1 0 0 1 0\nFigure12.2: Agraphanditsadjacencymatrix.\n216\n12.Graphs 12.2.AdjacencyLists: AGraphasaCollectionofLists\nTheseoperationsclearlytakeO(n)timeperoperation.\nAnotherdrawbackoftheadjacencymatrixrepresentationisthatitisbig. Itstores\nan n n boolean matrix, so it requires at least n2 bits of memory. The implementation\n\u00d7\nhere uses a matrix of bool values so it actually uses on the order of n2 bytes of memory.\nA more careful implementation, that packs w boolean values into each word of memory\ncouldreducethisspaceusagetoO(n2/w)wordsofmemory.\nTheorem 12.1. The AdjacencyMatrix data structure implements the Graph interface. An\nAdjacencyMatrixsupportstheoperations\n\u2022 addEdge(i,j),removeEdge(i,j),andhasEdge(i,j)inconstanttimeperoperation;and\n\u2022 inEdges(i),andoutEdges(i)inO(n)timeperoperation.\nThespaceusedbyanAdjacencyMatrixisO(n2).\nDespitethehighmemoryusageandpoorperformanceoftheinEdges(i)andoutEdges(i)\noperations, an AdjacencyMatrix can still be useful for some applications. In particular,\nwhenthegraphG isdense,i.e.,ithascloseton2 edges,thenamemoryusageofn2 maybe\nacceptable.\nTheAdjacencyMatrixdatastructureisalsocommonlyusedbecausealgebraicop-\nerations on the matrix a can be used to efficiently compute properties of the graph G.\nThis is a topic for a course on algorithms, but we point out one such property here: If we\ntreat the entries of a as integers (1 for true and 0 for false) and multiply a by itself us-\ning matrix multiplication then we get the matrix a2. Recall, from the definition of matrix\nmultiplication,that\nn 1\na2[i][j]= \u2212 a[i][k] a[k][j] .\n\u00b7\nk=0\n(cid:88)\nInterpretingthissumintermsofthegraphG,thisformulacountsthenumberofvertices,\nk, such that G contains both edges (i,k) and (k,j). That is, it counts the number of paths\nfromitoj(throughintermediatevertices,k)thathavelengthexactly2. Thisobservation\nis the foundation of an algorithm that computes the shortest paths between all pairs of\nverticesinGusingonlyO(logn)matrixmultiplications.\n12.2 AdjacencyLists: A Graph as a Collection of Lists\nAdjacencylistrepresentationstakesamorevertex-centricapproach. Therearemanydiffer-\nent possible implementations of adjacency lists. In this section, we present a simple one.\n217\n12.Graphs 12.2.AdjacencyLists: AGraphasaCollectionofLists\n0 1 2 3\n4 5 6 7\n8 9 10 11\n0 1 2 3 4 5 6 7 8 9 10 11\n1 0 1 2 0 1 5 6 4 8 9 10\n4 2 3 7 5 2 2 3 9 5 6 7\n6 6 8 6 7 11 10 11\n5 9 10\n4\nFigure12.3: Agraphanditsadjacencylists\nAt the end of the section, we discuss different possibilities. In an adjacency list represen-\ntation,thegraphG=(V,E)isrepresentedasanarray,adj,oflists. Thelistadj[i]contains\na list of all the vertices adjacent to vertex i. That is, it contains every index j such that\n(i,j) E.\n\u2208\nAdjacencyLists\nint n;\nList *adj;\n(An example is shown in Figure 12.3.) In this particular implementation, we represent\neach list in adj as a subclass of ArrayStack, because we would like constant time access\nbyposition. Otheroptionsarealsopossible. Specifically,wecouldhaveimplementedadj\nasaDLList.\nTheaddEdge(i,j)operationjustappendsthevaluejtothelistadj[i]:\nAdjacencyLists\nvoid addEdge(int i, int j) {\nadj[i].add(j);\n}\nThistakesconstanttime.\n218\n12.Graphs 12.2.AdjacencyLists: AGraphasaCollectionofLists\nTheremoveEdge(i,j)operationsearchesthroughthelistadj[i]untilitfindsjand\nthenremovesit:\nAdjacencyLists\nvoid removeEdge(int i, int j) {\nfor (int k = 0; k < adj[i].size(); k++) {\nif (adj[i].get(k) == j) {\nadj[i].remove(k);\nreturn;\n}\n}\n}\nThis takes O(deg(i)) time, where deg(i) (the degree of i) counts the number of edges in E\nthathaveiastheirsource.\nThe hasEdge(i,j) operation is similar; it searches through the list adj[i] until it\nfindsj(andreturnstrue),orreachestheendofthelist(andreturnsfalse):\nAdjacencyLists\nbool hasEdge(int i, int j) {\nreturn adj[i].contains(j);\n}\nThisalsotakesO(deg(i))time.\nTheoutEdges(i)operationisverysimple;Itsimplycopiesthevaluesinadj[i]into\ntheoutputlist:\nAdjacencyLists\nvoid outEdges(int i, LisT &edges) {\nfor (int k = 0; k < adj[i].size(); k++)\nedges.add(adj[i].get(k));\n}\nThisclearlytakesO(deg(i))time.\nTheinEdges(i)operationismuchmorework. Itscansovereveryvertexj checking\niftheedge(i,j)existsand,ifso,addingjtotheoutputlist:\nAdjacencyLists\nvoid inEdges(int i, LisT &edges) {\nfor (int j = 0; j < n; j++)\nif (adj[j].contains(i)) edges.add(j);\n}\nThis operation is very slow. It scans the adjacency list of every vertex, so it takes O(n+m)\ntime.\n219\n12.Graphs 12.3.GraphTraversal\nThefollowingtheoremsummarizestheperformanceoftheabovedatastructure:\nTheorem 12.2. The AdjacencyLists data structure implements the Graph interface. An\nAdjacencyListssupportstheoperations\n\u2022 addEdge(i,j)inconstanttimeperoperation;\n\u2022 removeEdge(i,j)andhasEdge(i,j)inO(deg(i))timeperoperation;\n\u2022 outEdges(i)inO(deg(i))timeperoperation;and\n\u2022 inEdges(i)inO(n+m)timeperoperation.\nThespaceusedbyaAdjacencyListsisO(n+m).\nAsalludedtoearlier,therearemanydifferentchoicestobemadewhenimplement-\ningagraphasanadjacencylist. Somequestionsthatcomeupinclude:\n\u2022 Whattypeofcollectionshouldbeusedtostoreeachelementofadj? Onecoulduse\nanarray-basedlist,alinked-list,orevenahashtable.\n\u2022 Should there be a second adjacency list, inadj, that stores, for each i, the list of\nvertices, j, such that (j,i) E? This can greatly reduce the running-time of the\n\u2208\ninEdges(i) operation, but requires slightly more work when adding or removing\nedges.\n\u2022 Should the entry for the edge (i,j) in adj[i] be linked by a reference to the corre-\nspondingentryininadj[j]?\n\u2022 Should edges be first-class objects with their own associated data? In this way, adj\nwouldcontainlistsofedgesratherthanlistsofvertices(integers).\nMost of these questions come down to a tradeoff between complexity (and space) of im-\nplementationandperformancefeaturesoftheimplementation.\n12.3 Graph Traversal\nIn this section we present two algorithms for exploring a graph, starting at one of its ver-\ntices, i, and finding all vertices that are reachable from i. Both of these algorithms are\nbestsuitedtographsrepresentedusinganadjacencylistrepresentation. Therefore,when\nanalyzing these algorithms we will assume that the underlying representation is as an\nAdjacencyLists.\n220\n12.Graphs 12.3.GraphTraversal\n12.3.1 Breadth-FirstSearch\nThe bread-first-search algorithm starts at a vertex i and visits, first the neighbours of i,\nthen the neighbours of the neighbours of i, then the neighbours of the neighbours of the\nneighboursofi,andsoon.\nThis algorithm is a generalization of the breadth-first-search algorithm for binary\ntrees (Section 6.1.2), and is very similar; it uses a queue, q, that initially contains only i.\nItthenrepeatedlyextractsanelementfromqandaddsitsneighbourstoq, providedthat\ntheseneighbourshaveneverbeeninqbefore. Theonlymajordifferencebetweenbreadth-\nfirst-search for graphs and for trees is that the algorithm for graphs has to ensure that\nit does not add the same vertex to q more than once. It does this by using an auxiliary\nbooleanarray,seen,thatkeepstrackofwhichverticeshavealreadybeendiscovered.\nAlgorithms\nvoid bfs(Graph &g, int r) {\nbool *seen = new bool[g.nVertices()];\nSLList<int> q;\nq.add(r);\nseen[r] = true;\nwhile (q.size() > 0) {\nint i = q.remove();\nArrayStack<int> edges;\ng.outEdges(i, edges);\nfor (int k = 0; k < edges.size(); k++) {\nint j = edges.get(k);\nif (!seen[j]) {\nq.add(j);\nseen[j] = true;\n}\n}\n}\ndelete[] seen;\n}\nAn example of running bfs(g,0) on the graph from Figure 12.1 is shown in Figure 12.4.\nDifferent executions are possible, depending on the ordering of the adjacency lists; Fig-\nure12.4usestheadjacencylistsinFigure12.3.\nAnalyzing the running-time of the bfs(g,i) routine is fairly straightforward. The\nuse of the seen array ensures that no vertex is added to q more than once. Adding (and\nlaterremoving)eachvertexfromqtakesconstanttimepervertexforatotalofO(n)time.\nSince each vertex is processed at most once by the inner loop, each adjacency list is pro-\n221\n12.Graphs 12.3.GraphTraversal\n0 1 3 7\n2 5 4 8\n6 10 9 11\nFigure 12.4: An example of bread-first-search starting at node 0. Nodes are labelled with\nthe order in which they are added to q. Edges that result in nodes being added to q are\ndrawninblack,otheredgesaredrawningrey.\ncessedatmostonce,soeachedgeofGisprocessedatmostonce. Thisprocessing,whichis\ndoneintheinnerlooptakesconstanttimeperiteration,foratotalofO(m)time. Therefore,\ntheentirealgorithmrunsinO(n+m)time.\nThefollowingtheoremsummarizestheperformanceofthebfs(g,r)algorithm.\nTheorem12.3. WhengivenasinputaGraph,g,thatisimplementedusingtheAdjacencyLists\ndatastructure,thebfs(g,r)algorithmrunsinO(n+m)time.\nA breadth-first traversal has some very special properties. Calling bfs(g,r) will\neventually enqueue (and eventually dequeue) every vertex j such that there is a directed\npathfromrtoj. Moreover,theverticesatdistance0fromr(ritself)willenterqbeforethe\nverticesatdistance1,whichwillenterqbeforetheverticesatdistance2,andsoon. Thus,\nthebfs(g,r)methodvisitsverticesinincreasingorderofdistancefromrandverticesthat\ncannotbereachedfromrareneveroutputatall.\nAparticularlyusefulapplicationofthebreadth-first-searchalgorithmis,therefore,\nin computing shortest paths. To compute the shortest path from r to every other vertex,\nweuseavariantofbfs(g,r)thatusesanauxilliaryarray,p,oflengthn. Whenanewvertex\njisaddedtoq,wesetp[j]=i. Inthisway,p[j]becomesthesecondlastnodeonashortest\npathfromrtoj. Repeatingthis,bytakingp[p[j],p[p[p[j]]],andsoonwecanreconstruct\nthe(reversalof)ashortestpathfromrtoj.\n222\n12.Graphs 12.3.GraphTraversal\n12.3.2 Depth-FirstSearch\nThe depth-first-search algorithm is similar to the standard algorithm for traversing binary\ntrees; it first fully explores one subtree before returning to the current node and then\nexploringtheothersubtree. Anotherwaytothinkofdepth-first-searchisbysayingthatit\nissimilartobreadth-firstsearchexceptthatitusesastackinsteadofaqueue.\nDuringtheexecutionofthedepth-first-searchalgorithm,eachvertex,i,isassigned\nacolor,c[i]: whiteifwehaveneverseenthevertexbefore,greyifwearecurrentlyvisiting\nthatvertex,andblackifwearedonevisitingthatvertex. Theeasiestwaytothinkofdepth-\nfirst-searchisasarecursivealgorithm. Itstartsbyvisitingr. Whenvisitingavertexi,we\nfirstmarkiasgrey. Next,wescani\u2019sadjacencylistandrecursivelyvisitanywhitevertex\nwefindinthislist. Finally,wearedoneprocessingi,sowecoloriblackandreturn.\nAlgorithms\nvoid dfs(Graph &g, int i, char *c) {\nc[i] = grey; // currently visiting i\nArrayStack<int> edges;\ng.outEdges(i, edges);\nfor (int k = 0; k < edges.size(); k++) {\nint j = edges.get(k);\nif (c[j] == white) {\nc[j] = grey;\ndfs(g, j, c);\n}\n}\nc[i] = black; // done visiting i\n}\nvoid dfs(Graph &g, int r) {\nchar *c = new char[g.nVertices()];\ndfs(g, r, c);\ndelete[] c;\n}\nAnexampleoftheexecutionofthisalgorithmisshowninFigure12.5\nAlthoughdepth-first-searchmaybestbethoughtofasarecursivealgorithm,recur-\nsion is not the best way to implement it. Indeed, the code given above will fail for many\nlarge graphs by causing a stack overflow. An alternative implementation is to replace the\nrecursionstackwithanexplicitstack,s. Thefollowingimplementationdoesjustthat:\nAlgorithms\nvoid dfs2(Graph &g, int r) {\nchar *c = new char[g.nVertices()];\nSLList<int> s;\n223\n12.Graphs 12.3.GraphTraversal\n0 1 2 3\n9 10 11 4\n8 7 6 5\nFigure12.5: Anexample ofdepth-first-searchstarting atnode0. Nodesare labelledwith\nthe order in which they are processed. Edges that result in a recursive call are drawn in\nblack,otheredgesaredrawningrey.\ns.push(r);\nwhile (s.size() > 0) {\nint i = s.pop();\nif (c[i] == white) {\nc[i] = grey;\nArrayStack<int> edges;\ng.outEdges(i, edges);\nfor (int k = 0; k < edges.size(); k++)\ns.push(edges.get(k));\n}\n}\ndelete[] c;\n}\nIn the above code, when next vertex, i, is processed, i is colored grey and then replaced,\nonthestack,withitsadjacentvertices. Duringthenextiteration,oneoftheseverticeswill\nbevisited.\nNotsurprisingly, therunningtimesofdfs(g,r)anddfs2(g,r)arethesameasthat\nofbfs(g,r):\nTheorem12.4. WhengivenasinputaGraph,g,thatisimplementedusingtheAdjacencyLists\ndatastructure,thedfs(g,r)anddfs2(g,r)algorithmseachruninO(n+m)time.\nAs with the breadth-first-search algorithm, there is an underlying tree associated\nwitheachexecutionofdepth-first-search. Whenanodei(cid:44)rgoesfromwhitetogrey,this\n224\n12.Graphs 12.4.DiscussionandExercises\nP\nj i\nC\nFigure 12.6: The depth-first-search algorithm can be used to detect cycles in G.The node\nj is colored grey while i is still grey. This implies there is a path, P, from i to j in the\ndepth-first-searchtree,andtheedge(j,i)impliesthatP isalsoacycle.\nisbecausedfs(g,i,c)wascalledrecursivelywhileprocessingsomenodei . (Inthecaseof\n(cid:48)\ndfs2(g,r) algorithm, i is one of the nodes that replaced i on the stack.) If we think of i\n(cid:48) (cid:48)\nastheparentofi,thenweobtainatreerootedatr. InFigure12.5,thistreeisapathfrom\nvertex0tovertex11.\nAn important property of the depth-first-search algorithm is the following: Sup-\nposethatwhennodeiiscoloredgrey,thereexistsapathfromitosomeothernodejthat\nusesonlywhitevertices. Thenjwillbecolored(firstgreythen)blackbeforeiiscolored\nblack. (Thiscanbeprovenbycontradiction,byconsideringanypathP fromitoj.)\nOne application of this property is the detection of cycles. Refer to Figure 12.6.\nConsider some cycle, C, that can be reached from r. Let i be the first node of C that is\ncolored grey, and let j be the node that precedes i on the cycle C. Then, by the above\nproperty, j will be colored grey and the edge (j,i) will be considered by the algorithm\nwhile i is still grey. Thus, the algorithm can conclude that there is a path, P, from i to j\ninthedepth-first-searchtreeandtheedge(j,i)exists. Therefore,P isalsoacycle.\n12.4 Discussion and Exercises\nTherunningtimesofthedepth-first-searchandbreadth-first-searchalgorithmsaresome-\nwhatoverstatedbytheTheorems12.3and12.4. Definen asthenumberofvertices,i,of\nr\nG, for which there exists a path from r to i. Define m as the number of edges that have\nr\nthese vertices as their sources. Then the following theorem is a more precise statement\nof the running times of the breadth-first-search and depth-first-search algorithms. (This\nmore refined statement of the running time is useful in some of the applications of these\nalgorithmsoutlinedintheexercises.)\nTheorem12.5. WhengivenasinputaGraph,g,thatisimplementedusingtheAdjacencyLists\ndatastructure,thebfs(g,r),dfs(g,r)anddfs2(g,r)algorithmseachruninO(n +m )time.\nr r\n225\n12.Graphs 12.4.DiscussionandExercises\n5\n9\n0\n4\n1 6\n3\n2\n8\n7\nFigure12.7: Anexamplegraph.\nBreadth-first search seems to have been discovered independently by Moore [47]\nandLee[44]inthecontextsofmazeexplorationandcircuitrouting,respectively.\nAdjacency-list representations of graphs were first popularized by Hopcroft and\nTarjan [36] as an alternative to the (then more common) adjacency-matrix representa-\ntion. This representation, and depth-first-search, played a major part in the celebrated\nHopcroft-Tarjan planarity testing algorithm that can determine, in O(n) time, if a graph\ncanbedrawn,intheplane,andinsuchawaythatnopairofedgescrosseachother[37].\nIn the following exercises, an undirected graph is one in which, for every i and j,\ntheedge(i,j)ispresentifandonlyiftheedge(j,i)ispresent.\nExercise 12.1. Draw an adjacencly list representation and an adjacency matrix represen-\ntationofthegraphinFigure12.7.\nExercise 12.2. The incidence matrix representation of a graph, G, is an n m matrix, A,\n\u00d7\nwhere\n1 ifvertexi thesourceofedgej\n\u2212\nA i,j\n=\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f20 +1\no\nif\nth\nve\ne\nr\nr\nt\nw\ne\ni\nx\nse\ni\n.\nthetargetofedgej\n1. Drawtheincidentmatri\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nxrepresentationofthegraphinFigure12.7.\n2. Design, analyze and implement an incidence matrix representation of a graph. Be\nsure to analyze the space, the cost of addEdge(i,j), removeEdge(i,j), hasEdge(i,j),\ninEdges(i),andoutEdges(i).\n226\n12.Graphs 12.4.DiscussionandExercises\nExercise 12.3. Illustrate an execution of the bfs(G,0) and dfs(G,0) on the graph, G, in\nFigure12.7.\nExercise 12.4. Let G be an undirected graph. We say G is connected if, for every pair of\nverticesiandjinG,thereisapathfromitoj(sinceGisundirected,thereisalsoapath\nfromjtoi). ShowhowtotestifGisconnectedinO(n+m)time.\nExercise 12.5. Let G be an undirected graph. A connected-component labelling of G parti-\ntionstheverticesofGintomaximalsets,eachofwhichformsaconnectedsubgraph. Show\nhowtocomputeaconnectedcomponentlabellingofG inO(n+m)time.\nExercise12.6. LetGbeanundirectedgraph. AspanningforestofGisacollectionoftrees,\nonepercomponent,whoseedgesareedgesofG andwhoseverticescontainallverticesof\nG. ShowhowtocomputeaspanningforestofofGinO(n+m)time.\nExercise 12.7. We say that a graph G is strongly-connected if, for every pair of vertices i\nand j in G, there is a path from i to j. Show how to test if G is strongly-connected in\nO(n+m)time.\nExercise 12.8. Given a graph G = (V,E) and some special vertex r V, show how to\n\u2208\ncomputethelengthoftheshortestpathfromrtoiforeveryvertexi V.\n\u2208\nExercise12.9. Givea(simple)examplewherethedfs(g,r)codevisitsthenodesofagraph\nin an order that is different from that of the dfs2(g,r) code. Write a version of dfs2(g,r)\nthatalwaysvisitsnodesinexactlythesameorderasdfs(g,r). (Hint: Juststarttracingthe\nexecutionofeachalgorithmonsomegraphwhereristhesourceofmorethan1edge.)\nExercise 12.10. A universal sink in a graph G is a vertex that is the target of n 1 edges\n\u2212\nand the source of no edges.1 Design and implement an algorithm that tests if a graph G,\nrepresented as an AdjacencyMatrix, has a universal sink. Your algorithm should run in\nO(n)time.\n1Auniversalsink,v,isalsosometimescalledacelebrity: Everyoneintheroomrecognizesv,butvdoesn\u2019t\nrecognizeanyoneelseintheroom.\n227\n12.Graphs 12.4.DiscussionandExercises\n228\nChapter 13\nData Structures for Integers\nIn this chapter, we return to the problem of implementing an SSet. The difference now\nis that we assume the elements stored in the SSet are w-bit integers. That is, we want to\nimplement add(x), remove(x), and find(x) where x 0,...,2w 1 . It is not too hard to\n\u2208 { \u2212 }\nthink of plenty of applications where the data\u2014or at least the key that we use for sorting\nthedata\u2014isaninteger.\nWe will discuss three data structures, each building on the ideas of the previous.\nThe first structure, the BinaryTrie performs all three SSet operations in O(w) time. This\nisnotveryimpressive,sinceanysubsetof 0,...,2w 1 hassizen 2w,sothatlogn w. All\n{ \u2212 } \u2264 \u2264\nthe other SSet implementations discussed in this book perform all operations in O(logn)\ntimesotheyareallatleastasfastasaBinaryTrie.\nThe second structure, the XFastTrie, speeds up the search in a BinaryTrie by\nusing hashing. With this speedup, the find(x) operation runs in O(logw) time. However,\nadd(x) and remove(x) operations in an XFastTrie still take O(w) time and the space used\nbyanXFastTrieisO(n w).\n\u00b7\nThethirddatastructure,theYFastTrie,usesanXFastTrietostoreonlyasample\nofroughlyoneoutofeverywelementsandstorestheremainingelementsastandardSSet\nstructure. This trick reduces the running time of add(x) and remove(x) to O(logw) and\ndecreasesthespacetoO(n).\nTheimplementationsusedasexamplesinthischaptercanstoreanytypeofdata,as\nlonganintegercanbeassociatedwithit. Inthecodesamples,thevariableixisalwaysthe\ninteger value associated with x, and the method intValue(x) converts x to its associated\ninteger. Inthetext,however,wewillsimplytreatxasifitisaninteger.\n229\n13.DataStructuresforIntegers 13.1.BinaryTrie: Adigitalsearchtree\n????\n0??? 1???\n00?? 01?? 10?? 11??\n000? 001? 010? 011? 100? 101? 110? 111?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nFigure13.1: Theintegersstoredinabinarytrieareencodedasroot-to-leafpaths.\n13.1 BinaryTrie: A digital search tree\nA BinaryTrie encode a set of w bit integers in a binary tree. All leaves in the tree have\ndepthwandeachintegerisencodedasaroot-to-leafpath. Thepathfortheintegerxturns\nleftatleveliiftheithmostsignificantbitofxisa0andturnsrightifitisa1. Figure13.1\nshowsanexampleforthecasew=4,inwhichthetriestorestheintegers3(0011),9(1001),\n12(1100),and13(1101).\nBecause the search path for a value x depends on the bits of x it will be helpful\nto name the children of a node, u, u.child[0] (left) and u.child[1] (right). These child\npointerswillactuallyservedouble-duty. Sincetheleavesinabinarytriehavenochildren,\nthe pointers are used to string the leaves together into a doubly-linked list. For a leaf in\nthebinarytrieu.child[0](prev)isthenodethatcomesbeforeuinthelistandu.child[1]\n(next) is the node that follows u in the list. A special node, dummy, is used both before\nthe first node and after the last node in the list (see Section 3.2). In the code samples,\nu.child[0], u.left, and u.prev refer to the same field in the node u, as do u.child[1],\nu.right,andu.next.\nEachnode,u,alsocontainsanadditionalpointeru.jump. Ifu\u2019sleftchildismissing,\nthen u.jump points to the smallest leaf in u\u2019s subtree. If u\u2019s right child is missing, then\nu.jumppointstothelargestleafinu\u2019ssubtree. AnexampleofaBinaryTrie,showingjump\npointersandthedoubly-linkedlistattheleaves,isshowninFigure13.2\nThe find(x) operation in a BinaryTrie is fairly straightforward. We try to follow\nthesearchpathforxinthetrie. Ifwereachaleaf,thenwehavefoundx. If,wereachanode\n230\n13.DataStructuresforIntegers 13.1.BinaryTrie: Adigitalsearchtree\n????\n0??? 1???\n00?? 01?? 10?? 11??\n000? 001? 010? 011? 100? 101? 110? 111?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nFigure13.2: ABinaryTriewithjumppointersshownascurveddashededges.\nu where we cannot proceed (because u is missing a child) then we follow u.jump, which\ntakes us either to smallest leaf larger than x or the largest leaf smaller than x. Which of\nthesetwocasesoccursdependsonwhetheruismissingitsleftorrightchild,respectively.\nIntheformercase(uismissingitsleftchild),wehavefoundthevaluewearelookingfor.\nInthelattercase(uismissingitsrightchild),wecanusethelinkedlisttoreachthevalue\nwearelookingfor. EachofthesecasesisillustratedinFigure13.3.\nBinaryTrie\nT find(T x) {\nint i, c = 0;\nunsigned ix = intValue(x);\nNode *u = &r;\nfor (i = 0; i < w; i++) {\nc = (ix >> (w-i-1)) & 1;\nif (u->child[c] == NULL) break;\nu = u->child[c];\n}\nif (i == w) return u->x; // found it\nu = (c == 0) ? u->jump : u->jump->next;\nreturn u == &dummy ? NULL : u->x;\n}\nThe running-time of the find(x) method is dominated by the time it takes to follow a\nroot-to-leafpath,soitrunsinO(w)time.\nThe add(x) operation in a BinaryTrie is fairly straightforward, but it has a lot of\nthingstotakecareof:\n231\n13.DataStructuresforIntegers 13.1.BinaryTrie: Adigitalsearchtree\n????\n0??? 1???\nfind(5) find(8)\n00?? 01?? 10?? 11??\n000? 001? 010? 011? 100? 101? 110? 111?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nFigure13.3: Thepathsfollowedbyfind(5)andfind(8).\n1. It follows the search path for x until reaching a node u where it can no longer pro-\nceed.\n2. Itcreatestheremainderofthesearchpathfromutoaleafthatcontainsx.\n3. Itaddsthenode,u ,containingxtothelinkedlistofleaves(ithasaccesstou \u2019sprede-\n(cid:48) (cid:48)\ncessor,pred,inthelinkedlistfromthejumppointerofthelastnode,u,encountered\nduringstep1.)\n4. It walks back up the search path for x adjusting jump pointers at the nodes whose\njumppointershouldnowpointtox.\nAnadditionisillustratedinFigure13.4.\nBinaryTrie\nbool add(T x) {\nint i, c = 0;\nunsigned ix = intValue(x);\nNode *u = &r;\n// 1 - search for ix until falling out of the trie\nfor (i = 0; i < w; i++) {\nc = (ix >> (w-i-1)) & 1;\nif (u->child[c] == NULL) break;\nu = u->child[c];\n}\nif (i == w) return false; // trie already contains x - abort\nNode *pred = (c == right) ? u->jump : u->jump->left; // save for step 3\nu->jump = NULL; // u will have two children shortly\n// 2 - add path to ix\n232\n13.DataStructuresforIntegers 13.1.BinaryTrie: Adigitalsearchtree\n????\n0??? 1???\n00?? 01?? 10?? 11??\n000? 001? 010? 011? 100? 101? 110? 111?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nFigure13.4: Addingthevalues2and15totheBinaryTrieinFigure13.2.\nfor (; i < w; i++) {\nc = (ix >> (w-i-1)) & 1;\nu->child[c] = new Node();\nu->child[c]->parent = u;\nu = u->child[c];\n}\nu->x = x;\n// 3 - add u to linked list\nu->prev = pred;\nu->next = pred->next;;\nu->prev->next = u;\nu->next->prev = u;\n// 4 - walk back up, updating jump pointers\nNode *v = u->parent;\nwhile (v != NULL) {\nif ((v->left == NULL\n&& (v->jump == NULL || intValue(v->jump->x) > ix))\n|| (v->right == NULL\n&& (v->jump == NULL || intValue(v->jump->x) < ix)))\nv->jump = u;\nv = v->parent;\n}\nn++;\nreturn true;\n}\nThis method performs one walk down the search path for x and one walk back up. Each\nstepofthesewalkstakesconstanttime,sotheadd(x)runsinO(w)time.\n233\n13.DataStructuresforIntegers 13.1.BinaryTrie: Adigitalsearchtree\n????\n0??? 1???\n00?? 01?? 10?? 11??\n000? 001? 010? 011? 100? 101? 110? 111?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nFigure13.5: Removingthevalue9fromtheBinaryTrieinFigure13.2.\nThe remove(x) operation undoes the work of add(x). Like add(x), it has a lot of\nthingstotakecareof:\n1. Itfollowsthesearchpathforxuntilreachingtheleaf,u,containingx.\n2. Itremovesufromthedoubly-linkedlist\n3. Itdeletesuandthenwalksbackupthesearchpathforxdeletingnodesuntilreach-\ninganodevthathasachildthatisnotonthesearchpathforx\n4. Itwalksupwardsfromvtotherootupdatinganyjumppointersthatpointtou.\nAremovalisillustratedinFigure13.5.\nBinaryTrie\nbool remove(T x) {\n// 1 - find leaf, u, containing x\nint i = 0, c;\nunsigned ix = intValue(x);\nNode *u = &r;\nfor (i = 0; i < w; i++) {\nc = (ix >> (w-i-1)) & 1;\nif (u->child[c] == NULL) return false;\nu = u->child[c];\n}\n// 2 - remove u from linked list\nu->prev->next = u->next;\nu->next->prev = u->prev;\nNode *v = u;\n// 3 - delete nodes on path to u\n234\n13.DataStructuresforIntegers 13.2.XFastTrie: SearchinginDoubly-LogarithmicTime\nfor (i = w-1; i >= 0; i--) {\nc = (ix >> (w-i-1)) & 1;\nv = v->parent;\ndelete v->child[c];\nv->child[c] = NULL;\nif (v->child[1-c] != NULL) break;\n}\n// 4 - update jump pointers\nv->jump = u;\nfor (; i >= 0; i--) {\nc = (ix >> (w-i-1)) & 1;\nif (v->jump == u)\nv->jump = u->child[1-c];\nv = v->parent;\n}\nn--;\nreturn true;\n}\nTheorem13.1. ABinaryTrieimplementstheSSetinterfaceforw-bitintegers. ABinaryTrie\nsupports the operations add(x), remove(x), and find(x) in O(w) time per operation. The space\nusedbyaBinaryTriethatstoresnvaluesisO(n w).\n\u00b7\n13.2 XFastTrie: Searching in Doubly-Logarithmic Time\nThe performance of the BinaryTrie structure is not that impressive. The number of el-\nements, n, stored in the structure is at most 2w, so logn w. In other words, any of the\n\u2264\ncomparison-basedSSetstructuresdescribedinotherpartsofthisbookareatleastaseffi-\ncientasaBinaryTrie,anddon\u2019thavetherestrictionofonlybeingabletostoreintegers.\nNextwedescribetheXFastTrie,whichisjustaBinaryTriewithw+1hashtables\u2014\noneforeachlevelofthetrie. Thesehashtablesareusedtospeedupthefind(x)operation\nto O(logw) time. Recall that the find(x) operation in a BinaryTrie is almost complete\nonce we reach a node, u, where the search path forx would like to proceed tou.right (or\nu.left)butuhasnoright(respectively,left)child. Atthispoint,thesearchusesu.jumpto\njump to a leaf, v, of the BinaryTrie and either return v or its successor in the linked list\nofleaves. AnXFastTriespeedsupthesearchprocessbyusingbinarysearchonthelevels\nofthetrietolocatethenodeu.\nTo use binary search, we need a way to determine if the node u we are looking for\nis above a particular level, i, of if u is at or below level i. This information is given by\nthe highest-order i bits in the binary representation of x; these bits determine the search\n235\n13.DataStructuresforIntegers 13.2.XFastTrie: SearchinginDoubly-LogarithmicTime\n???? 0\n1\n0??? 1??? 1\n1\n00?? 01?? 10?? 11?? 2\n1\n000? 001? 010? 011? 100? 101? 110? 111? 3\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 4\nFigure13.6: Thesearchpathfor14(1110)endsatthenodelabelled11(cid:63)(cid:63) sincethereisno\nnodelabelled111(cid:63).\npath that x takes from the root to level i. For an example, refer to Figure 13.6; in this\nfigure the last node, u, on search path for 14 (whose binary representation is 1110) is the\nnode labelled 11(cid:63)(cid:63) at level 2 because there is no node labelled 111(cid:63) at level 3. Thus, we\ncan label each node at level i with an i-bit integer. Then the node u we are searching\nfor is at or below level i if and only if there is a node at level i whose label matches the\nhighest-orderibitsofx.\nIn an XFastTrie, we store, for each i 0,...,w , all the nodes at level i in a USet,\n\u2208{ }\nt[i],thatisimplementedasahashtable(Chapter5). UsingthisUSetallowsustocheckin\nconstant expected time if there is a node at level i whose label matches the highest-order\nibitsofx. Infact,wecanevenfindthisnodeusingt[i].find(x>>(w i)).\n\u2212\nThe hash tables t[0],...,t[w] allow us to use binary search to find u. Initially, we\nknowthatuisatsomeleveliwith0 i<w+1. Wethereforeinitializel=0andh=w+1\n\u2264\nand repeatedly look at the hash table t[i], where i = (l+h)/2 . If t[i] contains a node\n(cid:98) (cid:99)\nwhose label matches x\u2019s highest-order i bits then we set l=i (u is at or below level i),\notherwise we set h=i (u is above level i). This process terminates when h l 1, in\n\u2212 \u2264\nwhichcasewedeterminethatuisatlevell. Wethencompletethefind(x)operationusing\nu.jumpandthedoubly-linkedlistofleaves.\nXFastTrie\nT find(T x) {\nint l = 0, h = w+1;\nunsigned ix = intValue(x);\nNode *v, *u = &r;\nwhile (h-l > 1) {\n236\n13.DataStructuresforIntegers 13.2.XFastTrie: SearchinginDoubly-LogarithmicTime\nint i = (l+h)/2;\nXPair<Node> p(ix >> (w-i));\nif ((v = t[i].find(p).u) == NULL) {\nh = i;\n} else {\nu = v;\nl = i;\n}\n}\nif (l == w) return u->x;\nNode *pred = (((ix >> (w-l-1)) & 1) == 1) ? u->jump : u->jump->prev;\nreturn (pred->next == &dummy) ? NULL : pred->next->x;\n}\nEachiterationofthewhileloopintheabovemethoddecreasesh lbyroughlyafactorof\n\u2212\n2,sothisloopfindsuafterO(logw)iterations. Eachiterationperformsaconstantamount\nof work and one find(x) operation in a USet, which takes constant expected time. The\nremaining work takes only constant time, so the find(x) method in an XFastTrie takes\nonlyO(logw)expectedtime.\nThe add(x) and remove(x) methods for an XFastTrie are almost identical to the\nsame methods in a BinaryTrie. The only modifications are for managing the hash tables\nt[0],...,t[w]. Duringtheadd(x)operation,whenanewnodeiscreatedatleveli,thisnode\nisaddedtot[i]. Duringaremove(x)operation,whenanodeisremovedformleveli,this\nnode is removed from t[i]. Since adding and removing from a hash table take constant\nexpected time, this does not increase the running times of add(x) and remove(x) by more\nthan a constant factor. We omit a code listing or add(x) and remove(x) since it is almost\nidenticaltothe(long)codelistingalreadyprovidedforthesamemethodsinaBinaryTrie.\nThefollowingtheoremsummarizestheperformanceofanXFastTrie:\nTheorem13.2. AnXFastTrieimplementstheSSetinterfaceforw-bitintegers. AnXFastTrie\nsupportstheoperations\n\u2022 add(x)andremove(x)inO(w)expectedtimeperoperationand\n\u2022 find(x)inO(logw)expectedtimeperoperation.\nThespaceusedbyanXFastTriethatstoresnvaluesisO(n w).\n\u00b7\n237\n13.DataStructuresforIntegers 13.3.YFastTrie: ADoubly-LogarithmicTimeSSet\n13.3 YFastTrie: A Doubly-Logarithmic Time SSet\nThe XFastTrie is a big improvement over the BinaryTrie in terms of query time\u2014some\nwouldevencallitanexponentialimprovement\u2014buttheadd(x)andremove(x)operations\nare still not terribly fast. Furthermore, the space usage, O(n w), is higher than the other\n\u00b7\nSSet implementation in this book, which all use O(n) space. These two problems are re-\nlated;ifnadd(x)operationsbuildastructureofsizen wthentheadd(x)operationrequires\n\u00b7\nontheorderofwtime(andspace)peroperation.\nThe YFastTrie data structure simultaneously addresses both the space and speed\nissuesofXFastTries. AYFastTrieusesanXFastTrie,xft,butonlystoresO(n/w)values\ninxft. Inthisway,thetotalspaceusedbyxftisonlyO(n). Furthermore,onlyoneoutof\neverywadd(x)orremove(x)operationsintheYFastTrieresultsinanadd(x)orremove(x)\noperation in xft. By doing this, the average cost incurred by calls to xft\u2019s add(x) and\nremove(x)operationsisonlyconstant.\nThe obvious question becomes: If xft only stores n/w elements, where do the re-\nmainingn(1 1/w)elementsgo? Theseelementsgointosecondarystructures,inthiscasean\n\u2212\nextended version of treaps (Section 7.2). There are roughly n/w of these secondary struc-\nturesso,onaverage,eachofthemstoresO(w)items. TreapssupportlogarithmictimeSSet\noperations,sotheoperationsonthesetreapswillruninO(logw)time,asrequired.\nMoreconcretely,aYFastTriecontainsanXFastTrie,xft,thatcontainsarandom\nsampleofthedata,whereeachelementappearsinthesampleindependentlywithproba-\nbility1/w. Forconvenience,thevalue2w 1,isalwayscontainedinxft. Letx <x < <\n0 1\n\u2212 \u00b7\u00b7\u00b7\nx denote the elements stored in xft. Associated with each element, x , is a treap, t ,\nk 1 i i\n\u2212\nthatstoresallvaluesintherangex +1,...,x . ThisisillustratedinFigure13.7.\ni 1 i\n\u2212\nThefind(x)operationinaYFastTrieisfairlyeasy. Wesearchforxinxftandfind\nsomevaluex associatedwiththetreapt . Whenthenusethetreapfind(x)methodont\ni i i\ntoanswerthequery. Theentiremethodisaone-liner:\nYFastTrie\nT find(T x) {\nreturn xft.find(YPair<T>(intValue(x))).t->find(x);\n}\nThefirstfind(x)operation(onxft)takesO(logw)time. Thesecondfind(x)operation(on\na treap) takes O(logr) time, where r is the size of the treap. Later in this section, we will\nshowthattheexpectedsizeofthetreapisO(w)sothatthisoperationtakesO(logw)time.1\n1ThisisanapplicationofJensen\u2019sInequality:IfE[r]=w,thenE[logr] logw.\n\u2264\n238\n13.DataStructuresforIntegers 13.3.YFastTrie: ADoubly-LogarithmicTimeSSet\n????\n0??? 1???\n00?? 01?? 10?? 11??\n000? 001? 010? 011? 100? 101? 110? 111?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n0,1,3 4,5,8,9 10,11,13\nFigure13.7: AYFastTriecontainingthevalues0,1,3,4,6,8,9,10,11,and13.\nAdding an element to a YFastTrie is also fairly simple\u2014most of the time. The\nadd(x) method calls xft.find(x) to locate the treap, t, into which x should be inserted. It\nthencallst.add(x)toaddxtot. Atthispoint,ittossesabiasedcoin,thatcomesupheads\nwithprobability1/w. Ifthiscoincomesupheads,xwillbeaddedtoxft.\nThis is where things get a little more complicated. When x is added to xft, the\ntreaptneedstobesplitintotwotreapst1andt . Thetreapt1containsallthevaluesless\n(cid:48)\nthan or equal to x; t is the original treap, t, with the elements of t1 removed. Once this\n(cid:48)\nisdoneweaddthepair(x,t1)toxft. Figure13.8showsanexample.\nYFastTrie\nbool add(T x) {\nunsigned ix = intValue(x);\nTreap1<T> *t = xft.find(YPair<T>(ix)).t;\nif (t->add(x)) {\nn++;\nif (rand() % w == 0) {\nTreap1<T> *t1 = (Treap1<T>*)t->split(x);\nxft.add(YPair<T>(ix, t1));\n}\nreturn true;\n}\nreturn false;\nreturn true;\n239\n13.DataStructuresforIntegers 13.3.YFastTrie: ADoubly-LogarithmicTimeSSet\n????\n0??? 1???\n00?? 01?? 10?? 11??\n000? 001? 010? 011? 100? 101? 110? 111?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n0,1,2,3 4,5,6 4,5,88,,99 10,11,13\nFigure13.8: Addingthevalues2and6toaYFastTrie. Thecointossfor6cameupheads,\nso6wasaddedtoxftandthetreapcontaining4,5,6,8,9wassplit.\n}\nAdding x to t takes O(logw) time. Exercise 7.12 shows that splitting t into t1 and t can\n(cid:48)\nalsobedoneinO(logw)expectedtime. Addingthepair(x,t1)toxfttakesO(w)time, but\nonly happens with probability 1/w. Therefore, the expected running time of the add(x)\noperationis\n1\nO(logw)+ O(w)=O(logw)\nw\nThe remove(x) method just undoes the work performed by add(x). We use xft to\nfindtheleaf,u,inxftthatcontainstheanswertoxft.find(x). Fromu,wegetthetreap,t,\ncontainingxandremovexfromt. Ifxwasalsostoredinxft(andxisnotequalto2w 1)\n\u2212\nthen we remove x from xft and add the elements from x\u2019s treap to the treap, t2, that is\nstoredbyu\u2019ssuccessorinthelinkedlist. ThisisillustratedinFigure13.9.\nYFastTrie\nbool remove(T x) {\nunsigned ix = intValue(x);\nXFastTrieNode1<YPair<T> > *u = xft.findNode(ix);\nbool ret = u->x.t->remove(x);\nif (ret) n--;\nif (u->x.ix == ix && ix != UINT_MAX) {\nTreap1<T> *t2 = u->child[1]->x.t;\n240\n13.DataStructuresforIntegers 13.3.YFastTrie: ADoubly-LogarithmicTimeSSet\n????\n0??? 1???\n00?? 01?? 10?? 11??\n000? 001? 010? 011? 100? 101? 110? 111?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n0,1,2,3 4,5,6 8,9 8,10,11,13\nFigure13.9: Removingthevalues1and9fromaYFastTrieinFigure13.8.\nt2->absorb(*u->x.t);\nxft.remove(u->x);\n}\nreturn ret;\n}\nFindingthenodeuinxfttakesO(logw)expectedtime. RemovingxfromttakesO(logw)\nexpected time. Again, Exercise 7.12 shows that merging all the elements of t into t2 can\nbe done in O(logw) time. If necessary, removing x from xft takes O(w) time, but x is only\ncontainedinxftwithprobability1/w. Therefore,theexpectedtimetoremoveanelement\nfromaYFastTrieisO(logw).\nEarlierinthediscussion,weputoffarguingaboutthesizesoftreapsinthisstruc-\ntureuntillater. Beforefinishingweprovetheresultweneed.\nLemma 13.1. Let x be an integer stored in a YFastTrie and let n denote the number of ele-\nx\nmentsinthetreap,t,thatcontainsx. ThenE[n ] 2w 1.\nx\n\u2264 \u2212\nProof. RefertoFigure13.10. Letx <x < <x =x< <x denotetheelementsstored\n1 2 i n\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\nin the YFastTrie. The treap t contains some elements greater than or equal to x. These\nare x ,x ,...,x , where x is the only one of these elements in which the biased\ni i+1 i+j 1 i+j 1\n\u2212 \u2212\ncoin toss performed in the add(x) method came up heads. In other words, E[j] is equal to\n241\n13.DataStructuresforIntegers 13.4.DiscussionandExercises\nelementsintreap,t,containingx\nH T T ... T T T T T ... T H\nx xz x ... x x x}=|x x x ... x x {\ni\n\u2212\nk\n\u2212\n1 i\n\u2212\nk i\n\u2212\nk + 1 i\n\u2212\n2 i\n\u2212\n1 i i + 1 i + 2 i + j\n\u2212\n2 i + j\n\u2212\n1\nk j\n| {z } | {z }\nFigure 13.10: The number of elements in the treap, t, containing x is determined by two\ncointossingexperiments.\nthe expected number of biased coin tosses required to obtain the first heads.2 Each coin\ntossisindependentandcomesupheadswithprobability1/w,soE[j] w. (SeeLemma4.2\n\u2264\nforananalysisofthisforthecasew=2.)\nSimilarly, the elements of t smaller than x are x ,...,x where all these k coin\ni 1 i k\n\u2212 \u2212\ntosses come up tails and the coin toss for x comes up heads. Therefore, E[k] w 1,\ni k 1\n\u2212 \u2212 \u2264 \u2212\nsincethisisthesamecointossingexperimentconsideredpreviously,butinwhichthelast\ntossdoesn\u2019tcount. Insummary,n =j+k,so\nx\nE[n ]=E[j+k]=E[j]+E[k] 2w 1 .\nx\n\u2264 \u2212\nLemma 13.1 was the last piece in the proof of the following theorem, which sum-\nmarizestheperformanceoftheYFastTrie:\nTheorem 13.3. A YFastTrie implements the SSet interface for w-bit integers. A YFastTrie\nsupportstheoperationsadd(x),remove(x),andfind(x)inO(logw)expectedtimeperoperation.\nThespaceusedbyaYFastTriethatstoresnvaluesisO(n+w).\nThe w term in the space requirement comes from the fact that xft always stores\nthe value 2w 1. The implementation could be modified (at the expense of adding some\n\u2212\nextra cases to the code) so that it is unnecessary to store this value. In this case, the space\nrequirementinthetheorembecomesO(n).\n13.4 Discussion and Exercises\nThefirstdatastructuretoprovideO(logw)timeadd(x),remove(x),andfind(x)operations\nwas proposed by van Emde Boas and has since become known as the van Emde Boas (or\n2Thisanalysisignoresthefactthatjneverexceedsn i+1.However,thisonlydecreasesE[j],sotheupper\n\u2212\nboundstillholds\n242\n13.DataStructuresforIntegers 13.4.DiscussionandExercises\nstratified) tree [65]. The original van Emde Boas structure had size 2w, so was impractical\nforlargeintegers.\nThe XFastTrie and YFastTrie data structures were discovered by Willard [68].\nThe XFastTrie structure is very closely related to van Emde Boas trees. One view of this\nis that the hash tables in an XFastTrie replace arrays in a van Emde Boas tree. That is,\ninsteadofstoringthehashtablet[i],avanEmdeBoastreestoresanarrayoflength2i.\nAnother structure for storing integers is Fredman and Willard\u2019s fusion trees [27].\nThisstructurecanstorenw-bitintegersinO(n)spacesothatthefind(x)operationrunsin\nO((logn)/(logw)) time. By using a fusion tree when logw > logn and a YFastTrie when\nlogw logn one obtains an O(n) space data structure that can implement the find(x)\n(cid:112)\n\u2264\noperationinO( logn)time. Recentlower-boundresultsofPa\u02c7tras\u00b8cuandThorup[52]show\n(cid:112)\nthattheseresultsaremore-or-lessoptimal,atleastforstructuresthatuseonlyO(n)space.\n(cid:112)\nExercise 13.1. Design and implement a simplified version of a BinaryTrie that doesn\u2019t\nhavealinkedlistorjumppointers,butforwhichfind(x)stillrunsinO(w)time.\nExercise 13.2. Design and implement a simplified implementation of an XFastTrie that\ndoesn\u2019tuseabinarytrieatall. Instead,yourimplementationshouldstoreeverythingina\ndoubly-linkedlistandw+1hashtables.\nExercise13.3. WecanthinkofaBinaryTrieasastructurethatstoresbitstringsoflength\nw in such a way that each bitstring is represented as a root to leaf path. Extend this idea\ninto an SSet implementation that stores variable-length strings and implements add(s),\nremove(s),andfind(s)intimeproporitionaltothelengthofs.\nHint: Each node in your data structure should store a hash table that is indexed by char-\nactervalues.\nExercise 13.4. For an integer x 0,...2w 1 , let d(x) denote the difference between x\n\u2208 { \u2212 }\nand the value returned by find(x) [if find(x) returns null, then define d(x) as 2w]. For\nexample,iffind(23)returns43,thend(23)=20.\n1. Designandimplementamodifiedversionofthefind(x)operationinanXFastTrie\nthat runs in O(1+logd(x)) expected time. Hint: The hash table t[w] contains all the\nvalues,x,suchthatd(x)=0,sothatwouldbeagoodplacetostart.\n2. Designandimplementamodifiedversionofthefind(x)operationinanXFastTrie\nthatrunsinO(1+loglogd(x))expectedtime.\n243\n13.DataStructuresforIntegers 13.4.DiscussionandExercises\n244\nBibliography\n[1] Free eBooks by Project Gutenberg. Available from: http://www.gutenberg.org/\n[cited2011-10-12].\n[2] IEEEStandardforFloating-PointArithmetic. Technicalreport,MicroprocessorStan-\ndardsCommitteeoftheIEEEComputerSociety,3ParkAvenue,NewYork,NY10016-\n5997,USA,August2008. doi:10.1109/IEEESTD.2008.4610935.\n[3] G.M.Adelson-VelskiiandE.M.Landis. Analgorithmfortheorganizationofinforma-\ntion. SovietMathematicsDoklady,3(1259-1262):4,1962.\n[4] A. Andersson. Improving partial rebuilding by using simple balance criteria. In\nF.K.H.A.Dehne,J.-R.Sack,andN.Santoro,editors,AlgorithmsandDataStructures,\nWorkshop WADS \u201989, Ottawa, Canada, August 17\u201319, 1989, Proceedings, volume 382\nofLectureNotesinComputerScience,pages393\u2013402.Springer,1989.\n[5] A. Andersson. Balanced search trees made simple. In F. K. H. A. Dehne, J.-R. Sack,\nN. Santoro, and S. Whitesides, editors, Algorithms and Data Structures, Third Work-\nshop, WADS \u201993, Montre\u00b4al, Canada, August 11\u201313, 1993, Proceedings, volume 709 of\nLectureNotesinComputerScience,pages60\u201371.Springer,1993.\n[6] A.Andersson. Generalbalancedtrees. JournalofAlgorithms,30(1):1\u201318,1999.\n[7] A. Bagchi, A. L. Buchsbaum, and M. T. Goodrich. Biased skip lists. In P. Bose and\nP. Morin, editors, Algorithms and Computation, 13th International Symposium, ISAAC\n2002Vancouver,BC,Canada,November21\u201323,2002,Proceedings,volume2518ofLec-\ntureNotesinComputerScience,pages1\u201313.Springer,2002.\n[8] Bibliography on hashing. Available from: http://liinwww.ira.uka.de/\nbibliography/Theory/hash.html[cited2011-07-20].\n[9] J.Black,S.Halevi,H.Krawczyk,T.Krovetz,andP.Rogaway. UMAC:Fastandsecure\nmessage authentication. In M. J. Wiener, editor, Advances in Cryptology - CRYPTO\n245\nBibliography Bibliography\n\u201999, 19th Annual International Cryptology Conference, Santa Barbara, California, USA,\nAugust 15\u201319, 1999, Proceedings, volume 1666 of Lecture Notes in Computer Science,\npages79\u201379.Springer,1999.\n[10] P.Bose,K.Dou\u00a8\u0131eb,andS.Langerman. Dynamicoptimalityforskiplistsandb-trees.\nIn S.-H. Teng, editor, Proceedings of the Nineteenth Annual ACM-SIAM Symposium on\nDiscreteAlgorithms,SODA2008,SanFrancisco,California,USA,January20\u201322,2008,\npages1106\u20131114.SIAM,2008.\n[11] A. Brodnik, S. Carlsson, E. D. Demaine, J. I. Munro, and R. Sedgewick. Resizable\narraysinoptimaltimeandspace. InDehneetal.[15],pages37\u201348.\n[12] J.L.CarterandM.N.Wegman.Universalclassesofhashfunctions.Journalofcomputer\nandsystemsciences,18(2):143\u2013154,1979.\n[13] C.A. Crane. Linear lists and priority queues as balanced binary trees. Technical\nReportSTAN-CS-72-259,ComputerScienceDepartment,StanfordUniversity,1972.\n[14] S.A. Crosby and D.S. Wallach. Denial of service via algorithmic complexity attacks.\nInProceedingsofthe12thUSENIXSecuritySymposium,pages29\u201344,2003.\n[15] F. K. H. A. Dehne, A. Gupta, J.-R. Sack, and R. Tamassia, editors. Algorithms and\nData Structures, 6th International Workshop, WADS \u201999, Vancouver, British Columbia,\nCanada, August 11\u201314, 1999, Proceedings, volume 1663 of Lecture Notes in Computer\nScience.Springer,1999.\n[16] L.Devroye. Applicationsofthetheoryofrecordsinthestudyofrandomtrees. Acta\nInformatica,26(1):123\u2013130,1988.\n[17] P. Dietz and J. Zhang. Lower bounds for monotonic list labeling. In J. R. Gilbert\nand R. G. Karlsson, editors, SWAT 90, 2nd Scandinavian Workshop on Algorithm The-\nory, Bergen, Norway, July 11\u201314, 1990, Proceedings, volume 447 of Lecture Notes in\nComputerScience,pages173\u2013180.Springer,1990.\n[18] M. Dietzfelbinger. Universal hashing and k-wise independent random variables via\ninteger arithmetic without primes. In C. Puech and R. Reischuk, editors, STACS 96,\n13th Annual Symposium on Theoretical Aspects of Computer Science, Grenoble, France,\nFebruary22\u201324,1996,Proceedings,volume1046ofLectureNotesinComputerScience,\npages567\u2013580.Springer,1996.\n246\nBibliography Bibliography\n[19] M.Dietzfelbinger,J.Gil,Y.Matias,andN.Pippenger. Polynomialhashfunctionsare\nreliable. In W. Kuich, editor, Automata, Languages and Programming, 19th Interna-\ntional Colloquium, ICALP92, Vienna, Austria, July 13\u201317, 1992, Proceedings, volume\n623ofLectureNotesinComputerScience,pages235\u2013246.Springer,1992.\n[20] M. Dietzfelbinger, T. Hagerup, J. Katajainen, and M. Penttonen. A reliable random-\nizedalgorithmfortheclosest-pairproblem. JournalofAlgorithms,25(1):19\u201351,1997.\n[21] M.Dietzfelbinger,A.R.Karlin,K.Mehlhorn,F.MeyeraufderHeide,H.Rohnert,and\nR. E. Tarjan. Dynamic perfect hashing: Upper and lower bounds. SIAM Journal on\nComputing,23(4):738\u2013761,1994.\n[22] A.Elmasry. PairingheapswithO(loglogn)decreasecost. InProceedingsofthetwenti-\nethAnnualACM-SIAMSymposiumonDiscreteAlgorithms,pages471\u2013476.Societyfor\nIndustrialandAppliedMathematics,2009.\n[23] F. Ergun, S. C. Sahinalp, J. Sharp, and R. Sinha. Biased dictionaries with fast in-\nsert/deletes. In Proceedings of the thirty-third annual ACM symposium on Theory of\ncomputing,pages483\u2013491,NewYork,NY,USA,2001.ACM.\n[24] M. Eytzinger. Thesaurus principum hac aetate in Europa viventium (Cologne). 1590.\nIn commentaries, \u2018Eytzinger\u2019 may appear in variant forms, including: Aitsingeri,\nAitsingero,Aitsingerum,Eyzingern.\n[25] R.W.Floyd. Algorithm245: Treesort3. CommunicationsoftheACM,7(12):701,1964.\n[26] M. L. Fredman, J. Komlo\u00b4s, and E. Szemere\u00b4di. Storing a sparse table with 0 (1) worst\ncaseaccesstime. JournaloftheACM,31(3):538\u2013544,1984.\n[27] M. L. Fredman and D. E. Willard. Surpassing the information theoretic bound with\nfusiontrees. Journalofcomputerandsystemsciences,47(3):424\u2013436,1993.\n[28] M.L.Fredman,R.Sedgewick,D.D.Sleator,andR.E.Tarjan. Thepairingheap: Anew\nformofself-adjustingheap. Algorithmica,1(1):111\u2013129,1986.\n[29] M.L. Fredman and R.E. Tarjan. Fibonacci heaps and their uses in improved network\noptimizationalgorithms. JournaloftheACM,34(3):596\u2013615,1987.\n[30] I.GalperinandR.L.Rivest. Scapegoattrees. InProceedingsofthefourthannualACM-\nSIAM Symposium on Discrete algorithms, pages 165\u2013174. Society for Industrial and\nAppliedMathematics,1993.\n247\nBibliography Bibliography\n[31] A. Gambin and A. Malinowski. Randomized meldable priority queues. In SOF-\nSEM98: TheoryandPracticeofInformatics,pages344\u2013349.Springer,1998.\n[32] M. T. Goodrich and J. G. Kloss. Tiered vectors: Efficient dynamic arrays for rank-\nbasedsequences. InDehneetal.[15],pages205\u2013216.\n[33] R.L.Graham,D.E.Knuth,andO.Patashnik. ConcreteMathematics. Addison-Wesley,\n2ndedition,1994.\n[34] L.J. Guibas and R. Sedgewick. A dichromatic framework for balanced trees. In 19th\nAnnual Symposium on Foundations of Computer Science, Ann Arbor, Michigan, 16\u201318\nOctober1978,Proceedings,pages8\u201321.IEEEComputerSociety,1978.\n[35] C.A.R.Hoare. Algorithm64: Quicksort. CommunicationsoftheACM,4(7):321,1961.\n[36] J.E.HopcroftandR.E.Tarjan. Algorithm447: Efficientalgorithmsforgraphmanip-\nulation. CommunicationsoftheACM,16(6):372\u2013378,1973.\n[37] J. E. Hopcroft and R. E. Tarjan. Efficient planarity testing. Journal of the ACM,\n21(4):549\u2013568,1974.\n[38] HP-UX process management white paper, version 1.3, 1997. Available\nfrom: http://h21007.www2.hp.com/portal/download/files/prot/files/STK/\npdfs/proc_mgt.pdf[cited2011-07-20].\n[39] P. Kirschenhofer, C. Martinez, and H. Prodinger. Analysis of an optimized search\nalgorithmforskiplists. TheoreticalComputerScience,144:199\u2013220,1995.\n[40] P.KirschenhoferandH.Prodinger. Thepathlengthofrandomskiplists. ActaInfor-\nmatica,31:775\u2013792,1994.\n[41] D. Knuth. Fundamental Algorithms, volume 1 of The Art of Computer Programming.\nAddison-Wesley,thirdedition,1997.\n[42] D. Knuth. Seminumerical Algorithms, volume 2 of The Art of Computer Programming.\nAddison-Wesley,thirdedition,1997.\n[43] D. Knuth. Sorting and Searching, volume 3 of The Art of Computer Programming.\nAddison-Wesley,secondedition,1997.\n[44] C. Y. Lee. An algorithm for path connection and its applications. IRE Transaction on\nElectronicComputers,EC-10(3):346\u2013365,1961.\n248\nBibliography Bibliography\n[45] E. Lehman, F. T. Leighton, and A. R. Meyer. Mathematics for Computer Science.\n2011.Availablefrom: http://courses.csail.mit.edu/6.042/spring12/mcsfull.\npdf[cited2012-03-22].\n[46] C. Mart\u00b4\u0131nez and S. Roura. Randomized binary search trees. Journal of the ACM,\n45(2):288\u2013323,1998.\n[47] E. F. Moore. The shortest path through a maze. In Proceedings of the International\nSymposiumontheTheoryofSwitching,pages285\u2013292,1959.\n[48] J.I.Munro,T.Papadakis,andR.Sedgewick. Deterministicskiplists. InProceedingsof\nthe third annual ACM-SIAM symposium on Discrete algorithms (SODA\u201992), pages 367\u2013\n375,Philadelphia,PA,USA,1992.SocietyforIndustrialandAppliedMathematics.\n[49] Oracle. The Collections Framework. Available from: http://download.oracle.com/\njavase/1.5.0/docs/guide/collections/[cited2011-07-19].\n[50] R.PaghandF.F.Rodler. Cuckoohashing. JournalofAlgorithms,51(2):122\u2013144,2004.\n[51] T. Papadakis, J. I. Munro, and P. V. Poblete. Average search and update costs in skip\nlists. BIT,32:316\u2013332,1992.\n[52] M.Pa\u02c7tras\u00b8cuandM.Thorup. Randomizationdoesnothelpsearchingpredecessors. In\nN. Bansal, K. Pruhs, and C. Stein, editors, Proceedings of the EighteenthAnnual ACM-\nSIAM Symposium on Discrete Algorithms, SODA 2007, New Orleans, Louisiana, USA,\nJanuary7\u20139,2007,pages555\u2013564.SIAM,2007.\n[53] W. Pugh. A skip list cookbook. Technical report, Institute for Advanced Computer\nStudies, Department of Computer Science, University of Maryland, College Park,\n1989.Availablefrom: ftp://ftp.cs.umd.edu/pub/skipLists/cookbook.pdf[cited\n2011-07-20].\n[54] W.Pugh. Skiplists: Aprobabilisticalternativetobalancedtrees. Communicationsof\ntheACM,33(6):668\u2013676,1990.\n[55] M. Pa\u02c7tras\u00b8cu and M. Thorup. The power of simple tabulation hashing, 2010. arXiv:\n1011.5200.\n[56] Redis. Availablefrom: http://redis.io/[cited2011-07-20].\n249\nBibliography Bibliography\n[57] B. Reed. The height of a random binary search tree. Journal of the ACM, 50(3):306\u2013\n332,2003.\n[58] S. M. Ross. Probability Models for Computer Science. Academic Press, Inc., Orlando,\nFL,USA,2001.\n[59] R. Sedgewick. Left-leaning red-black trees, September 2008. Available from: http:\n//www.cs.princeton.edu/\u02dcrs/talks/LLRB/LLRB.pdf[cited2011-07-21].\n[60] R. Seidel and C.R. Aragon. Randomized search trees. Algorithmica, 16(4):464\u2013497,\n1996.\n[61] H.H.Seward. Informationsortingintheapplicationofelectronicdigitalcomputers\ntobusinessoperations.Master\u2019sthesis,MassachusettsInstituteofTechnology,Digital\nComputerLaboratory,1954.\n[62] SkipDB. Available from: http://dekorte.com/projects/opensource/SkipDB/\n[cited2011-07-20].\n[63] D.D. Sleator and R.E. Tarjan. Self-adjusting binary trees. In Proceedings of the 15th\nAnnual ACM Symposium on Theory of Computing, 25\u201327 April, 1983, Boston, Mas-\nsachusetts,USA,pages235\u2013245.ACM,ACM,1983.\n[64] S. P. Thompson. Calculus Made Easy. MacMillan, Toronto, 1914. Project Gutenberg\nEBook 33283. Available from: http://www.gutenberg.org/ebooks/33283 [cited\n2012-06-14].\n[65] Peter van Emde Boas. Preserving order in a forest in less than logarithmic time and\nlinearspace. InformationProcessingLetters,6(3):80\u201382,1977.\n[66] J. Vuillemin. A data structure for manipulating priority queues. Communications of\ntheACM,21(4):309\u2013315,1978.\n[67] J. Vuillemin. A unifying look at data structures. Communications of the ACM,\n23(4):229\u2013239,1980.\n[68] D.E.Willard.Log-logarithmicworst-caserangequeriesarepossibleinspacetheta(n).\nInformationProcessingLetters,17(2):81\u201384,1983.\n[69] J.W.J.Williams. Algorithm232: Heapsort. CommunicationsoftheACM,7(6):347\u2013348,\n1964.\n250\n",
  "context": "(opendatastructures.org) and also, more importantly, on a reliable source code man-\nagementsite(github.com/patmorin/ods).\nThissourcecodeisreleasedunderaCreativeCommonsAttributionlicense,mean-",
  "source_file": "resources\\Year 1\\C++ Docs\\C++ Data Structure.pdf",
  "line_numbers": [
    46,
    9638
  ]
}